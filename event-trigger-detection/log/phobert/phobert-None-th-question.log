Namespace(model='vinai/phobert-base-v2', level='word', no_train=False, train_batch_size=8, eval_batch_size=8, lr=1e-05, num_epochs=20, log_step=100, warmup_proportion=0.1, no_cuda=False, gradient_accumulation_steps=1, nth_question=None)
Epoch 1|20:
Batch 1|1608: loss 3.4578 f1 0.00 precision 0.00 recall 0.00
Batch 101|1608: loss 3.3333 f1 0.00 precision 0.00 recall 0.00
Batch 201|1608: loss 2.4921 f1 0.00 precision 0.00 recall 0.00
Batch 301|1608: loss 1.6442 f1 0.00 precision 0.00 recall 0.00
Batch 401|1608: loss 1.1876 f1 0.00 precision 0.00 recall 0.00
Batch 501|1608: loss 0.8791 f1 0.00 precision 0.00 recall 0.00
Batch 601|1608: loss 0.7954 f1 0.00 precision 0.00 recall 0.00
Batch 701|1608: loss 0.6041 f1 0.00 precision 0.00 recall 0.00
Batch 801|1608: loss 0.4751 f1 0.00 precision 0.00 recall 0.00
Batch 901|1608: loss 0.4454 f1 0.00 precision 0.00 recall 0.00
Batch 1001|1608: loss 0.4759 f1 0.00 precision 0.00 recall 0.00
Batch 1101|1608: loss 0.3752 f1 0.00 precision 0.00 recall 0.00
Batch 1201|1608: loss 0.4123 f1 0.00 precision 0.00 recall 0.00
Batch 1301|1608: loss 0.3822 f1 0.00 precision 0.00 recall 0.00
Batch 1401|1608: loss 0.3232 f1 0.00 precision 0.00 recall 0.00
Batch 1501|1608: loss 0.3301 f1 0.00 precision 0.00 recall 0.00
Batch 1601|1608: loss 0.3513 f1 0.00 precision 0.00 recall 0.00
Batch 1608|1608: loss 0.2089 f1 0.00 precision 0.00 recall 0.00
Train: loss 1.0002 f1 0.00 precision 0.00 recall 0.04
Time: 376.63
Dev: loss 0.2960 f1 0.00 precision 0.00 recall 0.00

Epoch 2|20:
Batch 1|1608: loss 0.3647 f1 0.00 precision 0.00 recall 0.00
Batch 101|1608: loss 0.3804 f1 0.00 precision 0.00 recall 0.00
Batch 201|1608: loss 0.2128 f1 0.00 precision 0.00 recall 0.00
Batch 301|1608: loss 0.2096 f1 0.00 precision 0.00 recall 0.00
Batch 401|1608: loss 0.2915 f1 0.00 precision 0.00 recall 0.00
Batch 501|1608: loss 0.2766 f1 0.00 precision 0.00 recall 0.00
Batch 601|1608: loss 0.2206 f1 0.00 precision 0.00 recall 0.00
Batch 701|1608: loss 0.2045 f1 0.00 precision 0.00 recall 0.00
Batch 801|1608: loss 0.1202 f1 0.00 precision 0.00 recall 0.00
Batch 901|1608: loss 0.2319 f1 0.00 precision 0.00 recall 0.00
Batch 1001|1608: loss 0.2884 f1 0.00 precision 0.00 recall 0.00
Batch 1101|1608: loss 0.4204 f1 0.00 precision 0.00 recall 0.00
Batch 1201|1608: loss 0.3059 f1 0.00 precision 0.00 recall 0.00
Batch 1301|1608: loss 0.3933 f1 0.00 precision 0.00 recall 0.00
Batch 1401|1608: loss 0.2280 f1 0.00 precision 0.00 recall 0.00
Batch 1501|1608: loss 0.3110 f1 0.00 precision 0.00 recall 0.00
Batch 1601|1608: loss 0.0869 f1 0.00 precision 0.00 recall 0.00
Batch 1608|1608: loss 0.2504 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.2473 f1 0.00 precision 0.00 recall 0.00
Time: 378.92
Dev: loss 0.1883 f1 0.00 precision 0.00 recall 0.00

Epoch 3|20:
Batch 1|1608: loss 0.3519 f1 0.00 precision 0.00 recall 0.00
Batch 101|1608: loss 0.1142 f1 0.00 precision 0.00 recall 0.00
Batch 201|1608: loss 0.1923 f1 0.00 precision 0.00 recall 0.00
Batch 301|1608: loss 0.4891 f1 0.00 precision 0.00 recall 0.00
Batch 401|1608: loss 0.1632 f1 0.00 precision 0.00 recall 0.00
Batch 501|1608: loss 0.0879 f1 0.00 precision 0.00 recall 0.00
Batch 601|1608: loss 0.0354 f1 0.00 precision 0.00 recall 0.00
Batch 701|1608: loss 0.3263 f1 0.00 precision 0.00 recall 0.00
Batch 801|1608: loss 0.1583 f1 0.00 precision 0.00 recall 0.00
Batch 901|1608: loss 0.5172 f1 0.00 precision 0.00 recall 0.00
Batch 1001|1608: loss 0.1363 f1 0.00 precision 0.00 recall 0.00
Batch 1101|1608: loss 0.1753 f1 0.00 precision 0.00 recall 0.00
Batch 1201|1608: loss 0.2354 f1 0.00 precision 0.00 recall 0.00
Batch 1301|1608: loss 0.5606 f1 0.00 precision 0.00 recall 0.00
Batch 1401|1608: loss 0.1481 f1 0.00 precision 0.00 recall 0.00
Batch 1501|1608: loss 0.2424 f1 0.00 precision 0.00 recall 0.00
Batch 1601|1608: loss 0.1707 f1 0.00 precision 0.00 recall 0.00
Batch 1608|1608: loss 0.1620 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.1991 f1 0.00 precision 0.00 recall 0.00
Time: 378.92
Dev: loss 0.1806 f1 0.00 precision 0.00 recall 0.00

Epoch 4|20:
Batch 1|1608: loss 0.1209 f1 0.00 precision 0.00 recall 0.00
Batch 101|1608: loss 0.0844 f1 0.00 precision 0.00 recall 0.00
Batch 201|1608: loss 0.1323 f1 0.00 precision 0.00 recall 0.00
Batch 301|1608: loss 0.0746 f1 0.00 precision 0.00 recall 0.00
Batch 401|1608: loss 0.1526 f1 0.00 precision 0.00 recall 0.00
Batch 501|1608: loss 0.2056 f1 0.00 precision 0.00 recall 0.00
Batch 601|1608: loss 0.1040 f1 0.00 precision 0.00 recall 0.00
Batch 701|1608: loss 0.1984 f1 0.00 precision 0.00 recall 0.00
Batch 801|1608: loss 0.1993 f1 0.00 precision 0.00 recall 0.00
Batch 901|1608: loss 0.3576 f1 0.00 precision 0.00 recall 0.00
Batch 1001|1608: loss 0.0988 f1 0.00 precision 0.00 recall 0.00
Batch 1101|1608: loss 0.1376 f1 0.00 precision 0.00 recall 0.00
Batch 1201|1608: loss 0.1879 f1 0.00 precision 0.00 recall 0.00
Batch 1301|1608: loss 0.4862 f1 0.00 precision 0.00 recall 0.00
Batch 1401|1608: loss 0.1260 f1 0.00 precision 0.00 recall 0.00
Batch 1501|1608: loss 0.1497 f1 0.00 precision 0.00 recall 0.00
Batch 1601|1608: loss 0.1247 f1 0.00 precision 0.00 recall 0.00
Batch 1608|1608: loss 0.1443 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.1935 f1 0.00 precision 0.00 recall 0.00
Time: 378.94
Dev: loss 0.1794 f1 0.00 precision 0.00 recall 0.00

Epoch 5|20:
Batch 1|1608: loss 0.4285 f1 0.00 precision 0.00 recall 0.00
Batch 101|1608: loss 0.1077 f1 0.00 precision 0.00 recall 0.00
Batch 201|1608: loss 0.0913 f1 0.00 precision 0.00 recall 0.00
Batch 301|1608: loss 0.1706 f1 0.00 precision 0.00 recall 0.00
Batch 401|1608: loss 0.3935 f1 0.00 precision 0.00 recall 0.00
Batch 501|1608: loss 0.1924 f1 0.00 precision 0.00 recall 0.00
Batch 601|1608: loss 0.1310 f1 0.00 precision 0.00 recall 0.00
Batch 701|1608: loss 0.1492 f1 0.00 precision 0.00 recall 0.00
Batch 801|1608: loss 0.0651 f1 0.00 precision 0.00 recall 0.00
Batch 901|1608: loss 0.0659 f1 0.00 precision 0.00 recall 0.00
Batch 1001|1608: loss 0.1097 f1 0.00 precision 0.00 recall 0.00
Batch 1101|1608: loss 0.0973 f1 0.00 precision 0.00 recall 0.00
Batch 1201|1608: loss 0.2227 f1 0.00 precision 0.00 recall 0.00
Batch 1301|1608: loss 0.2768 f1 0.00 precision 0.00 recall 0.00
Batch 1401|1608: loss 0.3043 f1 0.00 precision 0.00 recall 0.00
Batch 1501|1608: loss 0.2023 f1 0.00 precision 0.00 recall 0.00
Batch 1601|1608: loss 0.2849 f1 0.00 precision 0.00 recall 0.00
Batch 1608|1608: loss 0.2162 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.1879 f1 0.00 precision 0.00 recall 0.00
Time: 379.00
Dev: loss 0.1692 f1 0.00 precision 0.00 recall 0.00

Epoch 6|20:
Batch 1|1608: loss 0.0715 f1 0.00 precision 0.00 recall 0.00
Batch 101|1608: loss 0.0843 f1 0.00 precision 0.00 recall 0.00
Batch 201|1608: loss 0.1121 f1 0.00 precision 0.00 recall 0.00
Batch 301|1608: loss 0.2777 f1 0.00 precision 0.00 recall 0.00
Batch 401|1608: loss 0.0665 f1 0.00 precision 0.00 recall 0.00
Batch 501|1608: loss 0.1924 f1 0.00 precision 0.00 recall 0.00
Batch 601|1608: loss 0.1445 f1 0.00 precision 0.00 recall 0.00
Batch 701|1608: loss 0.1643 f1 0.00 precision 0.00 recall 0.00
Batch 801|1608: loss 0.0563 f1 0.00 precision 0.00 recall 0.00
Batch 901|1608: loss 0.1441 f1 0.00 precision 0.00 recall 0.00
Batch 1001|1608: loss 0.1115 f1 0.00 precision 0.00 recall 0.00
Batch 1101|1608: loss 0.1099 f1 0.00 precision 0.00 recall 0.00
Batch 1201|1608: loss 0.1104 f1 33.33 precision 100.00 recall 20.00
Batch 1301|1608: loss 0.1999 f1 0.00 precision 0.00 recall 0.00
Batch 1401|1608: loss 0.0667 f1 0.00 precision 0.00 recall 0.00
Batch 1501|1608: loss 0.0570 f1 0.00 precision 0.00 recall 0.00
Batch 1601|1608: loss 0.2202 f1 40.00 precision 100.00 recall 25.00
Batch 1608|1608: loss 0.0585 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.1366 f1 2.92 precision 5.47 recall 2.22
Time: 379.36
Dev: loss 0.0901 f1 22.27 precision 57.58 recall 13.81
Save model weight!

Epoch 7|20:
Batch 1|1608: loss 0.1627 f1 0.00 precision 0.00 recall 0.00
Batch 101|1608: loss 0.0695 f1 0.00 precision 0.00 recall 0.00
Batch 201|1608: loss 0.1228 f1 0.00 precision 0.00 recall 0.00
Batch 301|1608: loss 0.0957 f1 0.00 precision 0.00 recall 0.00
Batch 401|1608: loss 0.0665 f1 57.14 precision 66.67 recall 50.00
Batch 501|1608: loss 0.0734 f1 0.00 precision 0.00 recall 0.00
Batch 601|1608: loss 0.0782 f1 44.44 precision 66.67 recall 33.33
Batch 701|1608: loss 0.0318 f1 80.00 precision 100.00 recall 66.67
Batch 801|1608: loss 0.0577 f1 66.67 precision 100.00 recall 50.00
Batch 901|1608: loss 0.0454 f1 0.00 precision 0.00 recall 0.00
Batch 1001|1608: loss 0.1908 f1 28.57 precision 100.00 recall 16.67
Batch 1101|1608: loss 0.1278 f1 54.55 precision 75.00 recall 42.86
Batch 1201|1608: loss 0.0540 f1 50.00 precision 100.00 recall 33.33
Batch 1301|1608: loss 0.1415 f1 50.00 precision 100.00 recall 33.33
Batch 1401|1608: loss 0.0892 f1 28.57 precision 100.00 recall 16.67
Batch 1501|1608: loss 0.0328 f1 66.67 precision 66.67 recall 66.67
Batch 1601|1608: loss 0.0758 f1 0.00 precision 0.00 recall 0.00
Batch 1608|1608: loss 0.0122 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0874 f1 24.15 precision 37.31 recall 20.02
Time: 379.99
Dev: loss 0.0714 f1 42.74 precision 48.82 recall 38.01
Save model weight!

Epoch 8|20:
Batch 1|1608: loss 0.0136 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0672 f1 0.00 precision 0.00 recall 0.00
Batch 201|1608: loss 0.1072 f1 0.00 precision 0.00 recall 0.00
Batch 301|1608: loss 0.1389 f1 50.00 precision 100.00 recall 33.33
Batch 401|1608: loss 0.0247 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.1450 f1 0.00 precision 0.00 recall 0.00
Batch 601|1608: loss 0.0914 f1 50.00 precision 100.00 recall 33.33
Batch 701|1608: loss 0.0210 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0190 f1 0.00 precision 0.00 recall 0.00
Batch 901|1608: loss 0.0604 f1 28.57 precision 33.33 recall 25.00
Batch 1001|1608: loss 0.1266 f1 0.00 precision 0.00 recall 0.00
Batch 1101|1608: loss 0.0213 f1 50.00 precision 50.00 recall 50.00
Batch 1201|1608: loss 0.0668 f1 61.54 precision 80.00 recall 50.00
Batch 1301|1608: loss 0.0394 f1 80.00 precision 100.00 recall 66.67
Batch 1401|1608: loss 0.0423 f1 66.67 precision 100.00 recall 50.00
Batch 1501|1608: loss 0.0877 f1 0.00 precision 0.00 recall 0.00
Batch 1601|1608: loss 0.0191 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0037 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0679 f1 37.69 precision 49.98 recall 33.70
Time: 379.71
Dev: loss 0.0784 f1 48.94 precision 41.83 recall 58.97
Save model weight!

Epoch 9|20:
Batch 1|1608: loss 0.0634 f1 66.67 precision 50.00 recall 100.00
Batch 101|1608: loss 0.0755 f1 57.14 precision 66.67 recall 50.00
Batch 201|1608: loss 0.0378 f1 80.00 precision 66.67 recall 100.00
Batch 301|1608: loss 0.0203 f1 80.00 precision 100.00 recall 66.67
Batch 401|1608: loss 0.0299 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0301 f1 66.67 precision 100.00 recall 50.00
Batch 601|1608: loss 0.0375 f1 90.91 precision 83.33 recall 100.00
Batch 701|1608: loss 0.1011 f1 0.00 precision 0.00 recall 0.00
Batch 801|1608: loss 0.2498 f1 60.00 precision 60.00 recall 60.00
Batch 901|1608: loss 0.0370 f1 0.00 precision 0.00 recall 0.00
Batch 1001|1608: loss 0.0170 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0279 f1 0.00 precision 0.00 recall 0.00
Batch 1201|1608: loss 0.1429 f1 40.00 precision 100.00 recall 25.00
Batch 1301|1608: loss 0.0371 f1 66.67 precision 100.00 recall 50.00
Batch 1401|1608: loss 0.0350 f1 80.00 precision 66.67 recall 100.00
Batch 1501|1608: loss 0.0325 f1 0.00 precision 0.00 recall 0.00
Batch 1601|1608: loss 0.0832 f1 60.00 precision 75.00 recall 50.00
Batch 1608|1608: loss 0.1165 f1 66.67 precision 66.67 recall 66.67
Train: loss 0.0558 f1 46.98 precision 55.82 recall 44.73
Time: 379.98
Dev: loss 0.0623 f1 53.80 precision 48.82 recall 59.92
Save model weight!

Epoch 10|20:
Batch 1|1608: loss 0.0501 f1 66.67 precision 60.00 recall 75.00
Batch 101|1608: loss 0.0367 f1 76.92 precision 71.43 recall 83.33
Batch 201|1608: loss 0.0378 f1 85.71 precision 75.00 recall 100.00
Batch 301|1608: loss 0.0098 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0223 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0141 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0188 f1 0.00 precision 0.00 recall 0.00
Batch 701|1608: loss 0.0606 f1 20.00 precision 25.00 recall 16.67
Batch 801|1608: loss 0.0100 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0466 f1 0.00 precision 0.00 recall 0.00
Batch 1001|1608: loss 0.0259 f1 85.71 precision 100.00 recall 75.00
Batch 1101|1608: loss 0.0199 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0469 f1 66.67 precision 100.00 recall 50.00
Batch 1301|1608: loss 0.0634 f1 50.00 precision 100.00 recall 33.33
Batch 1401|1608: loss 0.1023 f1 40.00 precision 100.00 recall 25.00
Batch 1501|1608: loss 0.0452 f1 85.71 precision 100.00 recall 75.00
Batch 1601|1608: loss 0.0493 f1 80.00 precision 100.00 recall 66.67
Batch 1608|1608: loss 0.0671 f1 50.00 precision 100.00 recall 33.33
Train: loss 0.0467 f1 54.88 precision 61.77 recall 53.76
Time: 380.23
Dev: loss 0.0609 f1 54.98 precision 48.35 recall 63.72
Save model weight!

Epoch 11|20:
Batch 1|1608: loss 0.0493 f1 76.92 precision 83.33 recall 71.43
Batch 101|1608: loss 0.0760 f1 33.33 precision 50.00 recall 25.00
Batch 201|1608: loss 0.0260 f1 66.67 precision 100.00 recall 50.00
Batch 301|1608: loss 0.0338 f1 66.67 precision 50.00 recall 100.00
Batch 401|1608: loss 0.0803 f1 66.67 precision 75.00 recall 60.00
Batch 501|1608: loss 0.0329 f1 66.67 precision 75.00 recall 60.00
Batch 601|1608: loss 0.0482 f1 80.00 precision 100.00 recall 66.67
Batch 701|1608: loss 0.0241 f1 66.67 precision 66.67 recall 66.67
Batch 801|1608: loss 0.0699 f1 75.00 precision 75.00 recall 75.00
Batch 901|1608: loss 0.0296 f1 90.91 precision 100.00 recall 83.33
Batch 1001|1608: loss 0.0875 f1 60.00 precision 75.00 recall 50.00
Batch 1101|1608: loss 0.0859 f1 50.00 precision 40.00 recall 66.67
Batch 1201|1608: loss 0.0291 f1 66.67 precision 66.67 recall 66.67
Batch 1301|1608: loss 0.0279 f1 75.00 precision 75.00 recall 75.00
Batch 1401|1608: loss 0.0043 f1 0.00 precision 0.00 recall 0.00
Batch 1501|1608: loss 0.0322 f1 50.00 precision 33.33 recall 100.00
Batch 1601|1608: loss 0.2043 f1 36.36 precision 40.00 recall 33.33
Batch 1608|1608: loss 0.0411 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0395 f1 60.97 precision 65.30 recall 60.97
Time: 379.53
Dev: loss 0.0608 f1 54.14 precision 48.15 recall 61.82

Epoch 12|20:
Batch 1|1608: loss 0.0219 f1 66.67 precision 66.67 recall 66.67
Batch 101|1608: loss 0.0236 f1 50.00 precision 33.33 recall 100.00
Batch 201|1608: loss 0.0345 f1 40.00 precision 33.33 recall 50.00
Batch 301|1608: loss 0.0391 f1 71.43 precision 62.50 recall 83.33
Batch 401|1608: loss 0.0312 f1 0.00 precision 0.00 recall 0.00
Batch 501|1608: loss 0.0897 f1 0.00 precision 0.00 recall 0.00
Batch 601|1608: loss 0.0553 f1 0.00 precision 0.00 recall 0.00
Batch 701|1608: loss 0.0363 f1 0.00 precision 0.00 recall 0.00
Batch 801|1608: loss 0.0203 f1 92.31 precision 85.71 recall 100.00
Batch 901|1608: loss 0.0496 f1 60.00 precision 50.00 recall 75.00
Batch 1001|1608: loss 0.0107 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0197 f1 85.71 precision 100.00 recall 75.00
Batch 1201|1608: loss 0.0184 f1 50.00 precision 50.00 recall 50.00
Batch 1301|1608: loss 0.0183 f1 80.00 precision 66.67 recall 100.00
Batch 1401|1608: loss 0.0094 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0235 f1 80.00 precision 66.67 recall 100.00
Batch 1601|1608: loss 0.0579 f1 57.14 precision 50.00 recall 66.67
Batch 1608|1608: loss 0.0369 f1 66.67 precision 100.00 recall 50.00
Train: loss 0.0339 f1 64.97 precision 68.03 recall 65.77
Time: 378.47
Dev: loss 0.0619 f1 56.97 precision 50.90 recall 64.67
Save model weight!

Epoch 13|20:
Batch 1|1608: loss 0.0518 f1 0.00 precision 0.00 recall 0.00
Batch 101|1608: loss 0.0146 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0288 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0455 f1 50.00 precision 50.00 recall 50.00
Batch 401|1608: loss 0.0150 f1 66.67 precision 100.00 recall 50.00
Batch 501|1608: loss 0.0328 f1 90.91 precision 83.33 recall 100.00
Batch 601|1608: loss 0.0029 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0226 f1 50.00 precision 50.00 recall 50.00
Batch 801|1608: loss 0.0637 f1 77.78 precision 77.78 recall 77.78
Batch 901|1608: loss 0.0119 f1 80.00 precision 100.00 recall 66.67
Batch 1001|1608: loss 0.0307 f1 80.00 precision 100.00 recall 66.67
Batch 1101|1608: loss 0.0380 f1 57.14 precision 66.67 recall 50.00
Batch 1201|1608: loss 0.0069 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0488 f1 66.67 precision 100.00 recall 50.00
Batch 1401|1608: loss 0.0237 f1 66.67 precision 60.00 recall 75.00
Batch 1501|1608: loss 0.0346 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0733 f1 50.00 precision 50.00 recall 50.00
Batch 1608|1608: loss 0.0044 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0302 f1 69.11 precision 71.56 recall 70.40
Time: 379.29
Dev: loss 0.0658 f1 55.92 precision 47.75 recall 67.47

Epoch 14|20:
Batch 1|1608: loss 0.0431 f1 88.89 precision 80.00 recall 100.00
Batch 101|1608: loss 0.0224 f1 50.00 precision 50.00 recall 50.00
Batch 201|1608: loss 0.0244 f1 66.67 precision 60.00 recall 75.00
Batch 301|1608: loss 0.0218 f1 50.00 precision 50.00 recall 50.00
Batch 401|1608: loss 0.0070 f1 0.00 precision 0.00 recall 0.00
Batch 501|1608: loss 0.0088 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0232 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0351 f1 66.67 precision 50.00 recall 100.00
Batch 801|1608: loss 0.0960 f1 44.44 precision 66.67 recall 33.33
Batch 901|1608: loss 0.0212 f1 80.00 precision 80.00 recall 80.00
Batch 1001|1608: loss 0.0140 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0064 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0083 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0029 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0380 f1 66.67 precision 66.67 recall 66.67
Batch 1501|1608: loss 0.0073 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0135 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0315 f1 66.67 precision 100.00 recall 50.00
Train: loss 0.0267 f1 71.61 precision 73.25 recall 73.09
Time: 378.61
Dev: loss 0.0644 f1 56.09 precision 49.42 recall 64.84

Epoch 15|20:
Batch 1|1608: loss 0.0291 f1 66.67 precision 66.67 recall 66.67
Batch 101|1608: loss 0.0368 f1 66.67 precision 60.00 recall 75.00
Batch 201|1608: loss 0.0373 f1 66.67 precision 75.00 recall 60.00
Batch 301|1608: loss 0.0084 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0161 f1 80.00 precision 100.00 recall 66.67
Batch 501|1608: loss 0.0143 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0029 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0425 f1 60.00 precision 50.00 recall 75.00
Batch 801|1608: loss 0.0242 f1 66.67 precision 75.00 recall 60.00
Batch 901|1608: loss 0.0390 f1 80.00 precision 80.00 recall 80.00
Batch 1001|1608: loss 0.0039 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0150 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0616 f1 80.00 precision 85.71 recall 75.00
Batch 1301|1608: loss 0.0544 f1 60.00 precision 75.00 recall 50.00
Batch 1401|1608: loss 0.0687 f1 75.00 precision 60.00 recall 100.00
Batch 1501|1608: loss 0.0485 f1 33.33 precision 25.00 recall 50.00
Batch 1601|1608: loss 0.0247 f1 66.67 precision 66.67 recall 66.67
Batch 1608|1608: loss 0.0621 f1 66.67 precision 66.67 recall 66.67
Train: loss 0.0239 f1 74.41 precision 75.62 recall 75.91
Time: 378.93
Dev: loss 0.0666 f1 58.21 precision 54.31 recall 62.72
Save model weight!

Epoch 16|20:
Batch 1|1608: loss 0.0101 f1 80.00 precision 100.00 recall 66.67
Batch 101|1608: loss 0.0078 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0060 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0175 f1 0.00 precision 0.00 recall 0.00
Batch 401|1608: loss 0.0413 f1 66.67 precision 66.67 recall 66.67
Batch 501|1608: loss 0.0166 f1 0.00 precision 0.00 recall 0.00
Batch 601|1608: loss 0.0252 f1 80.00 precision 80.00 recall 80.00
Batch 701|1608: loss 0.0387 f1 0.00 precision 0.00 recall 0.00
Batch 801|1608: loss 0.0212 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0076 f1 80.00 precision 66.67 recall 100.00
Batch 1001|1608: loss 0.0014 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0316 f1 83.33 precision 83.33 recall 83.33
Batch 1201|1608: loss 0.0073 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0344 f1 57.14 precision 50.00 recall 66.67
Batch 1401|1608: loss 0.0093 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0193 f1 40.00 precision 50.00 recall 33.33
Batch 1601|1608: loss 0.0293 f1 66.67 precision 100.00 recall 50.00
Batch 1608|1608: loss 0.0086 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0214 f1 77.33 precision 77.91 recall 79.40
Time: 380.12
Dev: loss 0.0695 f1 56.91 precision 50.43 recall 65.29

Epoch 17|20:
Batch 1|1608: loss 0.0093 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0104 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0378 f1 80.00 precision 80.00 recall 80.00
Batch 301|1608: loss 0.0118 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0076 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0168 f1 80.00 precision 100.00 recall 66.67
Batch 601|1608: loss 0.0249 f1 0.00 precision 0.00 recall 0.00
Batch 701|1608: loss 0.0598 f1 92.31 precision 85.71 recall 100.00
Batch 801|1608: loss 0.0103 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0185 f1 0.00 precision 0.00 recall 0.00
Batch 1001|1608: loss 0.0149 f1 66.67 precision 100.00 recall 50.00
Batch 1101|1608: loss 0.0238 f1 85.71 precision 100.00 recall 75.00
Batch 1201|1608: loss 0.0109 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0087 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0169 f1 85.71 precision 75.00 recall 100.00
Batch 1501|1608: loss 0.0030 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0204 f1 75.00 precision 100.00 recall 60.00
Batch 1608|1608: loss 0.0335 f1 33.33 precision 33.33 recall 33.33
Train: loss 0.0196 f1 78.80 precision 79.66 recall 80.17
Time: 379.72
Dev: loss 0.0704 f1 57.39 precision 50.66 recall 66.18

Epoch 18|20:
Batch 1|1608: loss 0.0076 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0106 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0163 f1 50.00 precision 50.00 recall 50.00
Batch 301|1608: loss 0.0366 f1 66.67 precision 66.67 recall 66.67
Batch 401|1608: loss 0.0472 f1 75.00 precision 75.00 recall 75.00
Batch 501|1608: loss 0.0402 f1 85.71 precision 100.00 recall 75.00
Batch 601|1608: loss 0.0161 f1 85.71 precision 75.00 recall 100.00
Batch 701|1608: loss 0.0049 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0208 f1 50.00 precision 50.00 recall 50.00
Batch 901|1608: loss 0.0049 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0198 f1 66.67 precision 60.00 recall 75.00
Batch 1101|1608: loss 0.0173 f1 83.33 precision 83.33 recall 83.33
Batch 1201|1608: loss 0.0030 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0071 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0199 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0137 f1 75.00 precision 75.00 recall 75.00
Batch 1601|1608: loss 0.0005 f1 0.00 precision 0.00 recall 0.00
Batch 1608|1608: loss 0.0121 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0184 f1 80.03 precision 80.76 recall 81.58
Time: 379.87
Dev: loss 0.0710 f1 57.59 precision 52.15 recall 64.28

Epoch 19|20:
Batch 1|1608: loss 0.0038 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0319 f1 80.00 precision 80.00 recall 80.00
Batch 201|1608: loss 0.0589 f1 57.14 precision 66.67 recall 50.00
Batch 301|1608: loss 0.0142 f1 85.71 precision 85.71 recall 85.71
Batch 401|1608: loss 0.0173 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0208 f1 0.00 precision 0.00 recall 0.00
Batch 601|1608: loss 0.0228 f1 85.71 precision 100.00 recall 75.00
Batch 701|1608: loss 0.0545 f1 72.73 precision 80.00 recall 66.67
Batch 801|1608: loss 0.0041 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0075 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0158 f1 85.71 precision 100.00 recall 75.00
Batch 1101|1608: loss 0.0226 f1 90.91 precision 100.00 recall 83.33
Batch 1201|1608: loss 0.0127 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0065 f1 66.67 precision 66.67 recall 66.67
Batch 1401|1608: loss 0.0154 f1 50.00 precision 50.00 recall 50.00
Batch 1501|1608: loss 0.0486 f1 54.55 precision 50.00 recall 60.00
Batch 1601|1608: loss 0.0061 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0285 f1 50.00 precision 50.00 recall 50.00
Train: loss 0.0171 f1 80.86 precision 81.10 recall 82.48
Time: 379.37
Dev: loss 0.0732 f1 58.01 precision 51.71 recall 66.07

Epoch 20|20:
Batch 1|1608: loss 0.0135 f1 88.89 precision 100.00 recall 80.00
Batch 101|1608: loss 0.0005 f1 0.00 precision 0.00 recall 0.00
Batch 201|1608: loss 0.0110 f1 85.71 precision 75.00 recall 100.00
Batch 301|1608: loss 0.0260 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0070 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0195 f1 66.67 precision 50.00 recall 100.00
Batch 601|1608: loss 0.0158 f1 88.89 precision 100.00 recall 80.00
Batch 701|1608: loss 0.0103 f1 50.00 precision 50.00 recall 50.00
Batch 801|1608: loss 0.0242 f1 80.00 precision 66.67 recall 100.00
Batch 901|1608: loss 0.0050 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0133 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0268 f1 80.00 precision 75.00 recall 85.71
Batch 1201|1608: loss 0.0336 f1 40.00 precision 33.33 recall 50.00
Batch 1301|1608: loss 0.0062 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0220 f1 50.00 precision 50.00 recall 50.00
Batch 1501|1608: loss 0.0028 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0190 f1 88.89 precision 100.00 recall 80.00
Batch 1608|1608: loss 0.0097 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0165 f1 81.68 precision 82.30 recall 83.16
Time: 379.13
Dev: loss 0.0739 f1 57.72 precision 51.75 recall 65.23

Restore best model!
Test: f1 59.26 precision 55.26 recall 63.88
