Namespace(no_train=False, train_batch_size=8, eval_batch_size=8, lr=0.0001, num_epochs=20, log_step=100, no_cuda=False, word_embedding_dim=300, use_pretrained=True, use_postag=False, postag_embedding_dim=25, hidden_size=512, dropout_rate=0.5)
Epoch 1|20:
Batch 1|1608: loss 3.4875 f1 0.00 precision 0.00 recall 0.00
Batch 101|1608: loss 0.1524 f1 0.00 precision 0.00 recall 0.00
Batch 201|1608: loss 0.2850 f1 0.00 precision 0.00 recall 0.00
Batch 301|1608: loss 0.2771 f1 0.00 precision 0.00 recall 0.00
Batch 401|1608: loss 0.1114 f1 0.00 precision 0.00 recall 0.00
Batch 501|1608: loss 0.0781 f1 0.00 precision 0.00 recall 0.00
Batch 601|1608: loss 0.0675 f1 0.00 precision 0.00 recall 0.00
Batch 701|1608: loss 0.1773 f1 33.33 precision 100.00 recall 20.00
Batch 801|1608: loss 0.0229 f1 0.00 precision 0.00 recall 0.00
Batch 901|1608: loss 0.2214 f1 0.00 precision 0.00 recall 0.00
Batch 1001|1608: loss 0.0668 f1 50.00 precision 50.00 recall 50.00
Batch 1101|1608: loss 0.1354 f1 40.00 precision 50.00 recall 33.33
Batch 1201|1608: loss 0.0422 f1 0.00 precision 0.00 recall 0.00
Batch 1301|1608: loss 0.2143 f1 22.22 precision 50.00 recall 14.29
Batch 1401|1608: loss 0.0199 f1 0.00 precision 0.00 recall 0.00
Batch 1501|1608: loss 0.0612 f1 80.00 precision 100.00 recall 66.67
Batch 1601|1608: loss 0.1430 f1 57.14 precision 100.00 recall 40.00
Batch 1608|1608: loss 0.0159 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.1612 f1 13.67 precision 22.00 recall 11.06
Time: 35.30
Dev: loss 0.0756 f1 35.15 precision 63.41 recall 24.32
Save model weight!

Epoch 2|20:
Batch 1|1608: loss 0.0457 f1 40.00 precision 33.33 recall 50.00
Batch 101|1608: loss 0.0683 f1 33.33 precision 50.00 recall 25.00
Batch 201|1608: loss 0.0209 f1 0.00 precision 0.00 recall 0.00
Batch 301|1608: loss 0.1088 f1 28.57 precision 100.00 recall 16.67
Batch 401|1608: loss 0.0255 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0414 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0633 f1 50.00 precision 50.00 recall 50.00
Batch 701|1608: loss 0.0881 f1 0.00 precision 0.00 recall 0.00
Batch 801|1608: loss 0.0561 f1 0.00 precision 0.00 recall 0.00
Batch 901|1608: loss 0.0392 f1 85.71 precision 100.00 recall 75.00
Batch 1001|1608: loss 0.0450 f1 0.00 precision 0.00 recall 0.00
Batch 1101|1608: loss 0.0974 f1 20.00 precision 33.33 recall 14.29
Batch 1201|1608: loss 0.0185 f1 0.00 precision 0.00 recall 0.00
Batch 1301|1608: loss 0.0374 f1 40.00 precision 33.33 recall 50.00
Batch 1401|1608: loss 0.1364 f1 66.67 precision 71.43 recall 62.50
Batch 1501|1608: loss 0.0112 f1 0.00 precision 0.00 recall 0.00
Batch 1601|1608: loss 0.0678 f1 0.00 precision 0.00 recall 0.00
Batch 1608|1608: loss 0.0449 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0747 f1 36.84 precision 48.64 recall 32.84
Time: 34.77
Dev: loss 0.0615 f1 49.07 precision 58.93 recall 42.03
Save model weight!

Epoch 3|20:
Batch 1|1608: loss 0.1115 f1 47.62 precision 55.56 recall 41.67
Batch 101|1608: loss 0.0831 f1 50.00 precision 50.00 recall 50.00
Batch 201|1608: loss 0.0655 f1 80.00 precision 100.00 recall 66.67
Batch 301|1608: loss 0.0762 f1 50.00 precision 100.00 recall 33.33
Batch 401|1608: loss 0.0539 f1 72.73 precision 100.00 recall 57.14
Batch 501|1608: loss 0.0325 f1 66.67 precision 50.00 recall 100.00
Batch 601|1608: loss 0.0691 f1 50.00 precision 100.00 recall 33.33
Batch 701|1608: loss 0.0382 f1 50.00 precision 33.33 recall 100.00
Batch 801|1608: loss 0.0314 f1 40.00 precision 33.33 recall 50.00
Batch 901|1608: loss 0.0139 f1 66.67 precision 50.00 recall 100.00
Batch 1001|1608: loss 0.0794 f1 57.14 precision 66.67 recall 50.00
Batch 1101|1608: loss 0.0486 f1 0.00 precision 0.00 recall 0.00
Batch 1201|1608: loss 0.0634 f1 66.67 precision 75.00 recall 60.00
Batch 1301|1608: loss 0.0767 f1 50.00 precision 66.67 recall 40.00
Batch 1401|1608: loss 0.0345 f1 66.67 precision 50.00 recall 100.00
Batch 1501|1608: loss 0.0485 f1 0.00 precision 0.00 recall 0.00
Batch 1601|1608: loss 0.0862 f1 18.18 precision 20.00 recall 16.67
Batch 1608|1608: loss 0.1975 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0628 f1 43.76 precision 53.61 recall 41.17
Time: 37.77
Dev: loss 0.0572 f1 48.18 precision 59.77 recall 40.36

Epoch 4|20:
Batch 1|1608: loss 0.0859 f1 33.33 precision 100.00 recall 20.00
Batch 101|1608: loss 0.0301 f1 57.14 precision 66.67 recall 50.00
Batch 201|1608: loss 0.0325 f1 0.00 precision 0.00 recall 0.00
Batch 301|1608: loss 0.0999 f1 33.33 precision 50.00 recall 25.00
Batch 401|1608: loss 0.0629 f1 22.22 precision 33.33 recall 16.67
Batch 501|1608: loss 0.0197 f1 80.00 precision 100.00 recall 66.67
Batch 601|1608: loss 0.0276 f1 85.71 precision 75.00 recall 100.00
Batch 701|1608: loss 0.0981 f1 40.00 precision 100.00 recall 25.00
Batch 801|1608: loss 0.0653 f1 90.91 precision 100.00 recall 83.33
Batch 901|1608: loss 0.0200 f1 0.00 precision 0.00 recall 0.00
Batch 1001|1608: loss 0.0214 f1 80.00 precision 66.67 recall 100.00
Batch 1101|1608: loss 0.0243 f1 66.67 precision 66.67 recall 66.67
Batch 1201|1608: loss 0.0674 f1 46.15 precision 42.86 recall 50.00
Batch 1301|1608: loss 0.1229 f1 25.00 precision 25.00 recall 25.00
Batch 1401|1608: loss 0.0578 f1 0.00 precision 0.00 recall 0.00
Batch 1501|1608: loss 0.2198 f1 0.00 precision 0.00 recall 0.00
Batch 1601|1608: loss 0.0945 f1 60.00 precision 60.00 recall 60.00
Batch 1608|1608: loss 0.0543 f1 66.67 precision 66.67 recall 66.67
Train: loss 0.0546 f1 47.88 precision 57.05 recall 45.78
Time: 35.58
Dev: loss 0.0560 f1 53.34 precision 54.35 recall 52.38
Save model weight!

Epoch 5|20:
Batch 1|1608: loss 0.0377 f1 80.00 precision 66.67 recall 100.00
Batch 101|1608: loss 0.0748 f1 66.67 precision 100.00 recall 50.00
Batch 201|1608: loss 0.0789 f1 42.86 precision 50.00 recall 37.50
Batch 301|1608: loss 0.0782 f1 66.67 precision 66.67 recall 66.67
Batch 401|1608: loss 0.0383 f1 85.71 precision 75.00 recall 100.00
Batch 501|1608: loss 0.1253 f1 33.33 precision 50.00 recall 25.00
Batch 601|1608: loss 0.1334 f1 40.00 precision 40.00 recall 40.00
Batch 701|1608: loss 0.1664 f1 50.00 precision 100.00 recall 33.33
Batch 801|1608: loss 0.0234 f1 66.67 precision 100.00 recall 50.00
Batch 901|1608: loss 0.0265 f1 66.67 precision 66.67 recall 66.67
Batch 1001|1608: loss 0.0269 f1 50.00 precision 50.00 recall 50.00
Batch 1101|1608: loss 0.0330 f1 0.00 precision 0.00 recall 0.00
Batch 1201|1608: loss 0.0398 f1 0.00 precision 0.00 recall 0.00
Batch 1301|1608: loss 0.0281 f1 0.00 precision 0.00 recall 0.00
Batch 1401|1608: loss 0.0428 f1 57.14 precision 100.00 recall 40.00
Batch 1501|1608: loss 0.0165 f1 80.00 precision 66.67 recall 100.00
Batch 1601|1608: loss 0.0621 f1 40.00 precision 33.33 recall 50.00
Batch 1608|1608: loss 0.0200 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0496 f1 50.80 precision 58.64 recall 49.36
Time: 35.88
Dev: loss 0.0541 f1 52.50 precision 57.44 recall 48.35

Epoch 6|20:
Batch 1|1608: loss 0.0202 f1 85.71 precision 75.00 recall 100.00
Batch 101|1608: loss 0.0123 f1 66.67 precision 50.00 recall 100.00
Batch 201|1608: loss 0.0645 f1 33.33 precision 50.00 recall 25.00
Batch 301|1608: loss 0.0101 f1 0.00 precision 0.00 recall 0.00
Batch 401|1608: loss 0.1194 f1 61.54 precision 66.67 recall 57.14
Batch 501|1608: loss 0.0519 f1 66.67 precision 62.50 recall 71.43
Batch 601|1608: loss 0.0595 f1 66.67 precision 66.67 recall 66.67
Batch 701|1608: loss 0.0458 f1 66.67 precision 80.00 recall 57.14
Batch 801|1608: loss 0.0223 f1 80.00 precision 100.00 recall 66.67
Batch 901|1608: loss 0.0198 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0199 f1 66.67 precision 100.00 recall 50.00
Batch 1101|1608: loss 0.0480 f1 50.00 precision 50.00 recall 50.00
Batch 1201|1608: loss 0.0257 f1 80.00 precision 80.00 recall 80.00
Batch 1301|1608: loss 0.0416 f1 54.55 precision 60.00 recall 50.00
Batch 1401|1608: loss 0.0993 f1 84.21 precision 100.00 recall 72.73
Batch 1501|1608: loss 0.0209 f1 85.71 precision 75.00 recall 100.00
Batch 1601|1608: loss 0.0641 f1 57.14 precision 66.67 recall 50.00
Batch 1608|1608: loss 0.2217 f1 33.33 precision 50.00 recall 25.00
Train: loss 0.0438 f1 53.41 precision 60.79 recall 51.99
Time: 35.68
Dev: loss 0.0546 f1 51.16 precision 58.06 recall 45.72

Epoch 7|20:
Batch 1|1608: loss 0.0018 f1 0.00 precision 0.00 recall 0.00
Batch 101|1608: loss 0.0162 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0531 f1 85.71 precision 100.00 recall 75.00
Batch 301|1608: loss 0.0263 f1 75.00 precision 75.00 recall 75.00
Batch 401|1608: loss 0.0119 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0094 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0267 f1 80.00 precision 100.00 recall 66.67
Batch 701|1608: loss 0.0426 f1 33.33 precision 33.33 recall 33.33
Batch 801|1608: loss 0.0297 f1 90.91 precision 83.33 recall 100.00
Batch 901|1608: loss 0.1233 f1 0.00 precision 0.00 recall 0.00
Batch 1001|1608: loss 0.0149 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0069 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0073 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0970 f1 50.00 precision 66.67 recall 40.00
Batch 1401|1608: loss 0.0198 f1 0.00 precision 0.00 recall 0.00
Batch 1501|1608: loss 0.0407 f1 88.89 precision 100.00 recall 80.00
Batch 1601|1608: loss 0.0223 f1 66.67 precision 100.00 recall 50.00
Batch 1608|1608: loss 0.1684 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0392 f1 56.50 precision 63.23 recall 55.61
Time: 35.75
Dev: loss 0.0543 f1 53.54 precision 56.17 recall 51.15
Save model weight!

Epoch 8|20:
Batch 1|1608: loss 0.0051 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0179 f1 80.00 precision 80.00 recall 80.00
Batch 201|1608: loss 0.0427 f1 80.00 precision 100.00 recall 66.67
Batch 301|1608: loss 0.0305 f1 66.67 precision 66.67 recall 66.67
Batch 401|1608: loss 0.0044 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0171 f1 66.67 precision 66.67 recall 66.67
Batch 601|1608: loss 0.1031 f1 50.00 precision 100.00 recall 33.33
Batch 701|1608: loss 0.0249 f1 0.00 precision 0.00 recall 0.00
Batch 801|1608: loss 0.0158 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0107 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0259 f1 66.67 precision 66.67 recall 66.67
Batch 1101|1608: loss 0.0424 f1 60.00 precision 75.00 recall 50.00
Batch 1201|1608: loss 0.0496 f1 61.54 precision 57.14 recall 66.67
Batch 1301|1608: loss 0.0028 f1 0.00 precision 0.00 recall 0.00
Batch 1401|1608: loss 0.0176 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0773 f1 80.00 precision 80.00 recall 80.00
Batch 1601|1608: loss 0.0108 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0171 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0344 f1 61.16 precision 66.95 recall 60.44
Time: 35.85
Dev: loss 0.0565 f1 53.23 precision 57.92 recall 49.25

Epoch 9|20:
Batch 1|1608: loss 0.0183 f1 57.14 precision 66.67 recall 50.00
Batch 101|1608: loss 0.0291 f1 33.33 precision 33.33 recall 33.33
Batch 201|1608: loss 0.0348 f1 85.71 precision 100.00 recall 75.00
Batch 301|1608: loss 0.0244 f1 50.00 precision 33.33 recall 100.00
Batch 401|1608: loss 0.0623 f1 0.00 precision 0.00 recall 0.00
Batch 501|1608: loss 0.0014 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0368 f1 0.00 precision 0.00 recall 0.00
Batch 701|1608: loss 0.0127 f1 80.00 precision 100.00 recall 66.67
Batch 801|1608: loss 0.0222 f1 50.00 precision 50.00 recall 50.00
Batch 901|1608: loss 0.0248 f1 66.67 precision 100.00 recall 50.00
Batch 1001|1608: loss 0.0529 f1 0.00 precision 0.00 recall 0.00
Batch 1101|1608: loss 0.0128 f1 80.00 precision 100.00 recall 66.67
Batch 1201|1608: loss 0.0231 f1 57.14 precision 66.67 recall 50.00
Batch 1301|1608: loss 0.0017 f1 0.00 precision 0.00 recall 0.00
Batch 1401|1608: loss 0.0758 f1 18.18 precision 16.67 recall 20.00
Batch 1501|1608: loss 0.1313 f1 28.57 precision 50.00 recall 20.00
Batch 1601|1608: loss 0.0370 f1 66.67 precision 100.00 recall 50.00
Batch 1608|1608: loss 0.0184 f1 66.67 precision 66.67 recall 66.67
Train: loss 0.0300 f1 65.07 precision 70.23 recall 64.22
Time: 35.82
Dev: loss 0.0578 f1 53.94 precision 53.55 recall 54.33
Save model weight!

Epoch 10|20:
Batch 1|1608: loss 0.0249 f1 40.00 precision 50.00 recall 33.33
Batch 101|1608: loss 0.0332 f1 60.00 precision 50.00 recall 75.00
Batch 201|1608: loss 0.0395 f1 0.00 precision 0.00 recall 0.00
Batch 301|1608: loss 0.0229 f1 40.00 precision 50.00 recall 33.33
Batch 401|1608: loss 0.0206 f1 83.33 precision 83.33 recall 83.33
Batch 501|1608: loss 0.0145 f1 66.67 precision 50.00 recall 100.00
Batch 601|1608: loss 0.0251 f1 33.33 precision 25.00 recall 50.00
Batch 701|1608: loss 0.0243 f1 60.00 precision 60.00 recall 60.00
Batch 801|1608: loss 0.0710 f1 66.67 precision 66.67 recall 66.67
Batch 901|1608: loss 0.0524 f1 28.57 precision 20.00 recall 50.00
Batch 1001|1608: loss 0.0061 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0187 f1 80.00 precision 66.67 recall 100.00
Batch 1201|1608: loss 0.0621 f1 80.00 precision 100.00 recall 66.67
Batch 1301|1608: loss 0.0297 f1 66.67 precision 66.67 recall 66.67
Batch 1401|1608: loss 0.0063 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0085 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0103 f1 85.71 precision 75.00 recall 100.00
Batch 1608|1608: loss 0.0067 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0258 f1 68.30 precision 72.83 recall 67.89
Time: 35.75
Dev: loss 0.0591 f1 54.48 precision 56.63 recall 52.49
Save model weight!

Epoch 11|20:
Batch 1|1608: loss 0.0305 f1 88.89 precision 100.00 recall 80.00
Batch 101|1608: loss 0.0991 f1 46.15 precision 42.86 recall 50.00
Batch 201|1608: loss 0.0363 f1 80.00 precision 80.00 recall 80.00
Batch 301|1608: loss 0.0089 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0122 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0078 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0079 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0507 f1 82.35 precision 77.78 recall 87.50
Batch 801|1608: loss 0.0345 f1 66.67 precision 66.67 recall 66.67
Batch 901|1608: loss 0.0297 f1 57.14 precision 50.00 recall 66.67
Batch 1001|1608: loss 0.0242 f1 72.73 precision 80.00 recall 66.67
Batch 1101|1608: loss 0.0150 f1 85.71 precision 100.00 recall 75.00
Batch 1201|1608: loss 0.0349 f1 72.73 precision 80.00 recall 66.67
Batch 1301|1608: loss 0.0192 f1 85.71 precision 100.00 recall 75.00
Batch 1401|1608: loss 0.0721 f1 44.44 precision 40.00 recall 50.00
Batch 1501|1608: loss 0.0142 f1 50.00 precision 50.00 recall 50.00
Batch 1601|1608: loss 0.0327 f1 80.00 precision 85.71 recall 75.00
Batch 1608|1608: loss 0.0384 f1 66.67 precision 50.00 recall 100.00
Train: loss 0.0222 f1 73.17 precision 76.85 recall 72.82
Time: 35.77
Dev: loss 0.0629 f1 54.28 precision 51.75 recall 57.07

Epoch 12|20:
Batch 1|1608: loss 0.0058 f1 0.00 precision 0.00 recall 0.00
Batch 101|1608: loss 0.0152 f1 80.00 precision 100.00 recall 66.67
Batch 201|1608: loss 0.0258 f1 85.71 precision 100.00 recall 75.00
Batch 301|1608: loss 0.0015 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0226 f1 0.00 precision 0.00 recall 0.00
Batch 501|1608: loss 0.0288 f1 66.67 precision 100.00 recall 50.00
Batch 601|1608: loss 0.0226 f1 50.00 precision 50.00 recall 50.00
Batch 701|1608: loss 0.0016 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0164 f1 66.67 precision 50.00 recall 100.00
Batch 901|1608: loss 0.0092 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0596 f1 44.44 precision 50.00 recall 40.00
Batch 1101|1608: loss 0.0037 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0090 f1 85.71 precision 100.00 recall 75.00
Batch 1301|1608: loss 0.0230 f1 75.00 precision 75.00 recall 75.00
Batch 1401|1608: loss 0.0090 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0107 f1 80.00 precision 66.67 recall 100.00
Batch 1601|1608: loss 0.0228 f1 75.00 precision 75.00 recall 75.00
Batch 1608|1608: loss 0.0081 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0192 f1 74.98 precision 78.38 recall 74.52
Time: 35.79
Dev: loss 0.0647 f1 52.50 precision 57.35 recall 48.41

Epoch 13|20:
Batch 1|1608: loss 0.0049 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0149 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0180 f1 88.89 precision 80.00 recall 100.00
Batch 301|1608: loss 0.0126 f1 90.91 precision 100.00 recall 83.33
Batch 401|1608: loss 0.0023 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0206 f1 80.00 precision 66.67 recall 100.00
Batch 601|1608: loss 0.0037 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0069 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0022 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0091 f1 92.31 precision 100.00 recall 85.71
Batch 1001|1608: loss 0.0084 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0311 f1 80.00 precision 100.00 recall 66.67
Batch 1201|1608: loss 0.0164 f1 57.14 precision 100.00 recall 40.00
Batch 1301|1608: loss 0.0019 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0567 f1 80.00 precision 75.00 recall 85.71
Batch 1501|1608: loss 0.0069 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0112 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0057 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0171 f1 77.59 precision 80.38 recall 77.60
Time: 35.70
Dev: loss 0.0705 f1 50.84 precision 56.23 recall 46.39

Epoch 14|20:
Batch 1|1608: loss 0.0087 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0029 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0801 f1 44.44 precision 50.00 recall 40.00
Batch 301|1608: loss 0.0091 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0468 f1 88.89 precision 80.00 recall 100.00
Batch 501|1608: loss 0.0110 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0794 f1 57.14 precision 40.00 recall 100.00
Batch 701|1608: loss 0.0036 f1 0.00 precision 0.00 recall 0.00
Batch 801|1608: loss 0.0518 f1 57.14 precision 66.67 recall 50.00
Batch 901|1608: loss 0.0047 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0393 f1 85.71 precision 100.00 recall 75.00
Batch 1101|1608: loss 0.0739 f1 50.00 precision 66.67 recall 40.00
Batch 1201|1608: loss 0.0172 f1 85.71 precision 75.00 recall 100.00
Batch 1301|1608: loss 0.0149 f1 0.00 precision 0.00 recall 0.00
Batch 1401|1608: loss 0.0367 f1 50.00 precision 33.33 recall 100.00
Batch 1501|1608: loss 0.0107 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0277 f1 66.67 precision 100.00 recall 50.00
Batch 1608|1608: loss 0.0034 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0154 f1 79.69 precision 81.93 recall 79.89
Time: 35.67
Dev: loss 0.0694 f1 53.33 precision 50.76 recall 56.18

Epoch 15|20:
Batch 1|1608: loss 0.0240 f1 50.00 precision 33.33 recall 100.00
Batch 101|1608: loss 0.0118 f1 75.00 precision 75.00 recall 75.00
Batch 201|1608: loss 0.0023 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0092 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0086 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0017 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0365 f1 66.67 precision 60.00 recall 75.00
Batch 701|1608: loss 0.0169 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0206 f1 72.73 precision 66.67 recall 80.00
Batch 901|1608: loss 0.0282 f1 50.00 precision 33.33 recall 100.00
Batch 1001|1608: loss 0.0344 f1 85.71 precision 100.00 recall 75.00
Batch 1101|1608: loss 0.0151 f1 66.67 precision 100.00 recall 50.00
Batch 1201|1608: loss 0.0040 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0061 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0025 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0109 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0009 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0138 f1 66.67 precision 66.67 recall 66.67
Train: loss 0.0135 f1 80.46 precision 82.09 recall 80.74
Time: 35.69
Dev: loss 0.0727 f1 52.72 precision 53.71 recall 51.76

Epoch 16|20:
Batch 1|1608: loss 0.0103 f1 66.67 precision 50.00 recall 100.00
Batch 101|1608: loss 0.0056 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0051 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0175 f1 66.67 precision 66.67 recall 66.67
Batch 401|1608: loss 0.0303 f1 33.33 precision 25.00 recall 50.00
Batch 501|1608: loss 0.0101 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0044 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0003 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0055 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0022 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0098 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0128 f1 75.00 precision 75.00 recall 75.00
Batch 1201|1608: loss 0.0072 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0760 f1 66.67 precision 80.00 recall 57.14
Batch 1401|1608: loss 0.0062 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0130 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0050 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.1525 f1 76.92 precision 100.00 recall 62.50
Train: loss 0.0127 f1 82.89 precision 84.59 recall 83.04
Time: 35.74
Dev: loss 0.0733 f1 53.37 precision 55.87 recall 51.09

Epoch 17|20:
Batch 1|1608: loss 0.0153 f1 87.50 precision 87.50 recall 87.50
Batch 101|1608: loss 0.0004 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0156 f1 50.00 precision 50.00 recall 50.00
Batch 301|1608: loss 0.0014 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0014 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0019 f1 0.00 precision 0.00 recall 0.00
Batch 601|1608: loss 0.0007 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0120 f1 88.89 precision 80.00 recall 100.00
Batch 801|1608: loss 0.0065 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0044 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0191 f1 80.00 precision 66.67 recall 100.00
Batch 1101|1608: loss 0.0047 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0012 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0179 f1 85.71 precision 75.00 recall 100.00
Batch 1401|1608: loss 0.0084 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0026 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0290 f1 90.91 precision 83.33 recall 100.00
Batch 1608|1608: loss 0.0047 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0118 f1 83.57 precision 85.27 recall 83.69
Time: 35.75
Dev: loss 0.0785 f1 52.21 precision 57.94 recall 47.51

Epoch 18|20:
Batch 1|1608: loss 0.0012 f1 0.00 precision 0.00 recall 0.00
Batch 101|1608: loss 0.0023 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0141 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0754 f1 66.67 precision 100.00 recall 50.00
Batch 401|1608: loss 0.0047 f1 75.00 precision 75.00 recall 75.00
Batch 501|1608: loss 0.0054 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0278 f1 80.00 precision 66.67 recall 100.00
Batch 701|1608: loss 0.0109 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0167 f1 85.71 precision 75.00 recall 100.00
Batch 901|1608: loss 0.0114 f1 75.00 precision 75.00 recall 75.00
Batch 1001|1608: loss 0.0108 f1 50.00 precision 50.00 recall 50.00
Batch 1101|1608: loss 0.0001 f1 0.00 precision 0.00 recall 0.00
Batch 1201|1608: loss 0.0036 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0014 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0133 f1 66.67 precision 66.67 recall 66.67
Batch 1501|1608: loss 0.0119 f1 80.00 precision 80.00 recall 80.00
Batch 1601|1608: loss 0.0187 f1 88.89 precision 100.00 recall 80.00
Batch 1608|1608: loss 0.0066 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0108 f1 84.99 precision 86.03 recall 85.48
Time: 35.88
Dev: loss 0.0782 f1 52.84 precision 53.66 recall 52.04

Epoch 19|20:
Batch 1|1608: loss 0.0154 f1 80.00 precision 100.00 recall 66.67
Batch 101|1608: loss 0.0095 f1 90.91 precision 83.33 recall 100.00
Batch 201|1608: loss 0.0123 f1 90.91 precision 100.00 recall 83.33
Batch 301|1608: loss 0.0018 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.1014 f1 36.36 precision 25.00 recall 66.67
Batch 501|1608: loss 0.0035 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0009 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0179 f1 80.00 precision 100.00 recall 66.67
Batch 801|1608: loss 0.0008 f1 0.00 precision 0.00 recall 0.00
Batch 901|1608: loss 0.0006 f1 0.00 precision 0.00 recall 0.00
Batch 1001|1608: loss 0.0044 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0105 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0029 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0096 f1 85.71 precision 75.00 recall 100.00
Batch 1401|1608: loss 0.0052 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0191 f1 66.67 precision 66.67 recall 66.67
Batch 1601|1608: loss 0.0101 f1 66.67 precision 66.67 recall 66.67
Batch 1608|1608: loss 0.0110 f1 66.67 precision 50.00 recall 100.00
Train: loss 0.0105 f1 85.20 precision 86.27 recall 85.82
Time: 35.68
Dev: loss 0.0800 f1 52.45 precision 55.53 recall 49.69

Epoch 20|20:
Batch 1|1608: loss 0.0116 f1 85.71 precision 75.00 recall 100.00
Batch 101|1608: loss 0.0067 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0005 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0006 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0051 f1 66.67 precision 100.00 recall 50.00
Batch 501|1608: loss 0.0011 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0163 f1 66.67 precision 100.00 recall 50.00
Batch 701|1608: loss 0.0270 f1 93.33 precision 87.50 recall 100.00
Batch 801|1608: loss 0.0172 f1 28.57 precision 20.00 recall 50.00
Batch 901|1608: loss 0.0005 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0015 f1 0.00 precision 0.00 recall 0.00
Batch 1101|1608: loss 0.0294 f1 80.00 precision 80.00 recall 80.00
Batch 1201|1608: loss 0.0522 f1 75.00 precision 60.00 recall 100.00
Batch 1301|1608: loss 0.0014 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0332 f1 66.67 precision 50.00 recall 100.00
Batch 1501|1608: loss 0.0031 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0219 f1 88.89 precision 88.89 recall 88.89
Batch 1608|1608: loss 0.0043 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0100 f1 85.99 precision 87.14 recall 86.50
Time: 35.64
Dev: loss 0.0826 f1 52.94 precision 56.14 recall 50.08

Restore best model !
Test: f1 55.77 precision 59.33 recall 52.61
Namespace(no_train=False, train_batch_size=8, eval_batch_size=8, lr=0.0001, num_epochs=30, log_step=100, no_cuda=False, word_embedding_dim=300, use_pretrained=True, use_postag=False, postag_embedding_dim=25, hidden_size=512, dropout_rate=0.5)
Epoch 1|30:
Batch 1|1608: loss 3.5167 f1 0.00 precision 0.00 recall 0.00
Batch 101|1608: loss 0.1681 f1 0.00 precision 0.00 recall 0.00
Batch 201|1608: loss 0.1938 f1 0.00 precision 0.00 recall 0.00
Batch 301|1608: loss 0.1620 f1 0.00 precision 0.00 recall 0.00
Batch 401|1608: loss 0.2967 f1 0.00 precision 0.00 recall 0.00
Batch 501|1608: loss 0.1033 f1 0.00 precision 0.00 recall 0.00
Batch 601|1608: loss 0.1065 f1 0.00 precision 0.00 recall 0.00
Batch 701|1608: loss 0.0132 f1 0.00 precision 0.00 recall 0.00
Batch 801|1608: loss 0.0562 f1 0.00 precision 0.00 recall 0.00
Batch 901|1608: loss 0.0813 f1 0.00 precision 0.00 recall 0.00
Batch 1001|1608: loss 0.0576 f1 40.00 precision 100.00 recall 25.00
Batch 1101|1608: loss 0.1319 f1 50.00 precision 100.00 recall 33.33
Batch 1201|1608: loss 0.0814 f1 50.00 precision 50.00 recall 50.00
Batch 1301|1608: loss 0.1085 f1 0.00 precision 0.00 recall 0.00
Batch 1401|1608: loss 0.1221 f1 0.00 precision 0.00 recall 0.00
Batch 1501|1608: loss 0.0612 f1 0.00 precision 0.00 recall 0.00
Batch 1601|1608: loss 0.0775 f1 0.00 precision 0.00 recall 0.00
Batch 1608|1608: loss 0.0064 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.1607 f1 14.60 precision 22.90 recall 11.92
Time: 35.06
Dev: loss 0.0756 f1 29.51 precision 68.08 recall 18.84
Save model weight!

Epoch 2|30:
Batch 1|1608: loss 0.0336 f1 0.00 precision 0.00 recall 0.00
Batch 101|1608: loss 0.0559 f1 80.00 precision 100.00 recall 66.67
Batch 201|1608: loss 0.0770 f1 28.57 precision 33.33 recall 25.00
Batch 301|1608: loss 0.1606 f1 0.00 precision 0.00 recall 0.00
Batch 401|1608: loss 0.0165 f1 0.00 precision 0.00 recall 0.00
Batch 501|1608: loss 0.0725 f1 0.00 precision 0.00 recall 0.00
Batch 601|1608: loss 0.0393 f1 66.67 precision 66.67 recall 66.67
Batch 701|1608: loss 0.0919 f1 66.67 precision 100.00 recall 50.00
Batch 801|1608: loss 0.0892 f1 0.00 precision 0.00 recall 0.00
Batch 901|1608: loss 0.0666 f1 75.00 precision 75.00 recall 75.00
Batch 1001|1608: loss 0.0253 f1 0.00 precision 0.00 recall 0.00
Batch 1101|1608: loss 0.0579 f1 57.14 precision 66.67 recall 50.00
Batch 1201|1608: loss 0.0708 f1 40.00 precision 50.00 recall 33.33
Batch 1301|1608: loss 0.1007 f1 33.33 precision 50.00 recall 25.00
Batch 1401|1608: loss 0.0375 f1 50.00 precision 33.33 recall 100.00
Batch 1501|1608: loss 0.0231 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0578 f1 50.00 precision 33.33 recall 100.00
Batch 1608|1608: loss 0.0854 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0736 f1 36.80 precision 48.44 recall 33.22
Time: 35.25
Dev: loss 0.0615 f1 48.29 precision 58.78 recall 40.97
Save model weight!

Epoch 3|30:
Batch 1|1608: loss 0.0461 f1 0.00 precision 0.00 recall 0.00
Batch 101|1608: loss 0.1946 f1 50.00 precision 100.00 recall 33.33
Batch 201|1608: loss 0.0330 f1 0.00 precision 0.00 recall 0.00
Batch 301|1608: loss 0.0277 f1 66.67 precision 66.67 recall 66.67
Batch 401|1608: loss 0.0204 f1 0.00 precision 0.00 recall 0.00
Batch 501|1608: loss 0.0720 f1 44.44 precision 50.00 recall 40.00
Batch 601|1608: loss 0.1037 f1 22.22 precision 50.00 recall 14.29
Batch 701|1608: loss 0.0156 f1 0.00 precision 0.00 recall 0.00
Batch 801|1608: loss 0.0234 f1 0.00 precision 0.00 recall 0.00
Batch 901|1608: loss 0.0717 f1 33.33 precision 33.33 recall 33.33
Batch 1001|1608: loss 0.0453 f1 75.00 precision 75.00 recall 75.00
Batch 1101|1608: loss 0.0527 f1 28.57 precision 100.00 recall 16.67
Batch 1201|1608: loss 0.1104 f1 0.00 precision 0.00 recall 0.00
Batch 1301|1608: loss 0.0431 f1 0.00 precision 0.00 recall 0.00
Batch 1401|1608: loss 0.1096 f1 40.00 precision 100.00 recall 25.00
Batch 1501|1608: loss 0.0854 f1 66.67 precision 57.14 recall 80.00
Batch 1601|1608: loss 0.0207 f1 75.00 precision 75.00 recall 75.00
Batch 1608|1608: loss 0.0222 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0626 f1 44.01 precision 53.54 recall 41.74
Time: 36.88
Dev: loss 0.0591 f1 45.28 precision 68.52 recall 33.82

Epoch 4|30:
Batch 1|1608: loss 0.0120 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0319 f1 57.14 precision 100.00 recall 40.00
Batch 201|1608: loss 0.1012 f1 33.33 precision 33.33 recall 33.33
Batch 301|1608: loss 0.0345 f1 50.00 precision 50.00 recall 50.00
Batch 401|1608: loss 0.0369 f1 50.00 precision 50.00 recall 50.00
Batch 501|1608: loss 0.0350 f1 0.00 precision 0.00 recall 0.00
Batch 601|1608: loss 0.0359 f1 0.00 precision 0.00 recall 0.00
Batch 701|1608: loss 0.0405 f1 0.00 precision 0.00 recall 0.00
Batch 801|1608: loss 0.0397 f1 57.14 precision 50.00 recall 66.67
Batch 901|1608: loss 0.0695 f1 57.14 precision 66.67 recall 50.00
Batch 1001|1608: loss 0.0492 f1 57.14 precision 66.67 recall 50.00
Batch 1101|1608: loss 0.0196 f1 0.00 precision 0.00 recall 0.00
Batch 1201|1608: loss 0.1036 f1 0.00 precision 0.00 recall 0.00
Batch 1301|1608: loss 0.0133 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0443 f1 0.00 precision 0.00 recall 0.00
Batch 1501|1608: loss 0.0791 f1 60.00 precision 50.00 recall 75.00
Batch 1601|1608: loss 0.0280 f1 80.00 precision 100.00 recall 66.67
Batch 1608|1608: loss 0.1206 f1 33.33 precision 50.00 recall 25.00
Train: loss 0.0549 f1 48.00 precision 56.74 recall 45.97
Time: 36.73
Dev: loss 0.0560 f1 49.44 precision 60.93 recall 41.59
Save model weight!

Epoch 5|30:
Batch 1|1608: loss 0.0326 f1 33.33 precision 50.00 recall 25.00
Batch 101|1608: loss 0.0385 f1 57.14 precision 50.00 recall 66.67
Batch 201|1608: loss 0.0434 f1 72.73 precision 100.00 recall 57.14
Batch 301|1608: loss 0.0216 f1 66.67 precision 100.00 recall 50.00
Batch 401|1608: loss 0.0687 f1 40.00 precision 50.00 recall 33.33
Batch 501|1608: loss 0.0223 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0391 f1 66.67 precision 50.00 recall 100.00
Batch 701|1608: loss 0.1016 f1 42.86 precision 50.00 recall 37.50
Batch 801|1608: loss 0.0610 f1 25.00 precision 50.00 recall 16.67
Batch 901|1608: loss 0.0339 f1 50.00 precision 50.00 recall 50.00
Batch 1001|1608: loss 0.0249 f1 0.00 precision 0.00 recall 0.00
Batch 1101|1608: loss 0.0516 f1 33.33 precision 100.00 recall 20.00
Batch 1201|1608: loss 0.0757 f1 44.44 precision 50.00 recall 40.00
Batch 1301|1608: loss 0.0078 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0167 f1 66.67 precision 66.67 recall 66.67
Batch 1501|1608: loss 0.0190 f1 85.71 precision 75.00 recall 100.00
Batch 1601|1608: loss 0.0312 f1 80.00 precision 66.67 recall 100.00
Batch 1608|1608: loss 0.1580 f1 66.67 precision 100.00 recall 50.00
Train: loss 0.0490 f1 50.38 precision 58.57 recall 48.70
Time: 36.90
Dev: loss 0.0551 f1 50.91 precision 60.73 recall 43.82
Save model weight!

Epoch 6|30:
Batch 1|1608: loss 0.0901 f1 22.22 precision 25.00 recall 20.00
Batch 101|1608: loss 0.0662 f1 38.10 precision 40.00 recall 36.36
Batch 201|1608: loss 0.0232 f1 28.57 precision 25.00 recall 33.33
Batch 301|1608: loss 0.0381 f1 0.00 precision 0.00 recall 0.00
Batch 401|1608: loss 0.0162 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.1331 f1 0.00 precision 0.00 recall 0.00
Batch 601|1608: loss 0.0272 f1 33.33 precision 25.00 recall 50.00
Batch 701|1608: loss 0.0613 f1 66.67 precision 100.00 recall 50.00
Batch 801|1608: loss 0.0464 f1 40.00 precision 33.33 recall 50.00
Batch 901|1608: loss 0.0157 f1 80.00 precision 66.67 recall 100.00
Batch 1001|1608: loss 0.0435 f1 57.14 precision 66.67 recall 50.00
Batch 1101|1608: loss 0.0177 f1 0.00 precision 0.00 recall 0.00
Batch 1201|1608: loss 0.0126 f1 50.00 precision 50.00 recall 50.00
Batch 1301|1608: loss 0.0504 f1 0.00 precision 0.00 recall 0.00
Batch 1401|1608: loss 0.0356 f1 0.00 precision 0.00 recall 0.00
Batch 1501|1608: loss 0.0999 f1 53.33 precision 50.00 recall 57.14
Batch 1601|1608: loss 0.0569 f1 80.00 precision 85.71 recall 75.00
Batch 1608|1608: loss 0.0114 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0442 f1 53.60 precision 60.77 recall 52.49
Time: 36.78
Dev: loss 0.0539 f1 51.82 precision 59.69 recall 45.78
Save model weight!

Epoch 7|30:
Batch 1|1608: loss 0.0118 f1 66.67 precision 100.00 recall 50.00
Batch 101|1608: loss 0.0499 f1 80.00 precision 80.00 recall 80.00
Batch 201|1608: loss 0.0705 f1 66.67 precision 66.67 recall 66.67
Batch 301|1608: loss 0.0156 f1 66.67 precision 100.00 recall 50.00
Batch 401|1608: loss 0.0373 f1 66.67 precision 66.67 recall 66.67
Batch 501|1608: loss 0.0132 f1 0.00 precision 0.00 recall 0.00
Batch 601|1608: loss 0.0377 f1 0.00 precision 0.00 recall 0.00
Batch 701|1608: loss 0.0312 f1 80.00 precision 100.00 recall 66.67
Batch 801|1608: loss 0.0128 f1 0.00 precision 0.00 recall 0.00
Batch 901|1608: loss 0.0023 f1 0.00 precision 0.00 recall 0.00
Batch 1001|1608: loss 0.0533 f1 66.67 precision 75.00 recall 60.00
Batch 1101|1608: loss 0.0254 f1 0.00 precision 0.00 recall 0.00
Batch 1201|1608: loss 0.0305 f1 66.67 precision 100.00 recall 50.00
Batch 1301|1608: loss 0.0941 f1 54.55 precision 75.00 recall 42.86
Batch 1401|1608: loss 0.0686 f1 0.00 precision 0.00 recall 0.00
Batch 1501|1608: loss 0.0753 f1 71.43 precision 83.33 recall 62.50
Batch 1601|1608: loss 0.0069 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0605 f1 33.33 precision 33.33 recall 33.33
Train: loss 0.0393 f1 56.14 precision 62.60 recall 55.09
Time: 36.82
Dev: loss 0.0552 f1 52.91 precision 61.74 recall 46.28
Save model weight!

Epoch 8|30:
Batch 1|1608: loss 0.0121 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0141 f1 85.71 precision 75.00 recall 100.00
Batch 201|1608: loss 0.0287 f1 83.33 precision 83.33 recall 83.33
Batch 301|1608: loss 0.0205 f1 80.00 precision 66.67 recall 100.00
Batch 401|1608: loss 0.0133 f1 0.00 precision 0.00 recall 0.00
Batch 501|1608: loss 0.0163 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0192 f1 0.00 precision 0.00 recall 0.00
Batch 701|1608: loss 0.0533 f1 50.00 precision 50.00 recall 50.00
Batch 801|1608: loss 0.0172 f1 0.00 precision 0.00 recall 0.00
Batch 901|1608: loss 0.0616 f1 40.00 precision 50.00 recall 33.33
Batch 1001|1608: loss 0.0566 f1 66.67 precision 100.00 recall 50.00
Batch 1101|1608: loss 0.0075 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0466 f1 40.00 precision 50.00 recall 33.33
Batch 1301|1608: loss 0.0155 f1 50.00 precision 33.33 recall 100.00
Batch 1401|1608: loss 0.0700 f1 44.44 precision 33.33 recall 66.67
Batch 1501|1608: loss 0.1054 f1 50.00 precision 66.67 recall 40.00
Batch 1601|1608: loss 0.0189 f1 80.00 precision 100.00 recall 66.67
Batch 1608|1608: loss 0.1297 f1 50.00 precision 50.00 recall 50.00
Train: loss 0.0335 f1 61.31 precision 66.99 recall 60.50
Time: 36.95
Dev: loss 0.0552 f1 53.29 precision 56.52 recall 50.42
Save model weight!

Epoch 9|30:
Batch 1|1608: loss 0.0198 f1 75.00 precision 75.00 recall 75.00
Batch 101|1608: loss 0.0368 f1 83.33 precision 83.33 recall 83.33
Batch 201|1608: loss 0.0057 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0097 f1 66.67 precision 100.00 recall 50.00
Batch 401|1608: loss 0.0022 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0593 f1 50.00 precision 66.67 recall 40.00
Batch 601|1608: loss 0.0444 f1 76.92 precision 83.33 recall 71.43
Batch 701|1608: loss 0.0328 f1 50.00 precision 50.00 recall 50.00
Batch 801|1608: loss 0.0369 f1 33.33 precision 25.00 recall 50.00
Batch 901|1608: loss 0.0193 f1 50.00 precision 100.00 recall 33.33
Batch 1001|1608: loss 0.0395 f1 72.73 precision 80.00 recall 66.67
Batch 1101|1608: loss 0.0196 f1 0.00 precision 0.00 recall 0.00
Batch 1201|1608: loss 0.0164 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0221 f1 66.67 precision 66.67 recall 66.67
Batch 1401|1608: loss 0.0083 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0101 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0216 f1 80.00 precision 66.67 recall 100.00
Batch 1608|1608: loss 0.0449 f1 66.67 precision 100.00 recall 50.00
Train: loss 0.0294 f1 64.50 precision 69.52 recall 63.81
Time: 36.96
Dev: loss 0.0591 f1 51.89 precision 58.15 recall 46.84

Epoch 10|30:
Batch 1|1608: loss 0.0069 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0367 f1 57.14 precision 66.67 recall 50.00
Batch 201|1608: loss 0.0284 f1 75.00 precision 75.00 recall 75.00
Batch 301|1608: loss 0.0263 f1 50.00 precision 100.00 recall 33.33
Batch 401|1608: loss 0.0346 f1 76.92 precision 83.33 recall 71.43
Batch 501|1608: loss 0.0329 f1 57.14 precision 66.67 recall 50.00
Batch 601|1608: loss 0.0147 f1 0.00 precision 0.00 recall 0.00
Batch 701|1608: loss 0.0126 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0258 f1 66.67 precision 66.67 recall 66.67
Batch 901|1608: loss 0.0132 f1 80.00 precision 80.00 recall 80.00
Batch 1001|1608: loss 0.0267 f1 0.00 precision 0.00 recall 0.00
Batch 1101|1608: loss 0.0097 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0156 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0141 f1 80.00 precision 100.00 recall 66.67
Batch 1401|1608: loss 0.0108 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0393 f1 80.00 precision 66.67 recall 100.00
Batch 1601|1608: loss 0.0457 f1 50.00 precision 50.00 recall 50.00
Batch 1608|1608: loss 0.0007 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0256 f1 68.09 precision 72.55 recall 67.48
Time: 36.88
Dev: loss 0.0641 f1 51.17 precision 63.57 recall 42.82

Epoch 11|30:
Batch 1|1608: loss 0.0301 f1 0.00 precision 0.00 recall 0.00
Batch 101|1608: loss 0.0509 f1 66.67 precision 66.67 recall 66.67
Batch 201|1608: loss 0.0125 f1 0.00 precision 0.00 recall 0.00
Batch 301|1608: loss 0.0110 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0185 f1 80.00 precision 80.00 recall 80.00
Batch 501|1608: loss 0.0088 f1 0.00 precision 0.00 recall 0.00
Batch 601|1608: loss 0.0316 f1 0.00 precision 0.00 recall 0.00
Batch 701|1608: loss 0.0029 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0303 f1 60.00 precision 75.00 recall 50.00
Batch 901|1608: loss 0.0071 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0102 f1 88.89 precision 80.00 recall 100.00
Batch 1101|1608: loss 0.0199 f1 66.67 precision 100.00 recall 50.00
Batch 1201|1608: loss 0.0309 f1 90.91 precision 100.00 recall 83.33
Batch 1301|1608: loss 0.0235 f1 66.67 precision 100.00 recall 50.00
Batch 1401|1608: loss 0.0090 f1 85.71 precision 100.00 recall 75.00
Batch 1501|1608: loss 0.0147 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0839 f1 0.00 precision 0.00 recall 0.00
Batch 1608|1608: loss 0.0822 f1 57.14 precision 66.67 recall 50.00
Train: loss 0.0220 f1 72.03 precision 75.62 recall 72.06
Time: 36.85
Dev: loss 0.0618 f1 54.34 precision 57.40 recall 51.59
Save model weight!

Epoch 12|30:
Batch 1|1608: loss 0.0148 f1 92.31 precision 100.00 recall 85.71
Batch 101|1608: loss 0.0018 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0049 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0012 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0211 f1 75.00 precision 100.00 recall 60.00
Batch 501|1608: loss 0.0142 f1 66.67 precision 66.67 recall 66.67
Batch 601|1608: loss 0.0199 f1 75.00 precision 75.00 recall 75.00
Batch 701|1608: loss 0.0195 f1 83.33 precision 83.33 recall 83.33
Batch 801|1608: loss 0.0081 f1 85.71 precision 75.00 recall 100.00
Batch 901|1608: loss 0.0189 f1 40.00 precision 33.33 recall 50.00
Batch 1001|1608: loss 0.0212 f1 40.00 precision 33.33 recall 50.00
Batch 1101|1608: loss 0.0325 f1 92.31 precision 85.71 recall 100.00
Batch 1201|1608: loss 0.0318 f1 75.00 precision 75.00 recall 75.00
Batch 1301|1608: loss 0.0171 f1 85.71 precision 75.00 recall 100.00
Batch 1401|1608: loss 0.0080 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0063 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0007 f1 0.00 precision 0.00 recall 0.00
Batch 1608|1608: loss 0.0175 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0192 f1 75.40 precision 78.70 recall 75.16
Time: 36.89
Dev: loss 0.0647 f1 54.94 precision 58.19 recall 52.04
Save model weight!

Epoch 13|30:
Batch 1|1608: loss 0.0033 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0098 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0169 f1 88.89 precision 100.00 recall 80.00
Batch 301|1608: loss 0.0403 f1 83.33 precision 100.00 recall 71.43
Batch 401|1608: loss 0.0130 f1 66.67 precision 50.00 recall 100.00
Batch 501|1608: loss 0.0165 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0731 f1 57.14 precision 100.00 recall 40.00
Batch 701|1608: loss 0.0098 f1 88.89 precision 100.00 recall 80.00
Batch 801|1608: loss 0.0148 f1 80.00 precision 100.00 recall 66.67
Batch 901|1608: loss 0.0273 f1 50.00 precision 50.00 recall 50.00
Batch 1001|1608: loss 0.0047 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0226 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0305 f1 80.00 precision 100.00 recall 66.67
Batch 1301|1608: loss 0.0321 f1 66.67 precision 100.00 recall 50.00
Batch 1401|1608: loss 0.0211 f1 85.71 precision 75.00 recall 100.00
Batch 1501|1608: loss 0.0078 f1 92.31 precision 85.71 recall 100.00
Batch 1601|1608: loss 0.0032 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0219 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0172 f1 76.78 precision 79.44 recall 76.89
Time: 36.84
Dev: loss 0.0676 f1 54.26 precision 56.35 recall 52.32

Epoch 14|30:
Batch 1|1608: loss 0.0061 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0200 f1 66.67 precision 66.67 recall 66.67
Batch 201|1608: loss 0.0012 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0053 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0191 f1 0.00 precision 0.00 recall 0.00
Batch 501|1608: loss 0.0086 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0195 f1 80.00 precision 80.00 recall 80.00
Batch 701|1608: loss 0.0092 f1 80.00 precision 100.00 recall 66.67
Batch 801|1608: loss 0.0018 f1 0.00 precision 0.00 recall 0.00
Batch 901|1608: loss 0.0016 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0241 f1 50.00 precision 50.00 recall 50.00
Batch 1101|1608: loss 0.0478 f1 50.00 precision 33.33 recall 100.00
Batch 1201|1608: loss 0.0157 f1 94.12 precision 88.89 recall 100.00
Batch 1301|1608: loss 0.0209 f1 80.00 precision 100.00 recall 66.67
Batch 1401|1608: loss 0.0051 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0131 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0199 f1 80.00 precision 100.00 recall 66.67
Batch 1608|1608: loss 0.0012 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0152 f1 79.62 precision 81.66 recall 79.81
Time: 36.82
Dev: loss 0.0722 f1 51.46 precision 57.35 recall 46.67

Epoch 15|30:
Batch 1|1608: loss 0.0348 f1 57.14 precision 100.00 recall 40.00
Batch 101|1608: loss 0.0225 f1 80.00 precision 75.00 recall 85.71
Batch 201|1608: loss 0.0008 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0116 f1 88.89 precision 100.00 recall 80.00
Batch 401|1608: loss 0.0037 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0226 f1 85.71 precision 75.00 recall 100.00
Batch 601|1608: loss 0.0122 f1 75.00 precision 75.00 recall 75.00
Batch 701|1608: loss 0.0702 f1 40.00 precision 50.00 recall 33.33
Batch 801|1608: loss 0.0437 f1 66.67 precision 66.67 recall 66.67
Batch 901|1608: loss 0.0266 f1 80.00 precision 100.00 recall 66.67
Batch 1001|1608: loss 0.0178 f1 88.89 precision 80.00 recall 100.00
Batch 1101|1608: loss 0.0193 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0152 f1 50.00 precision 50.00 recall 50.00
Batch 1301|1608: loss 0.0043 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0107 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0113 f1 66.67 precision 100.00 recall 50.00
Batch 1601|1608: loss 0.0111 f1 80.00 precision 100.00 recall 66.67
Batch 1608|1608: loss 0.0031 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0134 f1 82.76 precision 84.81 recall 82.98
Time: 36.81
Dev: loss 0.0730 f1 53.73 precision 55.93 recall 51.70

Epoch 16|30:
Batch 1|1608: loss 0.0054 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0041 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0357 f1 66.67 precision 75.00 recall 60.00
Batch 301|1608: loss 0.0217 f1 72.73 precision 66.67 recall 80.00
Batch 401|1608: loss 0.0297 f1 85.71 precision 100.00 recall 75.00
Batch 501|1608: loss 0.0057 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0072 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0251 f1 80.00 precision 80.00 recall 80.00
Batch 801|1608: loss 0.0148 f1 40.00 precision 50.00 recall 33.33
Batch 901|1608: loss 0.0072 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0023 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0039 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0058 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0022 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0020 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0043 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0277 f1 66.67 precision 66.67 recall 66.67
Batch 1608|1608: loss 0.0088 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0126 f1 82.80 precision 84.49 recall 83.17
Time: 37.03
Dev: loss 0.0747 f1 52.75 precision 57.31 recall 48.85

Epoch 17|30:
Batch 1|1608: loss 0.0151 f1 85.71 precision 75.00 recall 100.00
Batch 101|1608: loss 0.0026 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0025 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0058 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0017 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0142 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0172 f1 93.33 precision 100.00 recall 87.50
Batch 701|1608: loss 0.0007 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0060 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0002 f1 0.00 precision 0.00 recall 0.00
Batch 1001|1608: loss 0.0142 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0029 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0276 f1 75.00 precision 75.00 recall 75.00
Batch 1301|1608: loss 0.0021 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0061 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0018 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0061 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0085 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0117 f1 82.88 precision 84.57 recall 83.18
Time: 36.84
Dev: loss 0.0773 f1 53.37 precision 53.35 recall 53.38

Epoch 18|30:
Batch 1|1608: loss 0.0019 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0058 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0089 f1 85.71 precision 100.00 recall 75.00
Batch 301|1608: loss 0.0118 f1 0.00 precision 0.00 recall 0.00
Batch 401|1608: loss 0.0068 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0026 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0081 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0016 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0042 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0200 f1 66.67 precision 66.67 recall 66.67
Batch 1001|1608: loss 0.0017 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0059 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0477 f1 72.73 precision 80.00 recall 66.67
Batch 1301|1608: loss 0.0083 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0049 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0062 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0090 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0070 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0106 f1 85.17 precision 86.24 recall 85.78
Time: 36.82
Dev: loss 0.0790 f1 52.20 precision 57.90 recall 47.51

Epoch 19|30:
Batch 1|1608: loss 0.0056 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0097 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0359 f1 75.00 precision 75.00 recall 75.00
Batch 301|1608: loss 0.0129 f1 66.67 precision 50.00 recall 100.00
Batch 401|1608: loss 0.0158 f1 87.50 precision 100.00 recall 77.78
Batch 501|1608: loss 0.0181 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0102 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0057 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0155 f1 87.50 precision 87.50 recall 87.50
Batch 901|1608: loss 0.0039 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0153 f1 66.67 precision 66.67 recall 66.67
Batch 1101|1608: loss 0.0211 f1 80.00 precision 75.00 recall 85.71
Batch 1201|1608: loss 0.0111 f1 80.00 precision 80.00 recall 80.00
Batch 1301|1608: loss 0.0039 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0205 f1 50.00 precision 50.00 recall 50.00
Batch 1501|1608: loss 0.0051 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0104 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0223 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0103 f1 86.20 precision 87.19 recall 86.71
Time: 36.82
Dev: loss 0.0804 f1 53.30 precision 55.51 recall 51.26

Epoch 20|30:
Batch 1|1608: loss 0.0022 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0039 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0029 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0027 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0017 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0008 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0103 f1 80.00 precision 66.67 recall 100.00
Batch 701|1608: loss 0.0021 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0010 f1 0.00 precision 0.00 recall 0.00
Batch 901|1608: loss 0.0178 f1 66.67 precision 50.00 recall 100.00
Batch 1001|1608: loss 0.0067 f1 88.89 precision 100.00 recall 80.00
Batch 1101|1608: loss 0.0069 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0075 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0051 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0004 f1 0.00 precision 0.00 recall 0.00
Batch 1501|1608: loss 0.0061 f1 87.50 precision 87.50 recall 87.50
Batch 1601|1608: loss 0.0030 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0042 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0100 f1 85.55 precision 86.80 recall 85.97
Time: 36.85
Dev: loss 0.0815 f1 52.57 precision 55.46 recall 49.97

Epoch 21|30:
Batch 1|1608: loss 0.0025 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0006 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0009 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0362 f1 83.33 precision 71.43 recall 100.00
Batch 401|1608: loss 0.0005 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0294 f1 88.89 precision 80.00 recall 100.00
Batch 601|1608: loss 0.0000 f1 0.00 precision 0.00 recall 0.00
Batch 701|1608: loss 0.0060 f1 0.00 precision 0.00 recall 0.00
Batch 801|1608: loss 0.0015 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0021 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0044 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0055 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0184 f1 75.00 precision 100.00 recall 60.00
Batch 1301|1608: loss 0.0189 f1 80.00 precision 100.00 recall 66.67
Batch 1401|1608: loss 0.0073 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0129 f1 85.71 precision 75.00 recall 100.00
Batch 1601|1608: loss 0.0024 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0002 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0092 f1 85.72 precision 86.46 recall 86.45
Time: 36.93
Dev: loss 0.0843 f1 53.60 precision 58.18 recall 49.69

Epoch 22|30:
Batch 1|1608: loss 0.0117 f1 75.00 precision 100.00 recall 60.00
Batch 101|1608: loss 0.0080 f1 88.89 precision 100.00 recall 80.00
Batch 201|1608: loss 0.0602 f1 44.44 precision 40.00 recall 50.00
Batch 301|1608: loss 0.0010 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0009 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0029 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0013 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0320 f1 0.00 precision 0.00 recall 0.00
Batch 801|1608: loss 0.0003 f1 0.00 precision 0.00 recall 0.00
Batch 901|1608: loss 0.0021 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0110 f1 85.71 precision 75.00 recall 100.00
Batch 1101|1608: loss 0.0124 f1 88.89 precision 100.00 recall 80.00
Batch 1201|1608: loss 0.0020 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0038 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0094 f1 75.00 precision 75.00 recall 75.00
Batch 1501|1608: loss 0.0023 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0208 f1 85.71 precision 75.00 recall 100.00
Batch 1608|1608: loss 0.0028 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0091 f1 87.03 precision 87.90 recall 87.71
Time: 36.85
Dev: loss 0.0854 f1 52.87 precision 58.24 recall 48.41

Epoch 23|30:
Batch 1|1608: loss 0.0144 f1 87.50 precision 87.50 recall 87.50
Batch 101|1608: loss 0.0015 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0005 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0071 f1 80.00 precision 80.00 recall 80.00
Batch 401|1608: loss 0.0010 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0014 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0005 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0006 f1 0.00 precision 0.00 recall 0.00
Batch 801|1608: loss 0.0024 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0031 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0009 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0265 f1 80.00 precision 66.67 recall 100.00
Batch 1201|1608: loss 0.0001 f1 0.00 precision 0.00 recall 0.00
Batch 1301|1608: loss 0.0085 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0087 f1 88.89 precision 100.00 recall 80.00
Batch 1501|1608: loss 0.0061 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0395 f1 80.00 precision 80.00 recall 80.00
Batch 1608|1608: loss 0.0009 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0083 f1 87.85 precision 88.35 recall 88.88
Time: 36.85
Dev: loss 0.0876 f1 51.61 precision 60.10 recall 45.22

Epoch 24|30:
Batch 1|1608: loss 0.0091 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0048 f1 66.67 precision 50.00 recall 100.00
Batch 201|1608: loss 0.0006 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0015 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0063 f1 90.91 precision 100.00 recall 83.33
Batch 501|1608: loss 0.0016 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0036 f1 0.00 precision 0.00 recall 0.00
Batch 701|1608: loss 0.0560 f1 66.67 precision 66.67 recall 66.67
Batch 801|1608: loss 0.0012 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0038 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0002 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0284 f1 66.67 precision 60.00 recall 75.00
Batch 1201|1608: loss 0.0118 f1 0.00 precision 0.00 recall 0.00
Batch 1301|1608: loss 0.0021 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0004 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0190 f1 50.00 precision 50.00 recall 50.00
Batch 1601|1608: loss 0.0023 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0003 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0086 f1 87.16 precision 88.11 recall 87.66
Time: 36.86
Dev: loss 0.0851 f1 53.39 precision 57.02 recall 50.20

Epoch 25|30:
Batch 1|1608: loss 0.0045 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0038 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0006 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0021 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0173 f1 88.89 precision 100.00 recall 80.00
Batch 501|1608: loss 0.0051 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0312 f1 88.89 precision 100.00 recall 80.00
Batch 701|1608: loss 0.0004 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0071 f1 66.67 precision 66.67 recall 66.67
Batch 901|1608: loss 0.0120 f1 94.12 precision 100.00 recall 88.89
Batch 1001|1608: loss 0.0338 f1 94.12 precision 100.00 recall 88.89
Batch 1101|1608: loss 0.0056 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0252 f1 88.89 precision 80.00 recall 100.00
Batch 1301|1608: loss 0.0117 f1 80.00 precision 100.00 recall 66.67
Batch 1401|1608: loss 0.0100 f1 75.00 precision 75.00 recall 75.00
Batch 1501|1608: loss 0.0145 f1 88.89 precision 100.00 recall 80.00
Batch 1601|1608: loss 0.0201 f1 0.00 precision 0.00 recall 0.00
Batch 1608|1608: loss 0.0005 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0078 f1 87.77 precision 88.34 recall 88.43
Time: 36.84
Dev: loss 0.0886 f1 53.48 precision 57.45 recall 50.03

Epoch 26|30:
Batch 1|1608: loss 0.0018 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0244 f1 85.71 precision 75.00 recall 100.00
Batch 201|1608: loss 0.0074 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0025 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0015 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0019 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0007 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0288 f1 83.33 precision 100.00 recall 71.43
Batch 801|1608: loss 0.0002 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0055 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0013 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0052 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0005 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0030 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0209 f1 80.00 precision 100.00 recall 66.67
Batch 1501|1608: loss 0.0006 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0188 f1 50.00 precision 33.33 recall 100.00
Batch 1608|1608: loss 0.0479 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0075 f1 87.99 precision 88.47 recall 88.69
Time: 36.86
Dev: loss 0.0907 f1 53.01 precision 59.94 recall 47.51

Epoch 27|30:
Batch 1|1608: loss 0.0002 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0026 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0205 f1 0.00 precision 0.00 recall 0.00
Batch 301|1608: loss 0.0040 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0053 f1 75.00 precision 75.00 recall 75.00
Batch 501|1608: loss 0.0092 f1 80.00 precision 66.67 recall 100.00
Batch 601|1608: loss 0.0012 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0011 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0013 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0019 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0018 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0007 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0025 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0013 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0145 f1 66.67 precision 50.00 recall 100.00
Batch 1501|1608: loss 0.0300 f1 66.67 precision 66.67 recall 66.67
Batch 1601|1608: loss 0.0007 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0015 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0073 f1 88.38 precision 89.10 recall 89.07
Time: 36.81
Dev: loss 0.0860 f1 54.34 precision 55.15 recall 53.55

Epoch 28|30:
Batch 1|1608: loss 0.0025 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0008 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0012 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0011 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0001 f1 0.00 precision 0.00 recall 0.00
Batch 501|1608: loss 0.0201 f1 80.00 precision 100.00 recall 66.67
Batch 601|1608: loss 0.0257 f1 50.00 precision 50.00 recall 50.00
Batch 701|1608: loss 0.0001 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0008 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0015 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0347 f1 88.89 precision 100.00 recall 80.00
Batch 1101|1608: loss 0.0020 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0043 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0213 f1 50.00 precision 33.33 recall 100.00
Batch 1401|1608: loss 0.0010 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0176 f1 90.91 precision 83.33 recall 100.00
Batch 1601|1608: loss 0.0364 f1 90.91 precision 83.33 recall 100.00
Batch 1608|1608: loss 0.0005 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0071 f1 88.71 precision 89.32 recall 89.38
Time: 36.87
Dev: loss 0.0869 f1 53.88 precision 56.78 recall 51.26

Epoch 29|30:
Batch 1|1608: loss 0.0284 f1 87.50 precision 87.50 recall 87.50
Batch 101|1608: loss 0.0003 f1 0.00 precision 0.00 recall 0.00
Batch 201|1608: loss 0.0013 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0158 f1 95.24 precision 100.00 recall 90.91
Batch 401|1608: loss 0.0002 f1 0.00 precision 0.00 recall 0.00
Batch 501|1608: loss 0.0056 f1 85.71 precision 75.00 recall 100.00
Batch 601|1608: loss 0.0157 f1 0.00 precision 0.00 recall 0.00
Batch 701|1608: loss 0.0025 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0006 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0189 f1 40.00 precision 33.33 recall 50.00
Batch 1001|1608: loss 0.0004 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0210 f1 80.00 precision 66.67 recall 100.00
Batch 1201|1608: loss 0.0026 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0020 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0004 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0015 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0056 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0172 f1 66.67 precision 66.67 recall 66.67
Train: loss 0.0074 f1 87.69 precision 88.26 recall 88.39
Time: 36.90
Dev: loss 0.0952 f1 51.33 precision 58.32 recall 45.84

Epoch 30|30:
Batch 1|1608: loss 0.0004 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0023 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0012 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0181 f1 80.00 precision 80.00 recall 80.00
Batch 401|1608: loss 0.0028 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0002 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0138 f1 94.12 precision 88.89 recall 100.00
Batch 701|1608: loss 0.0052 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0002 f1 0.00 precision 0.00 recall 0.00
Batch 901|1608: loss 0.0101 f1 80.00 precision 100.00 recall 66.67
Batch 1001|1608: loss 0.0136 f1 85.71 precision 100.00 recall 75.00
Batch 1101|1608: loss 0.0030 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0002 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0005 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0003 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0001 f1 0.00 precision 0.00 recall 0.00
Batch 1601|1608: loss 0.0001 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0016 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0068 f1 89.97 precision 90.69 recall 90.54
Time: 36.86
Dev: loss 0.0911 f1 53.02 precision 58.78 recall 48.30

Restore best model !
Test: f1 54.90 precision 58.43 recall 51.77
