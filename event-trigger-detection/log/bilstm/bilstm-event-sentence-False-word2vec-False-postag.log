Namespace(no_train=False, train_batch_size=8, eval_batch_size=8, lr=0.0001, num_epochs=20, log_step=100, no_cuda=False, word_embedding_dim=300, use_pretrained=False, use_postag=False, postag_embedding_dim=25, hidden_size=512, dropout_rate=0.5)
Epoch 1|20:
Batch 1|1608: loss 3.4884 f1 1.57 precision 0.83 recall 14.29
Batch 101|1608: loss 0.1621 f1 0.00 precision 0.00 recall 0.00
Batch 201|1608: loss 0.2092 f1 0.00 precision 0.00 recall 0.00
Batch 301|1608: loss 0.0922 f1 0.00 precision 0.00 recall 0.00
Batch 401|1608: loss 0.0577 f1 0.00 precision 0.00 recall 0.00
Batch 501|1608: loss 0.0882 f1 0.00 precision 0.00 recall 0.00
Batch 601|1608: loss 0.1854 f1 0.00 precision 0.00 recall 0.00
Batch 701|1608: loss 0.3791 f1 0.00 precision 0.00 recall 0.00
Batch 801|1608: loss 0.1182 f1 0.00 precision 0.00 recall 0.00
Batch 901|1608: loss 0.0617 f1 0.00 precision 0.00 recall 0.00
Batch 1001|1608: loss 0.0839 f1 0.00 precision 0.00 recall 0.00
Batch 1101|1608: loss 0.2187 f1 33.33 precision 100.00 recall 20.00
Batch 1201|1608: loss 0.1056 f1 50.00 precision 100.00 recall 33.33
Namespace(no_train=False, train_batch_size=8, eval_batch_size=8, lr=0.0001, num_epochs=20, log_step=100, no_cuda=False, word_embedding_dim=300, use_pretrained=False, use_postag=False, postag_embedding_dim=25, hidden_size=512, dropout_rate=0.5)
Epoch 1|20:
Batch 1|1608: loss 3.5290 f1 0.00 precision 0.00 recall 0.00
Batch 101|1608: loss 0.3133 f1 0.00 precision 0.00 recall 0.00
Batch 201|1608: loss 0.2160 f1 0.00 precision 0.00 recall 0.00
Batch 301|1608: loss 0.0941 f1 0.00 precision 0.00 recall 0.00
Batch 401|1608: loss 0.0474 f1 0.00 precision 0.00 recall 0.00
Batch 501|1608: loss 0.1215 f1 0.00 precision 0.00 recall 0.00
Batch 601|1608: loss 0.0531 f1 0.00 precision 0.00 recall 0.00
Batch 701|1608: loss 0.0654 f1 0.00 precision 0.00 recall 0.00
Batch 801|1608: loss 0.3475 f1 0.00 precision 0.00 recall 0.00
Batch 901|1608: loss 0.1010 f1 0.00 precision 0.00 recall 0.00
Batch 1001|1608: loss 0.1858 f1 0.00 precision 0.00 recall 0.00
Batch 1101|1608: loss 0.1668 f1 0.00 precision 0.00 recall 0.00
Batch 1201|1608: loss 0.1043 f1 33.33 precision 100.00 recall 20.00
Batch 1301|1608: loss 0.1921 f1 0.00 precision 0.00 recall 0.00
Batch 1401|1608: loss 0.1194 f1 50.00 precision 66.67 recall 40.00
Batch 1501|1608: loss 0.0253 f1 0.00 precision 0.00 recall 0.00
Batch 1601|1608: loss 0.1869 f1 0.00 precision 0.00 recall 0.00
Batch 1608|1608: loss 0.2980 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.2094 f1 5.86 precision 11.53 recall 4.27
Time: 35.21
Dev: loss 0.1058 f1 27.98 precision 60.86 recall 18.17
Save model weight!

Epoch 2|20:
Batch 1|1608: loss 0.1671 f1 40.00 precision 100.00 recall 25.00
Batch 101|1608: loss 0.1039 f1 0.00 precision 0.00 recall 0.00
Batch 201|1608: loss 0.0894 f1 0.00 precision 0.00 recall 0.00
Batch 301|1608: loss 0.0984 f1 40.00 precision 50.00 recall 33.33
Batch 401|1608: loss 0.0911 f1 25.00 precision 33.33 recall 20.00
Batch 501|1608: loss 0.0971 f1 0.00 precision 0.00 recall 0.00
Batch 601|1608: loss 0.0330 f1 85.71 precision 75.00 recall 100.00
Batch 701|1608: loss 0.1270 f1 0.00 precision 0.00 recall 0.00
Batch 801|1608: loss 0.0180 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0208 f1 0.00 precision 0.00 recall 0.00
Batch 1001|1608: loss 0.2038 f1 44.44 precision 66.67 recall 33.33
Batch 1101|1608: loss 0.1679 f1 50.00 precision 80.00 recall 36.36
Batch 1201|1608: loss 0.0826 f1 50.00 precision 100.00 recall 33.33
Batch 1301|1608: loss 0.0783 f1 50.00 precision 100.00 recall 33.33
Batch 1401|1608: loss 0.1441 f1 33.33 precision 50.00 recall 25.00
Batch 1501|1608: loss 0.1005 f1 44.44 precision 33.33 recall 66.67
Batch 1601|1608: loss 0.0996 f1 0.00 precision 0.00 recall 0.00
Batch 1608|1608: loss 0.0658 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0974 f1 30.39 precision 45.12 recall 25.52
Time: 34.39
Dev: loss 0.0834 f1 40.62 precision 62.81 recall 30.02
Save model weight!

Epoch 3|20:
Batch 1|1608: loss 0.0337 f1 75.00 precision 100.00 recall 60.00
Batch 101|1608: loss 0.0845 f1 40.00 precision 50.00 recall 33.33
Batch 201|1608: loss 0.1824 f1 20.00 precision 25.00 recall 16.67
Batch 301|1608: loss 0.0582 f1 0.00 precision 0.00 recall 0.00
Batch 401|1608: loss 0.0975 f1 50.00 precision 100.00 recall 33.33
Batch 501|1608: loss 0.0532 f1 80.00 precision 66.67 recall 100.00
Batch 601|1608: loss 0.2168 f1 22.22 precision 50.00 recall 14.29
Batch 701|1608: loss 0.1250 f1 0.00 precision 0.00 recall 0.00
Batch 801|1608: loss 0.0267 f1 0.00 precision 0.00 recall 0.00
Batch 901|1608: loss 0.1279 f1 40.00 precision 40.00 recall 40.00
Batch 1001|1608: loss 0.1459 f1 40.00 precision 100.00 recall 25.00
Batch 1101|1608: loss 0.1206 f1 33.33 precision 100.00 recall 20.00
Batch 1201|1608: loss 0.0428 f1 50.00 precision 50.00 recall 50.00
Batch 1301|1608: loss 0.0196 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.3117 f1 0.00 precision 0.00 recall 0.00
Batch 1501|1608: loss 0.0283 f1 0.00 precision 0.00 recall 0.00
Batch 1601|1608: loss 0.0220 f1 66.67 precision 66.67 recall 66.67
Batch 1608|1608: loss 0.0909 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0775 f1 38.70 precision 51.61 recall 34.37
Time: 35.20
Dev: loss 0.0764 f1 47.36 precision 59.58 recall 39.30
Save model weight!

Epoch 4|20:
Batch 1|1608: loss 0.0301 f1 50.00 precision 100.00 recall 33.33
Batch 101|1608: loss 0.1879 f1 50.00 precision 100.00 recall 33.33
Batch 201|1608: loss 0.0408 f1 50.00 precision 100.00 recall 33.33
Batch 301|1608: loss 0.0288 f1 66.67 precision 66.67 recall 66.67
Batch 401|1608: loss 0.0488 f1 66.67 precision 100.00 recall 50.00
Batch 501|1608: loss 0.0165 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0349 f1 80.00 precision 100.00 recall 66.67
Batch 701|1608: loss 0.0547 f1 75.00 precision 75.00 recall 75.00
Batch 801|1608: loss 0.0910 f1 40.00 precision 50.00 recall 33.33
Batch 901|1608: loss 0.0469 f1 85.71 precision 100.00 recall 75.00
Batch 1001|1608: loss 0.0450 f1 72.73 precision 80.00 recall 66.67
Batch 1101|1608: loss 0.2919 f1 66.67 precision 100.00 recall 50.00
Batch 1201|1608: loss 0.0142 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0059 f1 0.00 precision 0.00 recall 0.00
Batch 1401|1608: loss 0.0394 f1 80.00 precision 100.00 recall 66.67
Batch 1501|1608: loss 0.0988 f1 25.00 precision 16.67 recall 50.00
Batch 1601|1608: loss 0.0118 f1 80.00 precision 100.00 recall 66.67
Batch 1608|1608: loss 0.0079 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0638 f1 45.27 precision 56.30 recall 41.95
Time: 35.28
Dev: loss 0.0716 f1 47.67 precision 57.76 recall 40.58
Save model weight!

Epoch 5|20:
Batch 1|1608: loss 0.0214 f1 88.89 precision 100.00 recall 80.00
Batch 101|1608: loss 0.1119 f1 46.15 precision 100.00 recall 30.00
Batch 201|1608: loss 0.0635 f1 0.00 precision 0.00 recall 0.00
Batch 301|1608: loss 0.0241 f1 80.00 precision 66.67 recall 100.00
Batch 401|1608: loss 0.0347 f1 33.33 precision 33.33 recall 33.33
Batch 501|1608: loss 0.0486 f1 50.00 precision 50.00 recall 50.00
Batch 601|1608: loss 0.0766 f1 0.00 precision 0.00 recall 0.00
Batch 701|1608: loss 0.1372 f1 50.00 precision 60.00 recall 42.86
Batch 801|1608: loss 0.0544 f1 0.00 precision 0.00 recall 0.00
Batch 901|1608: loss 0.0311 f1 50.00 precision 100.00 recall 33.33
Batch 1001|1608: loss 0.0510 f1 83.33 precision 83.33 recall 83.33
Batch 1101|1608: loss 0.0098 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0883 f1 40.00 precision 33.33 recall 50.00
Batch 1301|1608: loss 0.0276 f1 80.00 precision 80.00 recall 80.00
Batch 1401|1608: loss 0.0507 f1 40.00 precision 33.33 recall 50.00
Batch 1501|1608: loss 0.0316 f1 75.00 precision 60.00 recall 100.00
Batch 1601|1608: loss 0.0057 f1 0.00 precision 0.00 recall 0.00
Batch 1608|1608: loss 0.0037 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0533 f1 50.90 precision 60.79 recall 47.83
Time: 35.30
Dev: loss 0.0696 f1 51.14 precision 58.01 recall 45.72
Save model weight!

Epoch 6|20:
Batch 1|1608: loss 0.0552 f1 0.00 precision 0.00 recall 0.00
Batch 101|1608: loss 0.0746 f1 40.00 precision 50.00 recall 33.33
Batch 201|1608: loss 0.0287 f1 57.14 precision 50.00 recall 66.67
Batch 301|1608: loss 0.0657 f1 0.00 precision 0.00 recall 0.00
Batch 401|1608: loss 0.1136 f1 0.00 precision 0.00 recall 0.00
Batch 501|1608: loss 0.0595 f1 44.44 precision 33.33 recall 66.67
Batch 601|1608: loss 0.0748 f1 36.36 precision 33.33 recall 40.00
Batch 701|1608: loss 0.0292 f1 0.00 precision 0.00 recall 0.00
Batch 801|1608: loss 0.0705 f1 36.36 precision 33.33 recall 40.00
Batch 901|1608: loss 0.0065 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0167 f1 80.00 precision 66.67 recall 100.00
Batch 1101|1608: loss 0.0405 f1 75.00 precision 60.00 recall 100.00
Batch 1201|1608: loss 0.0741 f1 72.73 precision 100.00 recall 57.14
Batch 1301|1608: loss 0.0732 f1 60.00 precision 100.00 recall 42.86
Batch 1401|1608: loss 0.0298 f1 40.00 precision 50.00 recall 33.33
Batch 1501|1608: loss 0.0915 f1 75.00 precision 100.00 recall 60.00
Batch 1601|1608: loss 0.0137 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0796 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0442 f1 56.60 precision 65.48 recall 53.52
Time: 35.45
Dev: loss 0.0700 f1 51.93 precision 57.33 recall 47.46
Save model weight!

Epoch 7|20:
Batch 1|1608: loss 0.0231 f1 66.67 precision 66.67 recall 66.67
Batch 101|1608: loss 0.0298 f1 75.00 precision 100.00 recall 60.00
Batch 201|1608: loss 0.0175 f1 50.00 precision 50.00 recall 50.00
Batch 301|1608: loss 0.0163 f1 33.33 precision 50.00 recall 25.00
Batch 401|1608: loss 0.0936 f1 82.35 precision 100.00 recall 70.00
Batch 501|1608: loss 0.1119 f1 50.00 precision 66.67 recall 40.00
Batch 601|1608: loss 0.0450 f1 0.00 precision 0.00 recall 0.00
Batch 701|1608: loss 0.0107 f1 80.00 precision 100.00 recall 66.67
Batch 801|1608: loss 0.0447 f1 66.67 precision 100.00 recall 50.00
Batch 901|1608: loss 0.0338 f1 33.33 precision 50.00 recall 25.00
Batch 1001|1608: loss 0.0241 f1 0.00 precision 0.00 recall 0.00
Batch 1101|1608: loss 0.0326 f1 0.00 precision 0.00 recall 0.00
Batch 1201|1608: loss 0.0385 f1 85.71 precision 100.00 recall 75.00
Batch 1301|1608: loss 0.0366 f1 80.00 precision 100.00 recall 66.67
Batch 1401|1608: loss 0.0282 f1 60.00 precision 50.00 recall 75.00
Batch 1501|1608: loss 0.0457 f1 66.67 precision 100.00 recall 50.00
Batch 1601|1608: loss 0.0051 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0060 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0362 f1 61.21 precision 68.73 recall 59.00
Time: 35.39
Dev: loss 0.0705 f1 50.76 precision 60.75 recall 43.60

Epoch 8|20:
Batch 1|1608: loss 0.0483 f1 66.67 precision 66.67 recall 66.67
Batch 101|1608: loss 0.0111 f1 80.00 precision 100.00 recall 66.67
Batch 201|1608: loss 0.0080 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0176 f1 80.00 precision 100.00 recall 66.67
Batch 401|1608: loss 0.0068 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0208 f1 80.00 precision 100.00 recall 66.67
Batch 601|1608: loss 0.0155 f1 0.00 precision 0.00 recall 0.00
Batch 701|1608: loss 0.0070 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0579 f1 66.67 precision 66.67 recall 66.67
Batch 901|1608: loss 0.0551 f1 80.00 precision 80.00 recall 80.00
Batch 1001|1608: loss 0.0157 f1 85.71 precision 75.00 recall 100.00
Batch 1101|1608: loss 0.0078 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0085 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0530 f1 80.00 precision 100.00 recall 66.67
Batch 1401|1608: loss 0.0194 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0665 f1 44.44 precision 40.00 recall 50.00
Batch 1601|1608: loss 0.0293 f1 66.67 precision 60.00 recall 75.00
Batch 1608|1608: loss 0.0244 f1 75.00 precision 100.00 recall 60.00
Train: loss 0.0297 f1 67.36 precision 73.51 recall 65.40
Time: 35.34
Dev: loss 0.0714 f1 52.98 precision 56.81 recall 49.64
Save model weight!

Epoch 9|20:
Batch 1|1608: loss 0.0151 f1 80.00 precision 66.67 recall 100.00
Batch 101|1608: loss 0.0016 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0156 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0050 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0299 f1 85.71 precision 85.71 recall 85.71
Batch 501|1608: loss 0.0394 f1 83.33 precision 100.00 recall 71.43
Batch 601|1608: loss 0.0048 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0427 f1 90.91 precision 100.00 recall 83.33
Batch 801|1608: loss 0.0198 f1 80.00 precision 100.00 recall 66.67
Batch 901|1608: loss 0.0491 f1 66.67 precision 50.00 recall 100.00
Batch 1001|1608: loss 0.0044 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0138 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0107 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0207 f1 75.00 precision 100.00 recall 60.00
Batch 1401|1608: loss 0.0110 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0767 f1 33.33 precision 50.00 recall 25.00
Batch 1601|1608: loss 0.0072 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0134 f1 66.67 precision 66.67 recall 66.67
Train: loss 0.0241 f1 72.74 precision 77.72 recall 71.64
Time: 35.38
Dev: loss 0.0766 f1 51.05 precision 57.26 recall 46.06

Epoch 10|20:
Batch 1|1608: loss 0.0331 f1 40.00 precision 100.00 recall 25.00
Batch 101|1608: loss 0.0352 f1 50.00 precision 50.00 recall 50.00
Batch 201|1608: loss 0.0271 f1 66.67 precision 60.00 recall 75.00
Batch 301|1608: loss 0.0100 f1 80.00 precision 100.00 recall 66.67
Batch 401|1608: loss 0.0253 f1 75.00 precision 75.00 recall 75.00
Batch 501|1608: loss 0.0036 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0083 f1 85.71 precision 100.00 recall 75.00
Batch 701|1608: loss 0.0120 f1 85.71 precision 100.00 recall 75.00
Batch 801|1608: loss 0.0077 f1 0.00 precision 0.00 recall 0.00
Batch 901|1608: loss 0.0361 f1 76.92 precision 83.33 recall 71.43
Batch 1001|1608: loss 0.0038 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0134 f1 88.89 precision 100.00 recall 80.00
Batch 1201|1608: loss 0.0189 f1 85.71 precision 100.00 recall 75.00
Batch 1301|1608: loss 0.0233 f1 80.00 precision 100.00 recall 66.67
Batch 1401|1608: loss 0.0093 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0202 f1 0.00 precision 0.00 recall 0.00
Batch 1601|1608: loss 0.0083 f1 80.00 precision 100.00 recall 66.67
Batch 1608|1608: loss 0.0284 f1 66.67 precision 50.00 recall 100.00
Train: loss 0.0203 f1 75.70 precision 79.49 recall 74.95
Time: 35.34
Dev: loss 0.0767 f1 51.48 precision 55.85 recall 47.74

Epoch 11|20:
Batch 1|1608: loss 0.0052 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0262 f1 80.00 precision 80.00 recall 80.00
Batch 201|1608: loss 0.0250 f1 80.00 precision 100.00 recall 66.67
Batch 301|1608: loss 0.0112 f1 88.89 precision 100.00 recall 80.00
Batch 401|1608: loss 0.0170 f1 85.71 precision 75.00 recall 100.00
Batch 501|1608: loss 0.0183 f1 75.00 precision 75.00 recall 75.00
Batch 601|1608: loss 0.0107 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0113 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0038 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0221 f1 80.00 precision 100.00 recall 66.67
Batch 1001|1608: loss 0.0483 f1 0.00 precision 0.00 recall 0.00
Batch 1101|1608: loss 0.0244 f1 85.71 precision 75.00 recall 100.00
Batch 1201|1608: loss 0.0162 f1 0.00 precision 0.00 recall 0.00
Batch 1301|1608: loss 0.0061 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0139 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0449 f1 72.73 precision 66.67 recall 80.00
Batch 1601|1608: loss 0.0055 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0234 f1 80.00 precision 80.00 recall 80.00
Train: loss 0.0174 f1 77.87 precision 81.43 recall 76.95
Time: 35.35
Dev: loss 0.0800 f1 52.04 precision 55.09 recall 49.30

Epoch 12|20:
Batch 1|1608: loss 0.0030 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0173 f1 88.89 precision 100.00 recall 80.00
Batch 201|1608: loss 0.0168 f1 88.89 precision 100.00 recall 80.00
Batch 301|1608: loss 0.0070 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0119 f1 80.00 precision 100.00 recall 66.67
Batch 501|1608: loss 0.0031 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0160 f1 80.00 precision 100.00 recall 66.67
Batch 701|1608: loss 0.0107 f1 85.71 precision 75.00 recall 100.00
Batch 801|1608: loss 0.0212 f1 57.14 precision 66.67 recall 50.00
Batch 901|1608: loss 0.0122 f1 50.00 precision 50.00 recall 50.00
Batch 1001|1608: loss 0.0076 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0138 f1 80.00 precision 80.00 recall 80.00
Batch 1201|1608: loss 0.0082 f1 85.71 precision 75.00 recall 100.00
Batch 1301|1608: loss 0.0024 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0136 f1 88.89 precision 100.00 recall 80.00
Batch 1501|1608: loss 0.0002 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0599 f1 0.00 precision 0.00 recall 0.00
Batch 1608|1608: loss 0.0015 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0152 f1 81.13 precision 83.50 recall 81.16
Time: 35.36
Dev: loss 0.0837 f1 51.00 precision 58.65 recall 45.11

Epoch 13|20:
Batch 1|1608: loss 0.0130 f1 88.89 precision 100.00 recall 80.00
Batch 101|1608: loss 0.0231 f1 80.00 precision 66.67 recall 100.00
Batch 201|1608: loss 0.0585 f1 75.00 precision 100.00 recall 60.00
Batch 301|1608: loss 0.0033 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0142 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0142 f1 66.67 precision 50.00 recall 100.00
Batch 601|1608: loss 0.0045 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0031 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0118 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0129 f1 66.67 precision 100.00 recall 50.00
Batch 1001|1608: loss 0.0022 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0050 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0261 f1 88.89 precision 100.00 recall 80.00
Batch 1301|1608: loss 0.0121 f1 50.00 precision 50.00 recall 50.00
Batch 1401|1608: loss 0.0046 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0134 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0055 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0056 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0135 f1 82.70 precision 84.99 recall 82.67
Time: 35.40
Dev: loss 0.0882 f1 50.29 precision 56.22 recall 45.50

Epoch 14|20:
Batch 1|1608: loss 0.0082 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0018 f1 0.00 precision 0.00 recall 0.00
Batch 201|1608: loss 0.0043 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0031 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0025 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0109 f1 80.00 precision 80.00 recall 80.00
Batch 601|1608: loss 0.0165 f1 80.00 precision 100.00 recall 66.67
Batch 701|1608: loss 0.0045 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0229 f1 40.00 precision 50.00 recall 33.33
Batch 901|1608: loss 0.0671 f1 75.00 precision 60.00 recall 100.00
Batch 1001|1608: loss 0.0020 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0103 f1 75.00 precision 75.00 recall 75.00
Batch 1201|1608: loss 0.0183 f1 75.00 precision 100.00 recall 60.00
Batch 1301|1608: loss 0.0608 f1 50.00 precision 100.00 recall 33.33
Batch 1401|1608: loss 0.0029 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0062 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0038 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0002 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0118 f1 84.11 precision 85.75 recall 84.38
Time: 35.37
Dev: loss 0.0872 f1 51.68 precision 58.42 recall 46.34

Epoch 15|20:
Batch 1|1608: loss 0.0483 f1 85.71 precision 75.00 recall 100.00
Batch 101|1608: loss 0.0125 f1 90.91 precision 83.33 recall 100.00
Batch 201|1608: loss 0.0016 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0020 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0048 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0233 f1 88.89 precision 100.00 recall 80.00
Batch 601|1608: loss 0.0089 f1 80.00 precision 100.00 recall 66.67
Batch 701|1608: loss 0.1110 f1 80.00 precision 100.00 recall 66.67
Batch 801|1608: loss 0.0003 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0001 f1 0.00 precision 0.00 recall 0.00
Batch 1001|1608: loss 0.0029 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0026 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0398 f1 83.33 precision 83.33 recall 83.33
Batch 1301|1608: loss 0.0193 f1 90.91 precision 100.00 recall 83.33
Batch 1401|1608: loss 0.0088 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0175 f1 66.67 precision 100.00 recall 50.00
Batch 1601|1608: loss 0.0544 f1 76.92 precision 71.43 recall 83.33
Batch 1608|1608: loss 0.0031 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0113 f1 85.65 precision 87.15 recall 85.90
Time: 35.31
Dev: loss 0.0887 f1 50.39 precision 57.81 recall 44.66

Epoch 16|20:
Batch 1|1608: loss 0.0118 f1 80.00 precision 80.00 recall 80.00
Batch 101|1608: loss 0.0006 f1 0.00 precision 0.00 recall 0.00
Batch 201|1608: loss 0.0167 f1 66.67 precision 100.00 recall 50.00
Batch 301|1608: loss 0.0409 f1 60.00 precision 50.00 recall 75.00
Batch 401|1608: loss 0.0011 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0074 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0009 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0414 f1 80.00 precision 66.67 recall 100.00
Batch 801|1608: loss 0.0081 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0215 f1 80.00 precision 100.00 recall 66.67
Batch 1001|1608: loss 0.0026 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0166 f1 92.31 precision 85.71 recall 100.00
Batch 1201|1608: loss 0.0096 f1 66.67 precision 50.00 recall 100.00
Batch 1301|1608: loss 0.0202 f1 75.00 precision 75.00 recall 75.00
Batch 1401|1608: loss 0.0084 f1 80.00 precision 80.00 recall 80.00
Batch 1501|1608: loss 0.0031 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0396 f1 66.67 precision 66.67 recall 66.67
Batch 1608|1608: loss 0.0002 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0101 f1 86.28 precision 87.82 recall 86.42
Time: 35.30
Dev: loss 0.0908 f1 51.04 precision 57.40 recall 45.95

Epoch 17|20:
Batch 1|1608: loss 0.0063 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0037 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0010 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0114 f1 85.71 precision 75.00 recall 100.00
Batch 401|1608: loss 0.0068 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0004 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0019 f1 0.00 precision 0.00 recall 0.00
Batch 701|1608: loss 0.0430 f1 80.00 precision 100.00 recall 66.67
Batch 801|1608: loss 0.0057 f1 0.00 precision 0.00 recall 0.00
Batch 901|1608: loss 0.0012 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0262 f1 57.14 precision 50.00 recall 66.67
Batch 1101|1608: loss 0.0133 f1 80.00 precision 80.00 recall 80.00
Batch 1201|1608: loss 0.0144 f1 0.00 precision 0.00 recall 0.00
Batch 1301|1608: loss 0.0054 f1 66.67 precision 100.00 recall 50.00
Batch 1401|1608: loss 0.0168 f1 66.67 precision 66.67 recall 66.67
Batch 1501|1608: loss 0.0013 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0026 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0003 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0094 f1 87.04 precision 88.29 recall 87.42
Time: 35.32
Dev: loss 0.0912 f1 52.45 precision 58.03 recall 47.85

Epoch 18|20:
Batch 1|1608: loss 0.0008 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0046 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0033 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0157 f1 66.67 precision 100.00 recall 50.00
Batch 401|1608: loss 0.0030 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0101 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0370 f1 66.67 precision 66.67 recall 66.67
Batch 701|1608: loss 0.0079 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0026 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0193 f1 80.00 precision 100.00 recall 66.67
Batch 1001|1608: loss 0.0020 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0165 f1 85.71 precision 75.00 recall 100.00
Batch 1201|1608: loss 0.0006 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0305 f1 83.33 precision 71.43 recall 100.00
Batch 1401|1608: loss 0.0065 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0029 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0015 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0015 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0093 f1 86.16 precision 87.07 recall 86.85
Time: 35.33
Dev: loss 0.0899 f1 52.30 precision 55.76 recall 49.25

Epoch 19|20:
Batch 1|1608: loss 0.0028 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0226 f1 66.67 precision 100.00 recall 50.00
Batch 201|1608: loss 0.0022 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0005 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0016 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0002 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0287 f1 66.67 precision 50.00 recall 100.00
Batch 701|1608: loss 0.0092 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0125 f1 80.00 precision 80.00 recall 80.00
Batch 901|1608: loss 0.0387 f1 76.92 precision 83.33 recall 71.43
Batch 1001|1608: loss 0.0011 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0241 f1 80.00 precision 66.67 recall 100.00
Batch 1201|1608: loss 0.0110 f1 88.89 precision 88.89 recall 88.89
Batch 1301|1608: loss 0.0008 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0020 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0003 f1 0.00 precision 0.00 recall 0.00
Batch 1601|1608: loss 0.0016 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0014 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0083 f1 88.47 precision 89.69 recall 88.68
Time: 35.37
Dev: loss 0.0939 f1 52.69 precision 57.72 recall 48.46

Epoch 20|20:
Batch 1|1608: loss 0.0004 f1 0.00 precision 0.00 recall 0.00
Batch 101|1608: loss 0.0089 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0002 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0020 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0023 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0026 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0051 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0065 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0287 f1 80.00 precision 66.67 recall 100.00
Batch 901|1608: loss 0.0023 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0003 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0730 f1 75.00 precision 75.00 recall 75.00
Batch 1201|1608: loss 0.0002 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0028 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0018 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0004 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0229 f1 90.91 precision 90.91 recall 90.91
Batch 1608|1608: loss 0.0012 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0081 f1 87.54 precision 88.33 recall 88.13
Time: 35.32
Dev: loss 0.0931 f1 52.63 precision 55.37 recall 50.14

Restore best model !
Test: f1 52.83 precision 57.40 recall 48.94
Namespace(no_train=False, train_batch_size=8, eval_batch_size=8, lr=0.0001, num_epochs=20, log_step=100, no_cuda=False, word_embedding_dim=300, use_pretrained=False, use_postag=False, postag_embedding_dim=25, hidden_size=512, dropout_rate=0.5)
Epoch 1|20:
Batch 1|1608: loss 3.5448 f1 0.00 precision 0.00 recall 0.00
Batch 101|1608: loss 0.2767 f1 0.00 precision 0.00 recall 0.00
Batch 201|1608: loss 0.2457 f1 0.00 precision 0.00 recall 0.00
Batch 301|1608: loss 0.0854 f1 0.00 precision 0.00 recall 0.00
Batch 401|1608: loss 0.1097 f1 0.00 precision 0.00 recall 0.00
Batch 501|1608: loss 0.2383 f1 0.00 precision 0.00 recall 0.00
Batch 601|1608: loss 0.2571 f1 0.00 precision 0.00 recall 0.00
Batch 701|1608: loss 0.1724 f1 0.00 precision 0.00 recall 0.00
Batch 801|1608: loss 0.0189 f1 0.00 precision 0.00 recall 0.00
Batch 901|1608: loss 0.2864 f1 0.00 precision 0.00 recall 0.00
Batch 1001|1608: loss 0.2101 f1 0.00 precision 0.00 recall 0.00
Batch 1101|1608: loss 0.2107 f1 0.00 precision 0.00 recall 0.00
Batch 1201|1608: loss 0.0276 f1 0.00 precision 0.00 recall 0.00
Batch 1301|1608: loss 0.3933 f1 0.00 precision 0.00 recall 0.00
Batch 1401|1608: loss 0.1501 f1 0.00 precision 0.00 recall 0.00
Batch 1501|1608: loss 0.1015 f1 33.33 precision 50.00 recall 25.00
Batch 1601|1608: loss 0.1301 f1 40.00 precision 50.00 recall 33.33
Batch 1608|1608: loss 0.0904 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.2140 f1 5.83 precision 11.32 recall 4.35
Time: 35.90
Dev: loss 0.1055 f1 19.87 precision 60.00 recall 11.91
Save model weight!

Epoch 2|20:
Batch 1|1608: loss 0.1886 f1 0.00 precision 0.00 recall 0.00
Batch 101|1608: loss 0.0738 f1 33.33 precision 100.00 recall 20.00
Batch 201|1608: loss 0.1251 f1 0.00 precision 0.00 recall 0.00
Batch 301|1608: loss 0.0930 f1 0.00 precision 0.00 recall 0.00
Batch 401|1608: loss 0.1279 f1 40.00 precision 66.67 recall 28.57
Batch 501|1608: loss 0.1975 f1 0.00 precision 0.00 recall 0.00
Batch 601|1608: loss 0.0501 f1 0.00 precision 0.00 recall 0.00
Batch 701|1608: loss 0.0168 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0905 f1 66.67 precision 100.00 recall 50.00
Batch 901|1608: loss 0.1609 f1 40.00 precision 50.00 recall 33.33
Batch 1001|1608: loss 0.0691 f1 57.14 precision 66.67 recall 50.00
Batch 1101|1608: loss 0.1747 f1 40.00 precision 100.00 recall 25.00
Batch 1201|1608: loss 0.0746 f1 57.14 precision 66.67 recall 50.00
Batch 1301|1608: loss 0.1087 f1 0.00 precision 0.00 recall 0.00
Batch 1401|1608: loss 0.0076 f1 0.00 precision 0.00 recall 0.00
Batch 1501|1608: loss 0.2567 f1 0.00 precision 0.00 recall 0.00
Batch 1601|1608: loss 0.0879 f1 0.00 precision 0.00 recall 0.00
Batch 1608|1608: loss 0.0641 f1 50.00 precision 50.00 recall 50.00
Train: loss 0.0987 f1 28.90 precision 43.57 recall 24.29
Time: 34.71
Dev: loss 0.0805 f1 42.42 precision 61.16 recall 32.48
Save model weight!

Epoch 3|20:
Batch 1|1608: loss 0.0916 f1 0.00 precision 0.00 recall 0.00
Batch 101|1608: loss 0.0240 f1 0.00 precision 0.00 recall 0.00
Batch 201|1608: loss 0.0776 f1 0.00 precision 0.00 recall 0.00
Batch 301|1608: loss 0.0988 f1 50.00 precision 100.00 recall 33.33
Batch 401|1608: loss 0.0412 f1 0.00 precision 0.00 recall 0.00
Batch 501|1608: loss 0.1199 f1 0.00 precision 0.00 recall 0.00
Batch 601|1608: loss 0.0273 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.1810 f1 0.00 precision 0.00 recall 0.00
Batch 801|1608: loss 0.0197 f1 88.89 precision 100.00 recall 80.00
Batch 901|1608: loss 0.0184 f1 66.67 precision 100.00 recall 50.00
Batch 1001|1608: loss 0.0431 f1 85.71 precision 100.00 recall 75.00
Batch 1101|1608: loss 0.2046 f1 44.44 precision 66.67 recall 33.33
Batch 1201|1608: loss 0.1395 f1 0.00 precision 0.00 recall 0.00
Batch 1301|1608: loss 0.0917 f1 0.00 precision 0.00 recall 0.00
Batch 1401|1608: loss 0.2290 f1 40.00 precision 100.00 recall 25.00
Batch 1501|1608: loss 0.0865 f1 0.00 precision 0.00 recall 0.00
Batch 1601|1608: loss 0.1225 f1 33.33 precision 50.00 recall 25.00
Batch 1608|1608: loss 0.0623 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0775 f1 39.15 precision 51.97 recall 34.90
Time: 35.39
Dev: loss 0.0719 f1 47.38 precision 59.80 recall 39.24
Save model weight!

Epoch 4|20:
Batch 1|1608: loss 0.0640 f1 66.67 precision 75.00 recall 60.00
Batch 101|1608: loss 0.0490 f1 50.00 precision 50.00 recall 50.00
Batch 201|1608: loss 0.1079 f1 36.36 precision 66.67 recall 25.00
Batch 301|1608: loss 0.0476 f1 57.14 precision 40.00 recall 100.00
Batch 401|1608: loss 0.0152 f1 0.00 precision 0.00 recall 0.00
Batch 501|1608: loss 0.2588 f1 33.33 precision 50.00 recall 25.00
Batch 601|1608: loss 0.1626 f1 50.00 precision 100.00 recall 33.33
Batch 701|1608: loss 0.0418 f1 66.67 precision 66.67 recall 66.67
Batch 801|1608: loss 0.0445 f1 0.00 precision 0.00 recall 0.00
Batch 901|1608: loss 0.0355 f1 50.00 precision 50.00 recall 50.00
Batch 1001|1608: loss 0.0621 f1 72.73 precision 80.00 recall 66.67
Batch 1101|1608: loss 0.2238 f1 40.00 precision 100.00 recall 25.00
Batch 1201|1608: loss 0.0469 f1 0.00 precision 0.00 recall 0.00
Batch 1301|1608: loss 0.0218 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0489 f1 66.67 precision 66.67 recall 66.67
Batch 1501|1608: loss 0.0771 f1 44.44 precision 40.00 recall 50.00
Batch 1601|1608: loss 0.0990 f1 25.00 precision 50.00 recall 16.67
Batch 1608|1608: loss 0.0362 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0640 f1 45.79 precision 57.18 recall 42.15
Time: 36.16
Dev: loss 0.0691 f1 46.86 precision 63.69 recall 37.06

Epoch 5|20:
Batch 1|1608: loss 0.0742 f1 50.00 precision 66.67 recall 40.00
Batch 101|1608: loss 0.2205 f1 66.67 precision 100.00 recall 50.00
Batch 201|1608: loss 0.0246 f1 0.00 precision 0.00 recall 0.00
Batch 301|1608: loss 0.2011 f1 60.00 precision 60.00 recall 60.00
Batch 401|1608: loss 0.0121 f1 88.89 precision 80.00 recall 100.00
Batch 501|1608: loss 0.0236 f1 0.00 precision 0.00 recall 0.00
Batch 601|1608: loss 0.0460 f1 75.00 precision 75.00 recall 75.00
Batch 701|1608: loss 0.0670 f1 50.00 precision 100.00 recall 33.33
Batch 801|1608: loss 0.0469 f1 33.33 precision 33.33 recall 33.33
Batch 901|1608: loss 0.1637 f1 0.00 precision 0.00 recall 0.00
Batch 1001|1608: loss 0.0208 f1 0.00 precision 0.00 recall 0.00
Batch 1101|1608: loss 0.0817 f1 57.14 precision 66.67 recall 50.00
Batch 1201|1608: loss 0.0270 f1 66.67 precision 100.00 recall 50.00
Batch 1301|1608: loss 0.1408 f1 28.57 precision 50.00 recall 20.00
Batch 1401|1608: loss 0.0486 f1 50.00 precision 100.00 recall 33.33
Batch 1501|1608: loss 0.0558 f1 50.00 precision 50.00 recall 50.00
Batch 1601|1608: loss 0.0185 f1 66.67 precision 100.00 recall 50.00
Batch 1608|1608: loss 0.1035 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0536 f1 50.29 precision 60.48 recall 46.95
Time: 36.72
Dev: loss 0.0679 f1 46.78 precision 61.99 recall 37.56

Epoch 6|20:
Batch 1|1608: loss 0.1080 f1 66.67 precision 66.67 recall 66.67
Batch 101|1608: loss 0.1004 f1 72.73 precision 100.00 recall 57.14
Batch 201|1608: loss 0.0449 f1 80.00 precision 85.71 recall 75.00
Batch 301|1608: loss 0.1285 f1 33.33 precision 40.00 recall 28.57
Batch 401|1608: loss 0.0723 f1 28.57 precision 33.33 recall 25.00
Batch 501|1608: loss 0.0728 f1 57.14 precision 66.67 recall 50.00
Batch 601|1608: loss 0.0789 f1 44.44 precision 66.67 recall 33.33
Batch 701|1608: loss 0.0383 f1 80.00 precision 100.00 recall 66.67
Batch 801|1608: loss 0.0168 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0609 f1 0.00 precision 0.00 recall 0.00
Batch 1001|1608: loss 0.0114 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0201 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0276 f1 57.14 precision 66.67 recall 50.00
Batch 1301|1608: loss 0.0115 f1 66.67 precision 50.00 recall 100.00
Batch 1401|1608: loss 0.0131 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0590 f1 50.00 precision 50.00 recall 50.00
Batch 1601|1608: loss 0.0090 f1 0.00 precision 0.00 recall 0.00
Batch 1608|1608: loss 0.0080 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0441 f1 54.90 precision 63.49 recall 52.16
Time: 37.20
Dev: loss 0.0677 f1 49.95 precision 58.67 recall 43.49
Save model weight!

Epoch 7|20:
Batch 1|1608: loss 0.0063 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0232 f1 80.00 precision 100.00 recall 66.67
Batch 201|1608: loss 0.0139 f1 66.67 precision 100.00 recall 50.00
Batch 301|1608: loss 0.0291 f1 80.00 precision 66.67 recall 100.00
Batch 401|1608: loss 0.0043 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0292 f1 0.00 precision 0.00 recall 0.00
Batch 601|1608: loss 0.0043 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0361 f1 85.71 precision 100.00 recall 75.00
Batch 801|1608: loss 0.0028 f1 0.00 precision 0.00 recall 0.00
Batch 901|1608: loss 0.0219 f1 80.00 precision 66.67 recall 100.00
Batch 1001|1608: loss 0.0433 f1 40.00 precision 50.00 recall 33.33
Batch 1101|1608: loss 0.0097 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0252 f1 75.00 precision 100.00 recall 60.00
Batch 1301|1608: loss 0.0174 f1 80.00 precision 66.67 recall 100.00
Batch 1401|1608: loss 0.0897 f1 22.22 precision 25.00 recall 20.00
Batch 1501|1608: loss 0.0065 f1 0.00 precision 0.00 recall 0.00
Batch 1601|1608: loss 0.0130 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0063 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0358 f1 59.94 precision 67.03 recall 57.79
Time: 39.44
Dev: loss 0.0706 f1 47.54 precision 60.55 recall 39.13

Epoch 8|20:
Batch 1|1608: loss 0.0044 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0536 f1 50.00 precision 40.00 recall 66.67
Batch 201|1608: loss 0.0201 f1 66.67 precision 100.00 recall 50.00
Batch 301|1608: loss 0.0097 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0195 f1 85.71 precision 75.00 recall 100.00
Batch 501|1608: loss 0.0277 f1 85.71 precision 100.00 recall 75.00
Batch 601|1608: loss 0.0170 f1 75.00 precision 75.00 recall 75.00
Batch 701|1608: loss 0.0115 f1 0.00 precision 0.00 recall 0.00
Batch 801|1608: loss 0.0416 f1 57.14 precision 50.00 recall 66.67
Batch 901|1608: loss 0.0469 f1 87.50 precision 87.50 recall 87.50
Batch 1001|1608: loss 0.0893 f1 66.67 precision 100.00 recall 50.00
Batch 1101|1608: loss 0.0570 f1 66.67 precision 66.67 recall 66.67
Batch 1201|1608: loss 0.0366 f1 57.14 precision 100.00 recall 40.00
Batch 1301|1608: loss 0.0434 f1 90.91 precision 100.00 recall 83.33
Batch 1401|1608: loss 0.0097 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0229 f1 75.00 precision 60.00 recall 100.00
Batch 1601|1608: loss 0.0105 f1 85.71 precision 75.00 recall 100.00
Batch 1608|1608: loss 0.0238 f1 66.67 precision 50.00 recall 100.00
Train: loss 0.0299 f1 66.95 precision 73.08 recall 65.37
Time: 38.02
Dev: loss 0.0686 f1 52.24 precision 57.53 recall 47.85
Save model weight!

Epoch 9|20:
Batch 1|1608: loss 0.0044 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0328 f1 66.67 precision 66.67 recall 66.67
Batch 201|1608: loss 0.0453 f1 0.00 precision 0.00 recall 0.00
Batch 301|1608: loss 0.0153 f1 80.00 precision 100.00 recall 66.67
Batch 401|1608: loss 0.0246 f1 66.67 precision 75.00 recall 60.00
Batch 501|1608: loss 0.0588 f1 44.44 precision 40.00 recall 50.00
Batch 601|1608: loss 0.0207 f1 40.00 precision 33.33 recall 50.00
Batch 701|1608: loss 0.0285 f1 44.44 precision 40.00 recall 50.00
Batch 801|1608: loss 0.0126 f1 66.67 precision 50.00 recall 100.00
Batch 901|1608: loss 0.0359 f1 76.92 precision 100.00 recall 62.50
Batch 1001|1608: loss 0.0072 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0034 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0380 f1 85.71 precision 100.00 recall 75.00
Batch 1301|1608: loss 0.0191 f1 57.14 precision 66.67 recall 50.00
Batch 1401|1608: loss 0.0076 f1 88.89 precision 80.00 recall 100.00
Batch 1501|1608: loss 0.0036 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0540 f1 40.00 precision 33.33 recall 50.00
Batch 1608|1608: loss 0.0174 f1 50.00 precision 50.00 recall 50.00
Train: loss 0.0242 f1 71.97 precision 76.69 recall 70.81
Time: 37.76
Dev: loss 0.0717 f1 52.09 precision 57.49 recall 47.62

Epoch 10|20:
Batch 1|1608: loss 0.0185 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0190 f1 66.67 precision 100.00 recall 50.00
Batch 201|1608: loss 0.0333 f1 90.91 precision 100.00 recall 83.33
Batch 301|1608: loss 0.0098 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0050 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0216 f1 66.67 precision 66.67 recall 66.67
Batch 601|1608: loss 0.0115 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0192 f1 66.67 precision 100.00 recall 50.00
Batch 801|1608: loss 0.0546 f1 88.89 precision 100.00 recall 80.00
Batch 901|1608: loss 0.0008 f1 0.00 precision 0.00 recall 0.00
Batch 1001|1608: loss 0.0007 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0437 f1 50.00 precision 50.00 recall 50.00
Batch 1201|1608: loss 0.0076 f1 0.00 precision 0.00 recall 0.00
Batch 1301|1608: loss 0.0130 f1 66.67 precision 100.00 recall 50.00
Batch 1401|1608: loss 0.0037 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0080 f1 80.00 precision 100.00 recall 66.67
Batch 1601|1608: loss 0.0015 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0138 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0204 f1 75.00 precision 79.24 recall 74.10
Time: 37.59
Dev: loss 0.0760 f1 50.63 precision 59.83 recall 43.88

Epoch 11|20:
Batch 1|1608: loss 0.0019 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0028 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0132 f1 0.00 precision 0.00 recall 0.00
Batch 301|1608: loss 0.0459 f1 50.00 precision 100.00 recall 33.33
Batch 401|1608: loss 0.0155 f1 83.33 precision 83.33 recall 83.33
Batch 501|1608: loss 0.0190 f1 50.00 precision 50.00 recall 50.00
Batch 601|1608: loss 0.0039 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0451 f1 0.00 precision 0.00 recall 0.00
Batch 801|1608: loss 0.0052 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0070 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0074 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0060 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0105 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0219 f1 0.00 precision 0.00 recall 0.00
Batch 1401|1608: loss 0.0029 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0028 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0059 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0104 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0174 f1 78.02 precision 80.83 recall 77.81
Time: 37.57
Dev: loss 0.0777 f1 51.81 precision 57.28 recall 47.29

Epoch 12|20:
Batch 1|1608: loss 0.0082 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0057 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0039 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0079 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0268 f1 80.00 precision 66.67 recall 100.00
Batch 501|1608: loss 0.0042 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0017 f1 0.00 precision 0.00 recall 0.00
Batch 701|1608: loss 0.0025 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0154 f1 83.33 precision 83.33 recall 83.33
Batch 901|1608: loss 0.0099 f1 66.67 precision 100.00 recall 50.00
Batch 1001|1608: loss 0.0501 f1 66.67 precision 100.00 recall 50.00
Batch 1101|1608: loss 0.0234 f1 80.00 precision 100.00 recall 66.67
Batch 1201|1608: loss 0.0030 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0025 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0100 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0013 f1 0.00 precision 0.00 recall 0.00
Batch 1601|1608: loss 0.0041 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0024 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0156 f1 81.08 precision 83.68 recall 80.88
Time: 37.59
Dev: loss 0.0855 f1 49.57 precision 60.75 recall 41.87

Epoch 13|20:
Batch 1|1608: loss 0.0169 f1 0.00 precision 0.00 recall 0.00
Batch 101|1608: loss 0.0075 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0302 f1 93.33 precision 87.50 recall 100.00
Batch 301|1608: loss 0.0037 f1 0.00 precision 0.00 recall 0.00
Batch 401|1608: loss 0.0612 f1 57.14 precision 66.67 recall 50.00
Batch 501|1608: loss 0.0093 f1 75.00 precision 75.00 recall 75.00
Batch 601|1608: loss 0.0066 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0139 f1 85.71 precision 100.00 recall 75.00
Batch 801|1608: loss 0.0017 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0713 f1 75.00 precision 60.00 recall 100.00
Batch 1001|1608: loss 0.0172 f1 66.67 precision 50.00 recall 100.00
Batch 1101|1608: loss 0.0106 f1 80.00 precision 66.67 recall 100.00
Batch 1201|1608: loss 0.0068 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0316 f1 33.33 precision 50.00 recall 25.00
Batch 1401|1608: loss 0.0046 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0101 f1 66.67 precision 66.67 recall 66.67
Batch 1601|1608: loss 0.0212 f1 75.00 precision 75.00 recall 75.00
Batch 1608|1608: loss 0.0042 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0132 f1 82.57 precision 84.80 recall 82.27
Time: 37.62
Dev: loss 0.0863 f1 50.65 precision 60.09 recall 43.77

Epoch 14|20:
Batch 1|1608: loss 0.0051 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0092 f1 0.00 precision 0.00 recall 0.00
Batch 201|1608: loss 0.0006 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0144 f1 85.71 precision 100.00 recall 75.00
Batch 401|1608: loss 0.0024 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0128 f1 90.91 precision 100.00 recall 83.33
Batch 601|1608: loss 0.0636 f1 80.00 precision 100.00 recall 66.67
Batch 701|1608: loss 0.0110 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0113 f1 90.91 precision 100.00 recall 83.33
Batch 901|1608: loss 0.0022 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0011 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0044 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0069 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0025 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0274 f1 85.71 precision 75.00 recall 100.00
Batch 1501|1608: loss 0.0255 f1 80.00 precision 80.00 recall 80.00
Batch 1601|1608: loss 0.0068 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0399 f1 50.00 precision 50.00 recall 50.00
Train: loss 0.0122 f1 84.47 precision 86.27 recall 84.73
Time: 37.59
Dev: loss 0.0854 f1 51.42 precision 58.19 recall 46.06

Epoch 15|20:
Batch 1|1608: loss 0.0057 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0006 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0390 f1 80.00 precision 100.00 recall 66.67
Batch 301|1608: loss 0.0143 f1 85.71 precision 100.00 recall 75.00
Batch 401|1608: loss 0.0055 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0024 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0094 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0237 f1 66.67 precision 66.67 recall 66.67
Batch 801|1608: loss 0.0086 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0043 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0061 f1 90.91 precision 100.00 recall 83.33
Batch 1101|1608: loss 0.0128 f1 66.67 precision 66.67 recall 66.67
Batch 1201|1608: loss 0.0516 f1 57.14 precision 66.67 recall 50.00
Batch 1301|1608: loss 0.0092 f1 85.71 precision 75.00 recall 100.00
Batch 1401|1608: loss 0.0072 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0082 f1 88.89 precision 100.00 recall 80.00
Batch 1601|1608: loss 0.0035 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0030 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0111 f1 85.57 precision 87.05 recall 85.88
Time: 37.72
Dev: loss 0.0868 f1 50.97 precision 58.12 recall 45.39

Epoch 16|20:
Batch 1|1608: loss 0.0035 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0068 f1 66.67 precision 66.67 recall 66.67
Batch 201|1608: loss 0.0045 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0156 f1 50.00 precision 50.00 recall 50.00
Batch 401|1608: loss 0.0019 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0051 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0056 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0024 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0009 f1 0.00 precision 0.00 recall 0.00
Batch 901|1608: loss 0.0051 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0006 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0046 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0062 f1 80.00 precision 100.00 recall 66.67
Batch 1301|1608: loss 0.0045 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0036 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0058 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0046 f1 66.67 precision 100.00 recall 50.00
Batch 1608|1608: loss 0.0203 f1 80.00 precision 100.00 recall 66.67
Train: loss 0.0103 f1 85.60 precision 87.06 recall 85.91
Time: 37.74
Dev: loss 0.0950 f1 49.85 precision 59.44 recall 42.93

Epoch 17|20:
Batch 1|1608: loss 0.0144 f1 80.00 precision 66.67 recall 100.00
Batch 101|1608: loss 0.0019 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0079 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0030 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0012 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0019 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0019 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0157 f1 80.00 precision 66.67 recall 100.00
Batch 801|1608: loss 0.0006 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0467 f1 66.67 precision 50.00 recall 100.00
Batch 1001|1608: loss 0.0092 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0249 f1 85.71 precision 75.00 recall 100.00
Batch 1201|1608: loss 0.0251 f1 0.00 precision 0.00 recall 0.00
Batch 1301|1608: loss 0.0116 f1 80.00 precision 100.00 recall 66.67
Batch 1401|1608: loss 0.0023 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0078 f1 85.71 precision 100.00 recall 75.00
Batch 1601|1608: loss 0.0256 f1 80.00 precision 66.67 recall 100.00
Batch 1608|1608: loss 0.0112 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0095 f1 86.26 precision 87.64 recall 86.59
Time: 37.81
Dev: loss 0.1006 f1 49.28 precision 61.07 recall 41.31

Epoch 18|20:
Batch 1|1608: loss 0.0007 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0043 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0045 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0013 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0028 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0024 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0070 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0022 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0043 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0026 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0419 f1 50.00 precision 33.33 recall 100.00
Batch 1101|1608: loss 0.0067 f1 83.33 precision 83.33 recall 83.33
Batch 1201|1608: loss 0.0040 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0034 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0009 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0009 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0001 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0101 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0088 f1 87.53 precision 88.77 recall 87.75
Time: 38.11
Dev: loss 0.0938 f1 51.20 precision 57.90 recall 45.89

Epoch 19|20:
Batch 1|1608: loss 0.0014 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0006 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0030 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0010 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0037 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0005 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0212 f1 75.00 precision 75.00 recall 75.00
Batch 701|1608: loss 0.0029 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0067 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0339 f1 90.91 precision 83.33 recall 100.00
Batch 1001|1608: loss 0.0046 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0155 f1 88.89 precision 80.00 recall 100.00
Batch 1201|1608: loss 0.0020 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0060 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0024 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0070 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0024 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0067 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0085 f1 88.60 precision 89.27 recall 89.32
Time: 38.11
Dev: loss 0.0958 f1 50.64 precision 59.77 recall 43.94

Epoch 20|20:
Batch 1|1608: loss 0.0014 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0009 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0006 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0048 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0074 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0004 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0047 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0144 f1 75.00 precision 75.00 recall 75.00
Batch 801|1608: loss 0.0151 f1 87.50 precision 87.50 recall 87.50
Batch 901|1608: loss 0.0025 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0028 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0046 f1 85.71 precision 75.00 recall 100.00
Batch 1201|1608: loss 0.0007 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0057 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0008 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0048 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0036 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0208 f1 80.00 precision 66.67 recall 100.00
Train: loss 0.0080 f1 87.34 precision 88.01 recall 88.05
Time: 37.97
Dev: loss 0.1012 f1 49.33 precision 58.82 recall 42.48

Restore best model !
Test: f1 52.68 precision 59.59 recall 47.20
Namespace(no_train=False, train_batch_size=8, eval_batch_size=8, lr=0.0001, num_epochs=30, log_step=100, no_cuda=False, word_embedding_dim=300, use_pretrained=False, use_postag=False, postag_embedding_dim=25, hidden_size=512, dropout_rate=0.5)
Epoch 1|30:
Batch 1|1608: loss 3.5150 f1 0.00 precision 0.00 recall 0.00
Batch 101|1608: loss 0.1455 f1 0.00 precision 0.00 recall 0.00
Batch 201|1608: loss 0.0476 f1 0.00 precision 0.00 recall 0.00
Batch 301|1608: loss 0.2401 f1 0.00 precision 0.00 recall 0.00
Batch 401|1608: loss 0.2043 f1 0.00 precision 0.00 recall 0.00
Batch 501|1608: loss 0.1498 f1 0.00 precision 0.00 recall 0.00
Batch 601|1608: loss 0.1125 f1 0.00 precision 0.00 recall 0.00
Batch 701|1608: loss 0.1356 f1 0.00 precision 0.00 recall 0.00
Batch 801|1608: loss 0.0696 f1 0.00 precision 0.00 recall 0.00
Batch 901|1608: loss 0.1610 f1 0.00 precision 0.00 recall 0.00
Batch 1001|1608: loss 0.0808 f1 0.00 precision 0.00 recall 0.00
Batch 1101|1608: loss 0.0736 f1 0.00 precision 0.00 recall 0.00
Batch 1201|1608: loss 0.2480 f1 0.00 precision 0.00 recall 0.00
Batch 1301|1608: loss 0.0954 f1 0.00 precision 0.00 recall 0.00
Batch 1401|1608: loss 0.0802 f1 0.00 precision 0.00 recall 0.00
Batch 1501|1608: loss 0.0728 f1 0.00 precision 0.00 recall 0.00
Batch 1601|1608: loss 0.1874 f1 0.00 precision 0.00 recall 0.00
Batch 1608|1608: loss 0.3543 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.2085 f1 5.94 precision 11.41 recall 4.41
Time: 34.86
Dev: loss 0.1049 f1 22.56 precision 65.24 recall 13.64
Save model weight!

Epoch 2|30:
Batch 1|1608: loss 0.1949 f1 40.00 precision 100.00 recall 25.00
Batch 101|1608: loss 0.1865 f1 0.00 precision 0.00 recall 0.00
Batch 201|1608: loss 0.0436 f1 66.67 precision 100.00 recall 50.00
Batch 301|1608: loss 0.0556 f1 0.00 precision 0.00 recall 0.00
Batch 401|1608: loss 0.0223 f1 0.00 precision 0.00 recall 0.00
Batch 501|1608: loss 0.0455 f1 0.00 precision 0.00 recall 0.00
Batch 601|1608: loss 0.0888 f1 0.00 precision 0.00 recall 0.00
Batch 701|1608: loss 0.0464 f1 80.00 precision 100.00 recall 66.67
Batch 801|1608: loss 0.0480 f1 0.00 precision 0.00 recall 0.00
Batch 901|1608: loss 0.0204 f1 0.00 precision 0.00 recall 0.00
Batch 1001|1608: loss 0.0666 f1 60.00 precision 75.00 recall 50.00
Batch 1101|1608: loss 0.2098 f1 33.33 precision 100.00 recall 20.00
Batch 1201|1608: loss 0.0473 f1 0.00 precision 0.00 recall 0.00
Batch 1301|1608: loss 0.0742 f1 0.00 precision 0.00 recall 0.00
Batch 1401|1608: loss 0.2201 f1 0.00 precision 0.00 recall 0.00
Batch 1501|1608: loss 0.0576 f1 0.00 precision 0.00 recall 0.00
Batch 1601|1608: loss 0.0869 f1 0.00 precision 0.00 recall 0.00
Batch 1608|1608: loss 0.1113 f1 28.57 precision 50.00 recall 20.00
Train: loss 0.0988 f1 30.04 precision 44.61 recall 25.12
Time: 35.12
Dev: loss 0.0810 f1 43.32 precision 62.71 recall 33.09
Save model weight!

Epoch 3|30:
Batch 1|1608: loss 0.1536 f1 28.57 precision 100.00 recall 16.67
Batch 101|1608: loss 0.0701 f1 0.00 precision 0.00 recall 0.00
Batch 201|1608: loss 0.1434 f1 50.00 precision 100.00 recall 33.33
Batch 301|1608: loss 0.0409 f1 0.00 precision 0.00 recall 0.00
Batch 401|1608: loss 0.1115 f1 0.00 precision 0.00 recall 0.00
Batch 501|1608: loss 0.1556 f1 25.00 precision 33.33 recall 20.00
Batch 601|1608: loss 0.1056 f1 40.00 precision 100.00 recall 25.00
Batch 701|1608: loss 0.0600 f1 44.44 precision 40.00 recall 50.00
Batch 801|1608: loss 0.0140 f1 0.00 precision 0.00 recall 0.00
Batch 901|1608: loss 0.0249 f1 80.00 precision 100.00 recall 66.67
Batch 1001|1608: loss 0.0313 f1 66.67 precision 50.00 recall 100.00
Batch 1101|1608: loss 0.0901 f1 0.00 precision 0.00 recall 0.00
Batch 1201|1608: loss 0.0991 f1 0.00 precision 0.00 recall 0.00
Batch 1301|1608: loss 0.0738 f1 0.00 precision 0.00 recall 0.00
Batch 1401|1608: loss 0.0200 f1 80.00 precision 66.67 recall 100.00
Batch 1501|1608: loss 0.0291 f1 0.00 precision 0.00 recall 0.00
Batch 1601|1608: loss 0.0320 f1 0.00 precision 0.00 recall 0.00
Batch 1608|1608: loss 0.0696 f1 80.00 precision 80.00 recall 80.00
Train: loss 0.0784 f1 38.83 precision 51.79 recall 34.44
Time: 36.83
Dev: loss 0.0738 f1 45.98 precision 59.98 recall 37.28
Save model weight!

Epoch 4|30:
Batch 1|1608: loss 0.2064 f1 44.44 precision 66.67 recall 33.33
Batch 101|1608: loss 0.0748 f1 50.00 precision 66.67 recall 40.00
Batch 201|1608: loss 0.1220 f1 50.00 precision 66.67 recall 40.00
Batch 301|1608: loss 0.0455 f1 90.91 precision 83.33 recall 100.00
Batch 401|1608: loss 0.0352 f1 80.00 precision 100.00 recall 66.67
Batch 501|1608: loss 0.1055 f1 18.18 precision 25.00 recall 14.29
Batch 601|1608: loss 0.0711 f1 50.00 precision 50.00 recall 50.00
Batch 701|1608: loss 0.0344 f1 90.91 precision 100.00 recall 83.33
Batch 801|1608: loss 0.1217 f1 20.00 precision 25.00 recall 16.67
Batch 901|1608: loss 0.0276 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0251 f1 50.00 precision 33.33 recall 100.00
Batch 1101|1608: loss 0.0188 f1 80.00 precision 100.00 recall 66.67
Batch 1201|1608: loss 0.0354 f1 33.33 precision 25.00 recall 50.00
Batch 1301|1608: loss 0.0388 f1 88.89 precision 100.00 recall 80.00
Batch 1401|1608: loss 0.0449 f1 0.00 precision 0.00 recall 0.00
Batch 1501|1608: loss 0.1482 f1 0.00 precision 0.00 recall 0.00
Batch 1601|1608: loss 0.1168 f1 54.55 precision 75.00 recall 42.86
Batch 1608|1608: loss 0.0105 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0646 f1 44.33 precision 55.27 recall 40.84
Time: 37.14
Dev: loss 0.0766 f1 41.24 precision 65.89 recall 30.02

Epoch 5|30:
Batch 1|1608: loss 0.1184 f1 0.00 precision 0.00 recall 0.00
Batch 101|1608: loss 0.0370 f1 66.67 precision 100.00 recall 50.00
Batch 201|1608: loss 0.0259 f1 66.67 precision 66.67 recall 66.67
Batch 301|1608: loss 0.0931 f1 0.00 precision 0.00 recall 0.00
Batch 401|1608: loss 0.0753 f1 66.67 precision 100.00 recall 50.00
Batch 501|1608: loss 0.0366 f1 40.00 precision 50.00 recall 33.33
Batch 601|1608: loss 0.0110 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0320 f1 66.67 precision 100.00 recall 50.00
Batch 801|1608: loss 0.0774 f1 20.00 precision 20.00 recall 20.00
Batch 901|1608: loss 0.0691 f1 0.00 precision 0.00 recall 0.00
Batch 1001|1608: loss 0.0256 f1 50.00 precision 50.00 recall 50.00
Batch 1101|1608: loss 0.0946 f1 0.00 precision 0.00 recall 0.00
Batch 1201|1608: loss 0.0119 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0269 f1 50.00 precision 100.00 recall 33.33
Batch 1401|1608: loss 0.0929 f1 55.56 precision 71.43 recall 45.45
Batch 1501|1608: loss 0.0315 f1 66.67 precision 66.67 recall 66.67
Batch 1601|1608: loss 0.0080 f1 0.00 precision 0.00 recall 0.00
Batch 1608|1608: loss 0.0131 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0539 f1 50.26 precision 60.01 recall 47.17
Time: 36.77
Dev: loss 0.0693 f1 47.16 precision 60.81 recall 38.51
Save model weight!

Epoch 6|30:
Batch 1|1608: loss 0.0539 f1 57.14 precision 66.67 recall 50.00
Batch 101|1608: loss 0.0875 f1 50.00 precision 66.67 recall 40.00
Batch 201|1608: loss 0.0221 f1 88.89 precision 100.00 recall 80.00
Batch 301|1608: loss 0.0426 f1 66.67 precision 66.67 recall 66.67
Batch 401|1608: loss 0.0746 f1 28.57 precision 33.33 recall 25.00
Batch 501|1608: loss 0.0143 f1 80.00 precision 100.00 recall 66.67
Batch 601|1608: loss 0.0343 f1 50.00 precision 50.00 recall 50.00
Batch 701|1608: loss 0.0266 f1 66.67 precision 60.00 recall 75.00
Batch 801|1608: loss 0.0090 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0192 f1 40.00 precision 33.33 recall 50.00
Batch 1001|1608: loss 0.1147 f1 0.00 precision 0.00 recall 0.00
Batch 1101|1608: loss 0.0812 f1 40.00 precision 100.00 recall 25.00
Batch 1201|1608: loss 0.0393 f1 80.00 precision 80.00 recall 80.00
Batch 1301|1608: loss 0.0083 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0297 f1 92.31 precision 100.00 recall 85.71
Batch 1501|1608: loss 0.0371 f1 0.00 precision 0.00 recall 0.00
Batch 1601|1608: loss 0.0114 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0210 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0443 f1 56.18 precision 64.47 recall 53.59
Time: 37.02
Dev: loss 0.0698 f1 51.52 precision 55.72 recall 47.90
Save model weight!

Epoch 7|30:
Batch 1|1608: loss 0.0223 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0103 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0561 f1 0.00 precision 0.00 recall 0.00
Batch 301|1608: loss 0.0444 f1 66.67 precision 100.00 recall 50.00
Batch 401|1608: loss 0.0238 f1 0.00 precision 0.00 recall 0.00
Batch 501|1608: loss 0.0201 f1 80.00 precision 100.00 recall 66.67
Batch 601|1608: loss 0.0244 f1 0.00 precision 0.00 recall 0.00
Batch 701|1608: loss 0.0722 f1 40.00 precision 50.00 recall 33.33
Batch 801|1608: loss 0.0181 f1 85.71 precision 100.00 recall 75.00
Batch 901|1608: loss 0.0946 f1 57.14 precision 57.14 recall 57.14
Batch 1001|1608: loss 0.0275 f1 0.00 precision 0.00 recall 0.00
Batch 1101|1608: loss 0.0089 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0331 f1 0.00 precision 0.00 recall 0.00
Batch 1301|1608: loss 0.0777 f1 66.67 precision 100.00 recall 50.00
Batch 1401|1608: loss 0.0057 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0463 f1 72.73 precision 66.67 recall 80.00
Batch 1601|1608: loss 0.0162 f1 85.71 precision 75.00 recall 100.00
Batch 1608|1608: loss 0.0126 f1 66.67 precision 100.00 recall 50.00
Train: loss 0.0368 f1 62.33 precision 70.27 recall 59.92
Time: 36.82
Dev: loss 0.0738 f1 48.55 precision 58.31 recall 41.59

Epoch 8|30:
Batch 1|1608: loss 0.0266 f1 92.31 precision 100.00 recall 85.71
Batch 101|1608: loss 0.0140 f1 90.91 precision 100.00 recall 83.33
Batch 201|1608: loss 0.0052 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0066 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0052 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0096 f1 80.00 precision 66.67 recall 100.00
Batch 601|1608: loss 0.0241 f1 0.00 precision 0.00 recall 0.00
Batch 701|1608: loss 0.0115 f1 66.67 precision 66.67 recall 66.67
Batch 801|1608: loss 0.0138 f1 66.67 precision 100.00 recall 50.00
Batch 901|1608: loss 0.0375 f1 0.00 precision 0.00 recall 0.00
Batch 1001|1608: loss 0.0260 f1 66.67 precision 100.00 recall 50.00
Batch 1101|1608: loss 0.1642 f1 60.00 precision 75.00 recall 50.00
Batch 1201|1608: loss 0.0108 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0138 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0324 f1 66.67 precision 100.00 recall 50.00
Batch 1501|1608: loss 0.0648 f1 61.54 precision 66.67 recall 57.14
Batch 1601|1608: loss 0.0120 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0052 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0299 f1 66.58 precision 72.64 recall 64.73
Time: 36.82
Dev: loss 0.0765 f1 49.38 precision 57.60 recall 43.21

Epoch 9|30:
Batch 1|1608: loss 0.0452 f1 50.00 precision 50.00 recall 50.00
Batch 101|1608: loss 0.0471 f1 80.00 precision 100.00 recall 66.67
Batch 201|1608: loss 0.0092 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0115 f1 80.00 precision 100.00 recall 66.67
Batch 401|1608: loss 0.0151 f1 66.67 precision 50.00 recall 100.00
Batch 501|1608: loss 0.0403 f1 66.67 precision 50.00 recall 100.00
Batch 601|1608: loss 0.0095 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0079 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0082 f1 66.67 precision 100.00 recall 50.00
Batch 901|1608: loss 0.0408 f1 60.00 precision 75.00 recall 50.00
Batch 1001|1608: loss 0.0107 f1 66.67 precision 50.00 recall 100.00
Batch 1101|1608: loss 0.0031 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0280 f1 80.00 precision 66.67 recall 100.00
Batch 1301|1608: loss 0.0336 f1 80.00 precision 100.00 recall 66.67
Batch 1401|1608: loss 0.0115 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0062 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0131 f1 66.67 precision 50.00 recall 100.00
Batch 1608|1608: loss 0.0662 f1 80.00 precision 100.00 recall 66.67
Train: loss 0.0243 f1 71.44 precision 76.14 recall 70.30
Time: 36.84
Dev: loss 0.0753 f1 51.37 precision 57.88 recall 46.17

Epoch 10|30:
Batch 1|1608: loss 0.0369 f1 80.00 precision 100.00 recall 66.67
Batch 101|1608: loss 0.0104 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0316 f1 93.33 precision 100.00 recall 87.50
Batch 301|1608: loss 0.0452 f1 0.00 precision 0.00 recall 0.00
Batch 401|1608: loss 0.0040 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0428 f1 80.00 precision 80.00 recall 80.00
Batch 601|1608: loss 0.0155 f1 66.67 precision 75.00 recall 60.00
Batch 701|1608: loss 0.0215 f1 66.67 precision 100.00 recall 50.00
Batch 801|1608: loss 0.0054 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0485 f1 0.00 precision 0.00 recall 0.00
Batch 1001|1608: loss 0.0135 f1 80.00 precision 66.67 recall 100.00
Batch 1101|1608: loss 0.0292 f1 85.71 precision 75.00 recall 100.00
Batch 1201|1608: loss 0.0140 f1 50.00 precision 50.00 recall 50.00
Batch 1301|1608: loss 0.0064 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0136 f1 88.89 precision 80.00 recall 100.00
Batch 1501|1608: loss 0.0473 f1 33.33 precision 33.33 recall 33.33
Batch 1601|1608: loss 0.0108 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0009 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0205 f1 75.09 precision 79.06 recall 74.42
Time: 36.83
Dev: loss 0.0826 f1 48.46 precision 56.70 recall 42.31

Epoch 11|30:
Batch 1|1608: loss 0.0105 f1 75.00 precision 75.00 recall 75.00
Batch 101|1608: loss 0.0157 f1 66.67 precision 66.67 recall 66.67
Batch 201|1608: loss 0.0100 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0232 f1 66.67 precision 66.67 recall 66.67
Batch 401|1608: loss 0.0376 f1 90.91 precision 100.00 recall 83.33
Batch 501|1608: loss 0.0080 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0114 f1 80.00 precision 80.00 recall 80.00
Batch 701|1608: loss 0.0045 f1 0.00 precision 0.00 recall 0.00
Batch 801|1608: loss 0.0305 f1 80.00 precision 80.00 recall 80.00
Batch 901|1608: loss 0.0081 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0117 f1 66.67 precision 50.00 recall 100.00
Batch 1101|1608: loss 0.0202 f1 85.71 precision 100.00 recall 75.00
Batch 1201|1608: loss 0.0152 f1 50.00 precision 50.00 recall 50.00
Batch 1301|1608: loss 0.0166 f1 57.14 precision 50.00 recall 66.67
Batch 1401|1608: loss 0.0177 f1 85.71 precision 100.00 recall 75.00
Batch 1501|1608: loss 0.0111 f1 0.00 precision 0.00 recall 0.00
Batch 1601|1608: loss 0.0379 f1 80.00 precision 100.00 recall 66.67
Batch 1608|1608: loss 0.0007 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0174 f1 77.74 precision 80.90 recall 77.17
Time: 36.80
Dev: loss 0.0859 f1 49.58 precision 58.88 recall 42.82

Epoch 12|30:
Batch 1|1608: loss 0.0057 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0099 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0031 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0008 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0675 f1 0.00 precision 0.00 recall 0.00
Batch 501|1608: loss 0.0419 f1 0.00 precision 0.00 recall 0.00
Batch 601|1608: loss 0.0332 f1 66.67 precision 100.00 recall 50.00
Batch 701|1608: loss 0.0052 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0043 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0023 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0111 f1 0.00 precision 0.00 recall 0.00
Batch 1101|1608: loss 0.0016 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0111 f1 0.00 precision 0.00 recall 0.00
Batch 1301|1608: loss 0.0019 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0448 f1 71.43 precision 71.43 recall 71.43
Batch 1501|1608: loss 0.0014 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0034 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0022 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0151 f1 80.52 precision 83.20 recall 80.27
Time: 36.90
Dev: loss 0.0948 f1 46.07 precision 60.56 recall 37.17

Epoch 13|30:
Batch 1|1608: loss 0.0034 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0043 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0266 f1 94.74 precision 100.00 recall 90.00
Batch 301|1608: loss 0.0052 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0176 f1 72.73 precision 80.00 recall 66.67
Batch 501|1608: loss 0.0046 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0109 f1 80.00 precision 100.00 recall 66.67
Batch 701|1608: loss 0.0665 f1 50.00 precision 50.00 recall 50.00
Batch 801|1608: loss 0.0064 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0046 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0025 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0054 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0052 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0049 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0229 f1 93.33 precision 100.00 recall 87.50
Batch 1501|1608: loss 0.0127 f1 85.71 precision 75.00 recall 100.00
Batch 1601|1608: loss 0.0026 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0028 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0138 f1 83.07 precision 85.00 recall 83.14
Time: 36.82
Dev: loss 0.0931 f1 48.85 precision 60.83 recall 40.80

Epoch 14|30:
Batch 1|1608: loss 0.0142 f1 85.71 precision 100.00 recall 75.00
Batch 101|1608: loss 0.0233 f1 57.14 precision 66.67 recall 50.00
Batch 201|1608: loss 0.0193 f1 85.71 precision 100.00 recall 75.00
Batch 301|1608: loss 0.0186 f1 88.89 precision 80.00 recall 100.00
Batch 401|1608: loss 0.0656 f1 75.00 precision 60.00 recall 100.00
Batch 501|1608: loss 0.0196 f1 88.89 precision 100.00 recall 80.00
Batch 601|1608: loss 0.0110 f1 85.71 precision 100.00 recall 75.00
Batch 701|1608: loss 0.0285 f1 85.71 precision 100.00 recall 75.00
Batch 801|1608: loss 0.0046 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0090 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0042 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0027 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0012 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0036 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0094 f1 66.67 precision 100.00 recall 50.00
Batch 1501|1608: loss 0.0317 f1 82.35 precision 87.50 recall 77.78
Batch 1601|1608: loss 0.0015 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0003 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0120 f1 84.72 precision 86.34 recall 84.99
Time: 36.76
Dev: loss 0.0933 f1 48.97 precision 58.00 recall 42.37

Epoch 15|30:
Batch 1|1608: loss 0.0008 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0002 f1 0.00 precision 0.00 recall 0.00
Batch 201|1608: loss 0.0015 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0121 f1 80.00 precision 80.00 recall 80.00
Batch 401|1608: loss 0.0011 f1 0.00 precision 0.00 recall 0.00
Batch 501|1608: loss 0.0008 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0411 f1 85.71 precision 75.00 recall 100.00
Batch 701|1608: loss 0.0030 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0349 f1 66.67 precision 50.00 recall 100.00
Batch 901|1608: loss 0.0598 f1 80.00 precision 100.00 recall 66.67
Batch 1001|1608: loss 0.0030 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0340 f1 88.89 precision 100.00 recall 80.00
Batch 1201|1608: loss 0.0088 f1 75.00 precision 75.00 recall 75.00
Batch 1301|1608: loss 0.0004 f1 0.00 precision 0.00 recall 0.00
Batch 1401|1608: loss 0.0026 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0058 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0062 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0008 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0114 f1 85.82 precision 87.37 recall 86.00
Time: 36.90
Dev: loss 0.0949 f1 49.77 precision 60.53 recall 42.26

Epoch 16|30:
Batch 1|1608: loss 0.0051 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0279 f1 88.89 precision 80.00 recall 100.00
Batch 201|1608: loss 0.0073 f1 50.00 precision 50.00 recall 50.00
Batch 301|1608: loss 0.0007 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0018 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0291 f1 83.33 precision 83.33 recall 83.33
Batch 601|1608: loss 0.0163 f1 80.00 precision 66.67 recall 100.00
Batch 701|1608: loss 0.0129 f1 80.00 precision 100.00 recall 66.67
Batch 801|1608: loss 0.0060 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0299 f1 80.00 precision 100.00 recall 66.67
Batch 1001|1608: loss 0.0004 f1 0.00 precision 0.00 recall 0.00
Batch 1101|1608: loss 0.0089 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0053 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0021 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0123 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0186 f1 66.67 precision 100.00 recall 50.00
Batch 1601|1608: loss 0.0229 f1 0.00 precision 0.00 recall 0.00
Batch 1608|1608: loss 0.0035 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0099 f1 86.05 precision 87.32 recall 86.40
Time: 36.82
Dev: loss 0.0988 f1 47.74 precision 59.52 recall 39.85

Epoch 17|30:
Batch 1|1608: loss 0.0016 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0016 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0016 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0056 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0053 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0288 f1 85.71 precision 100.00 recall 75.00
Batch 601|1608: loss 0.0019 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0009 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0080 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0225 f1 80.00 precision 66.67 recall 100.00
Batch 1001|1608: loss 0.0019 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0011 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0031 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0095 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0020 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0133 f1 80.00 precision 80.00 recall 80.00
Batch 1601|1608: loss 0.0033 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0059 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0097 f1 86.55 precision 87.82 recall 86.94
Time: 36.82
Dev: loss 0.1046 f1 47.02 precision 58.76 recall 39.18

Epoch 18|30:
Batch 1|1608: loss 0.0019 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0035 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0065 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0126 f1 88.89 precision 100.00 recall 80.00
Batch 401|1608: loss 0.0030 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0043 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0008 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0018 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0326 f1 85.71 precision 100.00 recall 75.00
Batch 901|1608: loss 0.0016 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0007 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0010 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0223 f1 92.31 precision 100.00 recall 85.71
Batch 1301|1608: loss 0.0024 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0266 f1 85.71 precision 100.00 recall 75.00
Batch 1501|1608: loss 0.0080 f1 88.89 precision 100.00 recall 80.00
Batch 1601|1608: loss 0.0033 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0036 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0089 f1 86.62 precision 87.59 recall 87.09
Time: 36.87
Dev: loss 0.0974 f1 49.43 precision 57.06 recall 43.60

Epoch 19|30:
Batch 1|1608: loss 0.0018 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0026 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0003 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0008 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0155 f1 92.31 precision 85.71 recall 100.00
Batch 501|1608: loss 0.0003 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0078 f1 85.71 precision 75.00 recall 100.00
Batch 701|1608: loss 0.0012 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0187 f1 85.71 precision 100.00 recall 75.00
Batch 901|1608: loss 0.0620 f1 0.00 precision 0.00 recall 0.00
Batch 1001|1608: loss 0.0011 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0391 f1 80.00 precision 66.67 recall 100.00
Batch 1201|1608: loss 0.0117 f1 85.71 precision 75.00 recall 100.00
Batch 1301|1608: loss 0.0066 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0053 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0036 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0142 f1 50.00 precision 50.00 recall 50.00
Batch 1608|1608: loss 0.0088 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0087 f1 87.56 precision 88.45 recall 88.11
Time: 36.83
Dev: loss 0.0992 f1 50.40 precision 56.49 recall 45.50

Epoch 20|30:
Batch 1|1608: loss 0.0023 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0139 f1 85.71 precision 75.00 recall 100.00
Batch 201|1608: loss 0.0160 f1 88.89 precision 80.00 recall 100.00
Batch 301|1608: loss 0.0558 f1 85.71 precision 75.00 recall 100.00
Batch 401|1608: loss 0.0014 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0002 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0094 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0088 f1 80.00 precision 100.00 recall 66.67
Batch 801|1608: loss 0.0006 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0009 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0141 f1 90.91 precision 83.33 recall 100.00
Batch 1101|1608: loss 0.0018 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0419 f1 0.00 precision 0.00 recall 0.00
Batch 1301|1608: loss 0.0089 f1 85.71 precision 75.00 recall 100.00
Batch 1401|1608: loss 0.0154 f1 80.00 precision 100.00 recall 66.67
Batch 1501|1608: loss 0.0010 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0077 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0039 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0083 f1 87.44 precision 88.38 recall 88.01
Time: 36.90
Dev: loss 0.1015 f1 50.02 precision 58.75 recall 43.54

Epoch 21|30:
Batch 1|1608: loss 0.0019 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0050 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0011 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0061 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0001 f1 0.00 precision 0.00 recall 0.00
Batch 501|1608: loss 0.0013 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0006 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0011 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0106 f1 88.89 precision 100.00 recall 80.00
Batch 901|1608: loss 0.0555 f1 50.00 precision 100.00 recall 33.33
Batch 1001|1608: loss 0.0011 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0304 f1 50.00 precision 50.00 recall 50.00
Batch 1201|1608: loss 0.0093 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0477 f1 66.67 precision 50.00 recall 100.00
Batch 1401|1608: loss 0.0033 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0034 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0070 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0241 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0075 f1 88.47 precision 89.09 recall 89.14
Time: 37.08
Dev: loss 0.1106 f1 47.93 precision 57.97 recall 40.86

Epoch 22|30:
Batch 1|1608: loss 0.0051 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0071 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0217 f1 88.89 precision 80.00 recall 100.00
Batch 301|1608: loss 0.0010 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0118 f1 50.00 precision 33.33 recall 100.00
Batch 501|1608: loss 0.0012 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0035 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0030 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0043 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0107 f1 66.67 precision 100.00 recall 50.00
Batch 1001|1608: loss 0.0066 f1 92.31 precision 100.00 recall 85.71
Batch 1101|1608: loss 0.0010 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0008 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0058 f1 0.00 precision 0.00 recall 0.00
Batch 1401|1608: loss 0.0049 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0010 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0017 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0223 f1 66.67 precision 66.67 recall 66.67
Train: loss 0.0073 f1 88.90 precision 89.77 recall 89.49
Time: 36.85
Dev: loss 0.1084 f1 48.97 precision 60.13 recall 41.31

Epoch 23|30:
Batch 1|1608: loss 0.0311 f1 85.71 precision 100.00 recall 75.00
Batch 101|1608: loss 0.0003 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0012 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0089 f1 66.67 precision 66.67 recall 66.67
Batch 401|1608: loss 0.0008 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0400 f1 80.00 precision 66.67 recall 100.00
Batch 601|1608: loss 0.0085 f1 85.71 precision 85.71 recall 85.71
Batch 701|1608: loss 0.0039 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0007 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0005 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0007 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0019 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0010 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0106 f1 85.71 precision 75.00 recall 100.00
Batch 1401|1608: loss 0.0078 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0005 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0012 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0003 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0072 f1 88.95 precision 89.72 recall 89.54
Time: 36.94
Dev: loss 0.1097 f1 47.83 precision 57.55 recall 40.92

Epoch 24|30:
Batch 1|1608: loss 0.0009 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0015 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0218 f1 80.00 precision 80.00 recall 80.00
Batch 301|1608: loss 0.0008 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0014 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0063 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0003 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0074 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0024 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0008 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0052 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0017 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0018 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0466 f1 83.33 precision 100.00 recall 71.43
Batch 1401|1608: loss 0.0114 f1 88.89 precision 100.00 recall 80.00
Batch 1501|1608: loss 0.0133 f1 88.89 precision 80.00 recall 100.00
Batch 1601|1608: loss 0.0373 f1 85.71 precision 100.00 recall 75.00
Batch 1608|1608: loss 0.0003 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0067 f1 88.83 precision 89.37 recall 89.45
Time: 36.89
Dev: loss 0.1096 f1 50.69 precision 59.38 recall 44.21

Epoch 25|30:
Batch 1|1608: loss 0.0005 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0011 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0031 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0918 f1 44.44 precision 40.00 recall 50.00
Batch 401|1608: loss 0.0017 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0028 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0275 f1 90.91 precision 100.00 recall 83.33
Batch 701|1608: loss 0.0014 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0004 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0166 f1 94.12 precision 100.00 recall 88.89
Batch 1001|1608: loss 0.0054 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0009 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0016 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0140 f1 66.67 precision 66.67 recall 66.67
Batch 1401|1608: loss 0.0020 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0008 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0012 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0002 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0067 f1 88.54 precision 89.21 recall 89.12
Time: 36.89
Dev: loss 0.1166 f1 47.25 precision 59.51 recall 39.18

Epoch 26|30:
Batch 1|1608: loss 0.0023 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0007 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0057 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0004 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0014 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0036 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0010 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0002 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0186 f1 50.00 precision 50.00 recall 50.00
Batch 901|1608: loss 0.0045 f1 80.00 precision 66.67 recall 100.00
Batch 1001|1608: loss 0.0062 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0007 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0216 f1 85.71 precision 100.00 recall 75.00
Batch 1301|1608: loss 0.0116 f1 88.89 precision 100.00 recall 80.00
Batch 1401|1608: loss 0.0378 f1 57.14 precision 66.67 recall 50.00
Batch 1501|1608: loss 0.0290 f1 50.00 precision 66.67 recall 40.00
Batch 1601|1608: loss 0.0014 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0002 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0065 f1 89.41 precision 89.98 recall 90.15
Time: 36.82
Dev: loss 0.1148 f1 49.25 precision 59.23 recall 42.15

Epoch 27|30:
Batch 1|1608: loss 0.0001 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0163 f1 66.67 precision 50.00 recall 100.00
Batch 201|1608: loss 0.0391 f1 72.73 precision 66.67 recall 80.00
Batch 301|1608: loss 0.0002 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0092 f1 66.67 precision 50.00 recall 100.00
Batch 501|1608: loss 0.0064 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0011 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0010 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0001 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0011 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0006 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0007 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0205 f1 72.73 precision 80.00 recall 66.67
Batch 1301|1608: loss 0.0124 f1 66.67 precision 100.00 recall 50.00
Batch 1401|1608: loss 0.0033 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0216 f1 66.67 precision 50.00 recall 100.00
Batch 1601|1608: loss 0.0010 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0039 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0063 f1 89.04 precision 89.51 recall 89.83
Time: 37.00
Dev: loss 0.1129 f1 48.51 precision 58.74 recall 41.31

Epoch 28|30:
Batch 1|1608: loss 0.0013 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0002 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0002 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0032 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0036 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0003 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0017 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0117 f1 66.67 precision 50.00 recall 100.00
Batch 801|1608: loss 0.0008 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0007 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0002 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0001 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0015 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0028 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0063 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0011 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0010 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0037 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0060 f1 88.98 precision 89.58 recall 89.54
Time: 37.00
Dev: loss 0.1103 f1 49.46 precision 57.62 recall 43.32

Epoch 29|30:
Batch 1|1608: loss 0.0053 f1 83.33 precision 83.33 recall 83.33
Batch 101|1608: loss 0.0004 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0003 f1 0.00 precision 0.00 recall 0.00
Batch 301|1608: loss 0.0158 f1 85.71 precision 100.00 recall 75.00
Batch 401|1608: loss 0.0033 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0003 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0028 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0008 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0002 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0008 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0002 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0012 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0004 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0037 f1 0.00 precision 0.00 recall 0.00
Batch 1401|1608: loss 0.0031 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0042 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0010 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0011 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0059 f1 89.31 precision 89.88 recall 90.08
Time: 36.89
Dev: loss 0.1165 f1 49.92 precision 58.28 recall 43.66

Epoch 30|30:
Batch 1|1608: loss 0.0240 f1 92.31 precision 100.00 recall 85.71
Batch 101|1608: loss 0.0013 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0008 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0011 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0006 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0113 f1 75.00 precision 75.00 recall 75.00
Batch 601|1608: loss 0.0003 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0001 f1 0.00 precision 0.00 recall 0.00
Batch 801|1608: loss 0.0183 f1 66.67 precision 66.67 recall 66.67
Batch 901|1608: loss 0.0146 f1 85.71 precision 75.00 recall 100.00
Batch 1001|1608: loss 0.0021 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0006 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0004 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0081 f1 88.89 precision 80.00 recall 100.00
Batch 1401|1608: loss 0.0002 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0001 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0004 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0000 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0056 f1 89.58 precision 90.07 recall 90.29
Time: 36.81
Dev: loss 0.1189 f1 49.89 precision 60.19 recall 42.59

Restore best model !
Test: f1 51.45 precision 56.37 recall 47.33
