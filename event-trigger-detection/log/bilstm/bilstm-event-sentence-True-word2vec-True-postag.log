Namespace(no_train=False, train_batch_size=8, eval_batch_size=8, lr=0.0001, num_epochs=20, log_step=100, no_cuda=False, word_embedding_dim=300, use_pretrained=True, use_postag=True, postag_embedding_dim=25, hidden_size=512, dropout_rate=0.5)
Epoch 1|20:
Batch 1|1608: loss 3.5178 f1 0.00 precision 0.00 recall 0.00
Batch 101|1608: loss 0.1325 f1 0.00 precision 0.00 recall 0.00
Batch 201|1608: loss 0.1486 f1 0.00 precision 0.00 recall 0.00
Batch 301|1608: loss 0.2770 f1 0.00 precision 0.00 recall 0.00
Batch 401|1608: loss 0.1559 f1 0.00 precision 0.00 recall 0.00
Batch 501|1608: loss 0.0131 f1 0.00 precision 0.00 recall 0.00
Batch 601|1608: loss 0.0438 f1 0.00 precision 0.00 recall 0.00
Batch 701|1608: loss 0.0376 f1 0.00 precision 0.00 recall 0.00
Batch 801|1608: loss 0.0614 f1 0.00 precision 0.00 recall 0.00
Batch 901|1608: loss 0.0897 f1 40.00 precision 100.00 recall 25.00
Batch 1001|1608: loss 0.0430 f1 0.00 precision 0.00 recall 0.00
Batch 1101|1608: loss 0.0515 f1 0.00 precision 0.00 recall 0.00
Batch 1201|1608: loss 0.1057 f1 0.00 precision 0.00 recall 0.00
Batch 1301|1608: loss 0.0774 f1 0.00 precision 0.00 recall 0.00
Batch 1401|1608: loss 0.0652 f1 40.00 precision 100.00 recall 25.00
Batch 1501|1608: loss 0.0936 f1 66.67 precision 100.00 recall 50.00
Batch 1601|1608: loss 0.1539 f1 22.22 precision 33.33 recall 16.67
Batch 1608|1608: loss 0.0491 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.1591 f1 13.96 precision 22.44 recall 11.27
Time: 36.62
Dev: loss 0.0737 f1 37.40 precision 56.98 recall 27.84
Save model weight!

Epoch 2|20:
Batch 1|1608: loss 0.0554 f1 44.44 precision 40.00 recall 50.00
Batch 101|1608: loss 0.0861 f1 0.00 precision 0.00 recall 0.00
Batch 201|1608: loss 0.0553 f1 28.57 precision 100.00 recall 16.67
Batch 301|1608: loss 0.1038 f1 57.14 precision 66.67 recall 50.00
Batch 401|1608: loss 0.2406 f1 0.00 precision 0.00 recall 0.00
Batch 501|1608: loss 0.0092 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0821 f1 40.00 precision 50.00 recall 33.33
Batch 701|1608: loss 0.0185 f1 66.67 precision 100.00 recall 50.00
Batch 801|1608: loss 0.0502 f1 33.33 precision 25.00 recall 50.00
Batch 901|1608: loss 0.0726 f1 50.00 precision 100.00 recall 33.33
Batch 1001|1608: loss 0.1229 f1 28.57 precision 33.33 recall 25.00
Batch 1101|1608: loss 0.1035 f1 40.00 precision 100.00 recall 25.00
Batch 1201|1608: loss 0.1604 f1 25.00 precision 25.00 recall 25.00
Batch 1301|1608: loss 0.0901 f1 50.00 precision 50.00 recall 50.00
Batch 1401|1608: loss 0.0384 f1 66.67 precision 100.00 recall 50.00
Batch 1501|1608: loss 0.0434 f1 44.44 precision 40.00 recall 50.00
Batch 1601|1608: loss 0.0276 f1 0.00 precision 0.00 recall 0.00
Batch 1608|1608: loss 0.0184 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0728 f1 37.91 precision 49.90 recall 34.26
Time: 36.33
Dev: loss 0.0601 f1 47.47 precision 60.47 recall 39.07
Save model weight!

Epoch 3|20:
Batch 1|1608: loss 0.0864 f1 0.00 precision 0.00 recall 0.00
Batch 101|1608: loss 0.0715 f1 0.00 precision 0.00 recall 0.00
Batch 201|1608: loss 0.0288 f1 80.00 precision 100.00 recall 66.67
Batch 301|1608: loss 0.0640 f1 40.00 precision 66.67 recall 28.57
Batch 401|1608: loss 0.0684 f1 40.00 precision 100.00 recall 25.00
Batch 501|1608: loss 0.0528 f1 25.00 precision 33.33 recall 20.00
Batch 601|1608: loss 0.0166 f1 66.67 precision 100.00 recall 50.00
Batch 701|1608: loss 0.0526 f1 0.00 precision 0.00 recall 0.00
Batch 801|1608: loss 0.0816 f1 33.33 precision 33.33 recall 33.33
Batch 901|1608: loss 0.0076 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0177 f1 0.00 precision 0.00 recall 0.00
Batch 1101|1608: loss 0.0182 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0422 f1 0.00 precision 0.00 recall 0.00
Batch 1301|1608: loss 0.0433 f1 50.00 precision 100.00 recall 33.33
Batch 1401|1608: loss 0.0062 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0787 f1 0.00 precision 0.00 recall 0.00
Batch 1601|1608: loss 0.0807 f1 22.22 precision 25.00 recall 20.00
Batch 1608|1608: loss 0.1685 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0614 f1 44.30 precision 55.03 recall 41.39
Time: 36.74
Dev: loss 0.0565 f1 48.86 precision 60.76 recall 40.86
Save model weight!

Epoch 4|20:
Batch 1|1608: loss 0.0169 f1 66.67 precision 100.00 recall 50.00
Batch 101|1608: loss 0.0289 f1 66.67 precision 100.00 recall 50.00
Batch 201|1608: loss 0.0307 f1 66.67 precision 66.67 recall 66.67
Batch 301|1608: loss 0.0457 f1 88.89 precision 100.00 recall 80.00
Batch 401|1608: loss 0.0610 f1 50.00 precision 50.00 recall 50.00
Batch 501|1608: loss 0.0192 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0350 f1 0.00 precision 0.00 recall 0.00
Batch 701|1608: loss 0.0847 f1 60.00 precision 75.00 recall 50.00
Batch 801|1608: loss 0.0395 f1 57.14 precision 40.00 recall 100.00
Batch 901|1608: loss 0.0191 f1 50.00 precision 50.00 recall 50.00
Batch 1001|1608: loss 0.0249 f1 66.67 precision 50.00 recall 100.00
Batch 1101|1608: loss 0.0565 f1 0.00 precision 0.00 recall 0.00
Batch 1201|1608: loss 0.0483 f1 50.00 precision 50.00 recall 50.00
Batch 1301|1608: loss 0.0381 f1 66.67 precision 75.00 recall 60.00
Batch 1401|1608: loss 0.0429 f1 66.67 precision 66.67 recall 66.67
Batch 1501|1608: loss 0.0537 f1 28.57 precision 33.33 recall 25.00
Batch 1601|1608: loss 0.0651 f1 25.00 precision 25.00 recall 25.00
Batch 1608|1608: loss 0.0425 f1 66.67 precision 50.00 recall 100.00
Train: loss 0.0542 f1 47.17 precision 55.75 recall 45.03
Time: 36.94
Dev: loss 0.0533 f1 53.10 precision 60.82 recall 47.12
Save model weight!

Epoch 5|20:
Batch 1|1608: loss 0.0745 f1 75.00 precision 75.00 recall 75.00
Batch 101|1608: loss 0.0135 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0142 f1 66.67 precision 100.00 recall 50.00
Batch 301|1608: loss 0.0349 f1 66.67 precision 50.00 recall 100.00
Batch 401|1608: loss 0.0289 f1 0.00 precision 0.00 recall 0.00
Batch 501|1608: loss 0.0742 f1 40.00 precision 50.00 recall 33.33
Batch 601|1608: loss 0.0968 f1 85.71 precision 100.00 recall 75.00
Batch 701|1608: loss 0.0251 f1 80.00 precision 80.00 recall 80.00
Batch 801|1608: loss 0.0114 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0528 f1 66.67 precision 100.00 recall 50.00
Batch 1001|1608: loss 0.1312 f1 33.33 precision 50.00 recall 25.00
Batch 1101|1608: loss 0.0155 f1 66.67 precision 100.00 recall 50.00
Batch 1201|1608: loss 0.0657 f1 0.00 precision 0.00 recall 0.00
Batch 1301|1608: loss 0.0207 f1 50.00 precision 50.00 recall 50.00
Batch 1401|1608: loss 0.0614 f1 85.71 precision 75.00 recall 100.00
Batch 1501|1608: loss 0.0135 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0751 f1 66.67 precision 100.00 recall 50.00
Batch 1608|1608: loss 0.0090 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0482 f1 52.13 precision 60.13 recall 50.89
Time: 37.11
Dev: loss 0.0526 f1 54.80 precision 60.65 recall 49.97
Save model weight!

Epoch 6|20:
Batch 1|1608: loss 0.0620 f1 33.33 precision 25.00 recall 50.00
Batch 101|1608: loss 0.0436 f1 66.67 precision 100.00 recall 50.00
Batch 201|1608: loss 0.0248 f1 80.00 precision 66.67 recall 100.00
Batch 301|1608: loss 0.0227 f1 66.67 precision 66.67 recall 66.67
Batch 401|1608: loss 0.0350 f1 72.73 precision 80.00 recall 66.67
Batch 501|1608: loss 0.0301 f1 33.33 precision 25.00 recall 50.00
Batch 601|1608: loss 0.0424 f1 40.00 precision 50.00 recall 33.33
Batch 701|1608: loss 0.0262 f1 0.00 precision 0.00 recall 0.00
Batch 801|1608: loss 0.0724 f1 33.33 precision 100.00 recall 20.00
Batch 901|1608: loss 0.0829 f1 46.15 precision 75.00 recall 33.33
Batch 1001|1608: loss 0.0657 f1 60.00 precision 60.00 recall 60.00
Batch 1101|1608: loss 0.0366 f1 82.35 precision 77.78 recall 87.50
Batch 1201|1608: loss 0.0225 f1 60.00 precision 60.00 recall 60.00
Batch 1301|1608: loss 0.0822 f1 46.15 precision 50.00 recall 42.86
Batch 1401|1608: loss 0.0304 f1 50.00 precision 100.00 recall 33.33
Batch 1501|1608: loss 0.0101 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0187 f1 85.71 precision 100.00 recall 75.00
Batch 1608|1608: loss 0.0803 f1 66.67 precision 75.00 recall 60.00
Train: loss 0.0426 f1 53.89 precision 61.86 recall 52.06
Time: 37.35
Dev: loss 0.0542 f1 54.49 precision 54.88 recall 54.11

Epoch 7|20:
Batch 1|1608: loss 0.0063 f1 0.00 precision 0.00 recall 0.00
Batch 101|1608: loss 0.0266 f1 50.00 precision 50.00 recall 50.00
Batch 201|1608: loss 0.0480 f1 75.00 precision 75.00 recall 75.00
Batch 301|1608: loss 0.0419 f1 66.67 precision 66.67 recall 66.67
Batch 401|1608: loss 0.0432 f1 50.00 precision 100.00 recall 33.33
Batch 501|1608: loss 0.1572 f1 46.15 precision 50.00 recall 42.86
Batch 601|1608: loss 0.0351 f1 66.67 precision 50.00 recall 100.00
Batch 701|1608: loss 0.0158 f1 50.00 precision 50.00 recall 50.00
Batch 801|1608: loss 0.0113 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0095 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0298 f1 50.00 precision 50.00 recall 50.00
Batch 1101|1608: loss 0.0336 f1 50.00 precision 50.00 recall 50.00
Batch 1201|1608: loss 0.0286 f1 50.00 precision 100.00 recall 33.33
Batch 1301|1608: loss 0.0992 f1 33.33 precision 33.33 recall 33.33
Batch 1401|1608: loss 0.0148 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0352 f1 50.00 precision 42.86 recall 60.00
Batch 1601|1608: loss 0.0130 f1 0.00 precision 0.00 recall 0.00
Batch 1608|1608: loss 0.0485 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0375 f1 58.35 precision 64.49 recall 57.50
Time: 37.30
Dev: loss 0.0546 f1 53.81 precision 53.73 recall 53.88

Epoch 8|20:
Batch 1|1608: loss 0.0869 f1 33.33 precision 33.33 recall 33.33
Batch 101|1608: loss 0.0044 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0113 f1 0.00 precision 0.00 recall 0.00
Batch 301|1608: loss 0.0097 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0112 f1 80.00 precision 66.67 recall 100.00
Batch 501|1608: loss 0.0223 f1 75.00 precision 75.00 recall 75.00
Batch 601|1608: loss 0.0266 f1 80.00 precision 66.67 recall 100.00
Batch 701|1608: loss 0.1261 f1 66.67 precision 75.00 recall 60.00
Batch 801|1608: loss 0.0463 f1 66.67 precision 75.00 recall 60.00
Batch 901|1608: loss 0.0351 f1 90.91 precision 100.00 recall 83.33
Batch 1001|1608: loss 0.0771 f1 46.15 precision 60.00 recall 37.50
Batch 1101|1608: loss 0.0130 f1 66.67 precision 50.00 recall 100.00
Batch 1201|1608: loss 0.0413 f1 44.44 precision 50.00 recall 40.00
Batch 1301|1608: loss 0.0111 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0527 f1 80.00 precision 100.00 recall 66.67
Batch 1501|1608: loss 0.0221 f1 66.67 precision 66.67 recall 66.67
Batch 1601|1608: loss 0.0854 f1 57.14 precision 66.67 recall 50.00
Batch 1608|1608: loss 0.0378 f1 66.67 precision 50.00 recall 100.00
Train: loss 0.0324 f1 62.71 precision 68.25 recall 61.83
Time: 37.32
Dev: loss 0.0555 f1 53.67 precision 60.22 recall 48.41

Epoch 9|20:
Batch 1|1608: loss 0.0079 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0202 f1 66.67 precision 100.00 recall 50.00
Batch 201|1608: loss 0.0254 f1 50.00 precision 50.00 recall 50.00
Batch 301|1608: loss 0.0586 f1 0.00 precision 0.00 recall 0.00
Batch 401|1608: loss 0.0082 f1 0.00 precision 0.00 recall 0.00
Batch 501|1608: loss 0.0018 f1 0.00 precision 0.00 recall 0.00
Batch 601|1608: loss 0.0319 f1 40.00 precision 33.33 recall 50.00
Batch 701|1608: loss 0.0732 f1 57.14 precision 66.67 recall 50.00
Batch 801|1608: loss 0.0190 f1 90.91 precision 100.00 recall 83.33
Batch 901|1608: loss 0.0233 f1 50.00 precision 100.00 recall 33.33
Batch 1001|1608: loss 0.0168 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0946 f1 0.00 precision 0.00 recall 0.00
Batch 1201|1608: loss 0.0049 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0068 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0186 f1 75.00 precision 75.00 recall 75.00
Batch 1501|1608: loss 0.0041 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0081 f1 66.67 precision 100.00 recall 50.00
Batch 1608|1608: loss 0.0582 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0286 f1 65.22 precision 70.21 recall 64.49
Time: 37.35
Dev: loss 0.0577 f1 54.28 precision 56.66 recall 52.10

Epoch 10|20:
Batch 1|1608: loss 0.0269 f1 0.00 precision 0.00 recall 0.00
Batch 101|1608: loss 0.0180 f1 66.67 precision 66.67 recall 66.67
Batch 201|1608: loss 0.0167 f1 85.71 precision 75.00 recall 100.00
Batch 301|1608: loss 0.0363 f1 75.00 precision 75.00 recall 75.00
Batch 401|1608: loss 0.0131 f1 80.00 precision 66.67 recall 100.00
Batch 501|1608: loss 0.0086 f1 85.71 precision 100.00 recall 75.00
Batch 601|1608: loss 0.0308 f1 85.71 precision 100.00 recall 75.00
Batch 701|1608: loss 0.0242 f1 75.00 precision 60.00 recall 100.00
Batch 801|1608: loss 0.0554 f1 66.67 precision 62.50 recall 71.43
Batch 901|1608: loss 0.0327 f1 66.67 precision 100.00 recall 50.00
Batch 1001|1608: loss 0.0026 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0070 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0066 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0074 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0159 f1 80.00 precision 100.00 recall 66.67
Batch 1501|1608: loss 0.0094 f1 0.00 precision 0.00 recall 0.00
Batch 1601|1608: loss 0.0313 f1 71.43 precision 71.43 recall 71.43
Batch 1608|1608: loss 0.1117 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0243 f1 69.62 precision 73.63 recall 69.29
Time: 37.42
Dev: loss 0.0609 f1 53.77 precision 54.51 recall 53.05

Epoch 11|20:
Batch 1|1608: loss 0.0159 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0058 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0225 f1 66.67 precision 100.00 recall 50.00
Batch 301|1608: loss 0.0131 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0458 f1 72.73 precision 100.00 recall 57.14
Batch 501|1608: loss 0.0187 f1 88.89 precision 80.00 recall 100.00
Batch 601|1608: loss 0.0269 f1 0.00 precision 0.00 recall 0.00
Batch 701|1608: loss 0.0283 f1 40.00 precision 33.33 recall 50.00
Batch 801|1608: loss 0.0086 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0199 f1 40.00 precision 50.00 recall 33.33
Batch 1001|1608: loss 0.0730 f1 76.92 precision 71.43 recall 83.33
Batch 1101|1608: loss 0.0372 f1 90.00 precision 100.00 recall 81.82
Batch 1201|1608: loss 0.0475 f1 66.67 precision 75.00 recall 60.00
Batch 1301|1608: loss 0.0170 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0316 f1 75.00 precision 60.00 recall 100.00
Batch 1501|1608: loss 0.0170 f1 50.00 precision 40.00 recall 66.67
Batch 1601|1608: loss 0.0048 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0140 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0211 f1 72.76 precision 76.18 recall 72.41
Time: 37.44
Dev: loss 0.0623 f1 54.85 precision 56.91 recall 52.93
Save model weight!

Epoch 12|20:
Batch 1|1608: loss 0.0236 f1 66.67 precision 50.00 recall 100.00
Batch 101|1608: loss 0.0329 f1 66.67 precision 66.67 recall 66.67
Batch 201|1608: loss 0.0130 f1 90.91 precision 100.00 recall 83.33
Batch 301|1608: loss 0.0220 f1 57.14 precision 66.67 recall 50.00
Batch 401|1608: loss 0.0610 f1 40.00 precision 66.67 recall 28.57
Batch 501|1608: loss 0.0177 f1 0.00 precision 0.00 recall 0.00
Batch 601|1608: loss 0.0063 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0156 f1 85.71 precision 75.00 recall 100.00
Batch 801|1608: loss 0.0034 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0106 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0195 f1 50.00 precision 50.00 recall 50.00
Batch 1101|1608: loss 0.0255 f1 66.67 precision 66.67 recall 66.67
Batch 1201|1608: loss 0.0013 f1 0.00 precision 0.00 recall 0.00
Batch 1301|1608: loss 0.0007 f1 0.00 precision 0.00 recall 0.00
Batch 1401|1608: loss 0.0075 f1 66.67 precision 100.00 recall 50.00
Batch 1501|1608: loss 0.0232 f1 66.67 precision 100.00 recall 50.00
Batch 1601|1608: loss 0.0202 f1 66.67 precision 75.00 recall 60.00
Batch 1608|1608: loss 0.0015 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0184 f1 76.26 precision 79.16 recall 76.26
Time: 37.44
Dev: loss 0.0638 f1 53.18 precision 54.24 recall 52.15

Epoch 13|20:
Batch 1|1608: loss 0.0175 f1 72.73 precision 66.67 recall 80.00
Batch 101|1608: loss 0.0036 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0061 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0123 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0029 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0040 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0367 f1 80.00 precision 100.00 recall 66.67
Batch 701|1608: loss 0.0044 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0390 f1 28.57 precision 25.00 recall 33.33
Batch 901|1608: loss 0.0273 f1 66.67 precision 66.67 recall 66.67
Batch 1001|1608: loss 0.0096 f1 66.67 precision 100.00 recall 50.00
Batch 1101|1608: loss 0.0053 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0098 f1 88.89 precision 100.00 recall 80.00
Batch 1301|1608: loss 0.0411 f1 40.00 precision 50.00 recall 33.33
Batch 1401|1608: loss 0.0182 f1 80.00 precision 66.67 recall 100.00
Batch 1501|1608: loss 0.0237 f1 83.33 precision 83.33 recall 83.33
Batch 1601|1608: loss 0.0037 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0947 f1 66.67 precision 100.00 recall 50.00
Train: loss 0.0159 f1 78.31 precision 80.57 recall 78.60
Time: 37.47
Dev: loss 0.0700 f1 53.25 precision 60.64 recall 47.46

Epoch 14|20:
Batch 1|1608: loss 0.0246 f1 60.00 precision 75.00 recall 50.00
Batch 101|1608: loss 0.0006 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0067 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0121 f1 80.00 precision 100.00 recall 66.67
Batch 401|1608: loss 0.0117 f1 85.71 precision 100.00 recall 75.00
Batch 501|1608: loss 0.0026 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0113 f1 66.67 precision 100.00 recall 50.00
Batch 701|1608: loss 0.0069 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0239 f1 80.00 precision 100.00 recall 66.67
Batch 901|1608: loss 0.0183 f1 80.00 precision 100.00 recall 66.67
Batch 1001|1608: loss 0.0153 f1 50.00 precision 50.00 recall 50.00
Batch 1101|1608: loss 0.0055 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0023 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0046 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0067 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0121 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0431 f1 85.71 precision 100.00 recall 75.00
Batch 1608|1608: loss 0.0067 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0146 f1 80.88 precision 82.77 recall 81.29
Time: 37.50
Dev: loss 0.0690 f1 54.13 precision 51.57 recall 56.96

Epoch 15|20:
Batch 1|1608: loss 0.0083 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0037 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0120 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0092 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0188 f1 80.00 precision 100.00 recall 66.67
Batch 501|1608: loss 0.0167 f1 88.89 precision 100.00 recall 80.00
Batch 601|1608: loss 0.0017 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0113 f1 85.71 precision 100.00 recall 75.00
Batch 801|1608: loss 0.0079 f1 88.89 precision 100.00 recall 80.00
Batch 901|1608: loss 0.0034 f1 0.00 precision 0.00 recall 0.00
Batch 1001|1608: loss 0.0080 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0053 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0043 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0227 f1 80.00 precision 100.00 recall 66.67
Batch 1401|1608: loss 0.0043 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0151 f1 80.00 precision 80.00 recall 80.00
Batch 1601|1608: loss 0.0251 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0014 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0134 f1 81.78 precision 83.44 recall 82.11
Time: 37.50
Dev: loss 0.0729 f1 52.96 precision 57.50 recall 49.08

Epoch 16|20:
Batch 1|1608: loss 0.0049 f1 50.00 precision 50.00 recall 50.00
Batch 101|1608: loss 0.0148 f1 92.31 precision 85.71 recall 100.00
Batch 201|1608: loss 0.0013 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0059 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0219 f1 88.89 precision 80.00 recall 100.00
Batch 501|1608: loss 0.0374 f1 60.00 precision 60.00 recall 60.00
Batch 601|1608: loss 0.0085 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0202 f1 88.89 precision 80.00 recall 100.00
Batch 801|1608: loss 0.0014 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0009 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0012 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0007 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0018 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0021 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0060 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0346 f1 60.00 precision 60.00 recall 60.00
Batch 1601|1608: loss 0.0082 f1 80.00 precision 66.67 recall 100.00
Batch 1608|1608: loss 0.0208 f1 66.67 precision 50.00 recall 100.00
Train: loss 0.0119 f1 83.64 precision 84.92 recall 84.16
Time: 37.47
Dev: loss 0.0769 f1 52.57 precision 57.20 recall 48.63

Epoch 17|20:
Batch 1|1608: loss 0.0024 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0058 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0068 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0064 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0014 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0039 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0018 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0051 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0057 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0304 f1 80.00 precision 100.00 recall 66.67
Batch 1001|1608: loss 0.0067 f1 88.89 precision 80.00 recall 100.00
Batch 1101|1608: loss 0.0136 f1 88.89 precision 80.00 recall 100.00
Batch 1201|1608: loss 0.0025 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0242 f1 0.00 precision 0.00 recall 0.00
Batch 1401|1608: loss 0.0009 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0141 f1 90.91 precision 83.33 recall 100.00
Batch 1601|1608: loss 0.0005 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0082 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0113 f1 85.59 precision 87.01 recall 86.01
Time: 37.44
Dev: loss 0.0760 f1 52.95 precision 57.03 recall 49.41

Epoch 18|20:
Batch 1|1608: loss 0.0119 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0007 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0032 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0037 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0042 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0002 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0176 f1 66.67 precision 75.00 recall 60.00
Batch 701|1608: loss 0.0254 f1 25.00 precision 25.00 recall 25.00
Batch 801|1608: loss 0.0053 f1 0.00 precision 0.00 recall 0.00
Batch 901|1608: loss 0.0076 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0154 f1 85.71 precision 75.00 recall 100.00
Batch 1101|1608: loss 0.0054 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0031 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0056 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0267 f1 80.00 precision 66.67 recall 100.00
Batch 1501|1608: loss 0.0008 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0039 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0006 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0101 f1 85.20 precision 86.20 recall 85.91
Time: 37.48
Dev: loss 0.0779 f1 53.45 precision 52.66 recall 54.28

Epoch 19|20:
Batch 1|1608: loss 0.0153 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0055 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0160 f1 57.14 precision 50.00 recall 66.67
Batch 301|1608: loss 0.0028 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0062 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0059 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0007 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0005 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0029 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0010 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0236 f1 66.67 precision 50.00 recall 100.00
Batch 1101|1608: loss 0.0120 f1 66.67 precision 50.00 recall 100.00
Batch 1201|1608: loss 0.0566 f1 66.67 precision 80.00 recall 57.14
Batch 1301|1608: loss 0.0049 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0016 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0010 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0109 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0024 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0097 f1 85.89 precision 86.94 recall 86.43
Time: 37.48
Dev: loss 0.0798 f1 54.05 precision 55.95 recall 52.26

Epoch 20|20:
Batch 1|1608: loss 0.0013 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0098 f1 40.00 precision 50.00 recall 33.33
Batch 201|1608: loss 0.0025 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0055 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0034 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0449 f1 66.67 precision 66.67 recall 66.67
Batch 601|1608: loss 0.0016 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0249 f1 88.89 precision 80.00 recall 100.00
Batch 801|1608: loss 0.0084 f1 80.00 precision 80.00 recall 80.00
Batch 901|1608: loss 0.0144 f1 92.31 precision 100.00 recall 85.71
Batch 1001|1608: loss 0.0028 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0087 f1 90.91 precision 100.00 recall 83.33
Batch 1201|1608: loss 0.0345 f1 76.92 precision 83.33 recall 71.43
Batch 1301|1608: loss 0.0045 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0003 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0001 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0118 f1 50.00 precision 50.00 recall 50.00
Batch 1608|1608: loss 0.0006 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0094 f1 86.69 precision 87.45 recall 87.47
Time: 37.56
Dev: loss 0.0799 f1 54.12 precision 57.19 recall 51.37

Restore best model !
Test: f1 55.58 precision 58.90 recall 52.61
Namespace(no_train=False, train_batch_size=8, eval_batch_size=8, lr=0.0001, num_epochs=20, log_step=100, no_cuda=False, word_embedding_dim=300, use_pretrained=True, use_postag=True, postag_embedding_dim=25, hidden_size=512, dropout_rate=0.5)
Namespace(no_train=False, train_batch_size=8, eval_batch_size=8, lr=0.0001, num_epochs=20, log_step=100, no_cuda=False, word_embedding_dim=300, use_pretrained=True, use_postag=True, postag_embedding_dim=25, hidden_size=512, dropout_rate=0.5)
Epoch 1|20:
Batch 1|1608: loss 3.5740 f1 1.67 precision 0.88 recall 16.67
Batch 101|1608: loss 0.0832 f1 0.00 precision 0.00 recall 0.00
Batch 201|1608: loss 0.1642 f1 0.00 precision 0.00 recall 0.00
Batch 301|1608: loss 0.2022 f1 0.00 precision 0.00 recall 0.00
Batch 401|1608: loss 0.1955 f1 0.00 precision 0.00 recall 0.00
Batch 501|1608: loss 0.1388 f1 0.00 precision 0.00 recall 0.00
Batch 601|1608: loss 0.2477 f1 44.44 precision 66.67 recall 33.33
Batch 701|1608: loss 0.1008 f1 0.00 precision 0.00 recall 0.00
Batch 801|1608: loss 0.0387 f1 0.00 precision 0.00 recall 0.00
Batch 901|1608: loss 0.1748 f1 0.00 precision 0.00 recall 0.00
Batch 1001|1608: loss 0.1393 f1 0.00 precision 0.00 recall 0.00
Batch 1101|1608: loss 0.0162 f1 0.00 precision 0.00 recall 0.00
Batch 1201|1608: loss 0.0878 f1 57.14 precision 66.67 recall 50.00
Batch 1301|1608: loss 0.0606 f1 0.00 precision 0.00 recall 0.00
Batch 1401|1608: loss 0.0716 f1 0.00 precision 0.00 recall 0.00
Batch 1501|1608: loss 0.1009 f1 20.00 precision 25.00 recall 16.67
Batch 1601|1608: loss 0.0741 f1 0.00 precision 0.00 recall 0.00
Batch 1608|1608: loss 0.0316 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.1582 f1 14.41 precision 23.17 recall 11.78
Time: 990.54
Dev: loss 0.0766 f1 28.24 precision 67.09 recall 17.89
Save model weight!

Epoch 2|20:
Batch 1|1608: loss 0.0041 f1 0.00 precision 0.00 recall 0.00
Batch 101|1608: loss 0.0039 f1 0.00 precision 0.00 recall 0.00
Batch 201|1608: loss 0.1142 f1 25.00 precision 50.00 recall 16.67
Batch 301|1608: loss 0.1793 f1 28.57 precision 33.33 recall 25.00
Batch 401|1608: loss 0.0387 f1 80.00 precision 66.67 recall 100.00
Batch 501|1608: loss 0.1555 f1 66.67 precision 100.00 recall 50.00
Batch 601|1608: loss 0.0485 f1 0.00 precision 0.00 recall 0.00
Batch 701|1608: loss 0.1136 f1 33.33 precision 50.00 recall 25.00
Batch 801|1608: loss 0.1042 f1 28.57 precision 50.00 recall 20.00
Batch 901|1608: loss 0.0559 f1 66.67 precision 75.00 recall 60.00
Batch 1001|1608: loss 0.0357 f1 66.67 precision 100.00 recall 50.00
Batch 1101|1608: loss 0.0762 f1 80.00 precision 85.71 recall 75.00
Batch 1201|1608: loss 0.1412 f1 28.57 precision 50.00 recall 20.00
Batch 1301|1608: loss 0.1418 f1 40.00 precision 100.00 recall 25.00
Batch 1401|1608: loss 0.0192 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0929 f1 40.00 precision 100.00 recall 25.00
Batch 1601|1608: loss 0.0925 f1 0.00 precision 0.00 recall 0.00
Batch 1608|1608: loss 0.0155 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0732 f1 38.01 precision 49.59 recall 34.79
Time: 966.62
Dev: loss 0.0601 f1 46.79 precision 59.60 recall 38.51
Save model weight!

Epoch 3|20:
Batch 1|1608: loss 0.0141 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0769 f1 85.71 precision 100.00 recall 75.00
Batch 201|1608: loss 0.0334 f1 66.67 precision 100.00 recall 50.00
Batch 301|1608: loss 0.0611 f1 33.33 precision 50.00 recall 25.00
Batch 401|1608: loss 0.0860 f1 40.00 precision 33.33 recall 50.00
Batch 501|1608: loss 0.1228 f1 53.33 precision 66.67 recall 44.44
Batch 601|1608: loss 0.1094 f1 57.14 precision 66.67 recall 50.00
Batch 701|1608: loss 0.0399 f1 0.00 precision 0.00 recall 0.00
Batch 801|1608: loss 0.0118 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0261 f1 0.00 precision 0.00 recall 0.00
Batch 1001|1608: loss 0.0619 f1 57.14 precision 100.00 recall 40.00
Batch 1101|1608: loss 0.0477 f1 0.00 precision 0.00 recall 0.00
Batch 1201|1608: loss 0.0831 f1 0.00 precision 0.00 recall 0.00
Batch 1301|1608: loss 0.0905 f1 66.67 precision 100.00 recall 50.00
Batch 1401|1608: loss 0.0360 f1 80.00 precision 100.00 recall 66.67
Batch 1501|1608: loss 0.0269 f1 66.67 precision 100.00 recall 50.00
Batch 1601|1608: loss 0.1193 f1 0.00 precision 0.00 recall 0.00
Batch 1608|1608: loss 0.1330 f1 50.00 precision 50.00 recall 50.00
Train: loss 0.0609 f1 44.57 precision 54.32 recall 42.27
Time: 928.86
Dev: loss 0.0572 f1 54.43 precision 54.25 recall 54.61
Save model weight!

Epoch 4|20:
Batch 1|1608: loss 0.0250 f1 85.71 precision 75.00 recall 100.00
Batch 101|1608: loss 0.0309 f1 50.00 precision 100.00 recall 33.33
Batch 201|1608: loss 0.0168 f1 0.00 precision 0.00 recall 0.00
Batch 301|1608: loss 0.0422 f1 80.00 precision 100.00 recall 66.67
Batch 401|1608: loss 0.0410 f1 85.71 precision 100.00 recall 75.00
Batch 501|1608: loss 0.1886 f1 50.00 precision 100.00 recall 33.33
Batch 601|1608: loss 0.0518 f1 66.67 precision 66.67 recall 66.67
Batch 701|1608: loss 0.0375 f1 54.55 precision 50.00 recall 60.00
Batch 801|1608: loss 0.0265 f1 40.00 precision 50.00 recall 33.33
Batch 901|1608: loss 0.0323 f1 85.71 precision 100.00 recall 75.00
Batch 1001|1608: loss 0.0726 f1 50.00 precision 66.67 recall 40.00
Batch 1101|1608: loss 0.0420 f1 66.67 precision 100.00 recall 50.00
Batch 1201|1608: loss 0.0944 f1 50.00 precision 75.00 recall 37.50
Batch 1301|1608: loss 0.0298 f1 50.00 precision 50.00 recall 50.00
Batch 1401|1608: loss 0.0441 f1 57.14 precision 50.00 recall 66.67
Namespace(no_train=False, train_batch_size=8, eval_batch_size=8, lr=0.0001, num_epochs=30, log_step=100, no_cuda=False, word_embedding_dim=300, use_pretrained=True, use_postag=True, postag_embedding_dim=25, hidden_size=512, dropout_rate=0.5)
Epoch 1|30:
Batch 1|1608: loss 3.4602 f1 0.00 precision 0.00 recall 0.00
Batch 101|1608: loss 0.2708 f1 0.00 precision 0.00 recall 0.00
Batch 201|1608: loss 0.1796 f1 0.00 precision 0.00 recall 0.00
Batch 301|1608: loss 0.2366 f1 0.00 precision 0.00 recall 0.00
Batch 401|1608: loss 0.0248 f1 0.00 precision 0.00 recall 0.00
Batch 501|1608: loss 0.0948 f1 0.00 precision 0.00 recall 0.00
Batch 601|1608: loss 0.1814 f1 22.22 precision 100.00 recall 12.50
Batch 701|1608: loss 0.1042 f1 33.33 precision 50.00 recall 25.00
Batch 801|1608: loss 0.0717 f1 50.00 precision 100.00 recall 33.33
Batch 901|1608: loss 0.1218 f1 0.00 precision 0.00 recall 0.00
Batch 1001|1608: loss 0.0729 f1 0.00 precision 0.00 recall 0.00
Batch 1101|1608: loss 0.0271 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.1508 f1 0.00 precision 0.00 recall 0.00
Batch 1301|1608: loss 0.0074 f1 0.00 precision 0.00 recall 0.00
Batch 1401|1608: loss 0.0609 f1 40.00 precision 50.00 recall 33.33
Batch 1501|1608: loss 0.1565 f1 44.44 precision 50.00 recall 40.00
Batch 1601|1608: loss 0.0326 f1 66.67 precision 100.00 recall 50.00
Batch 1608|1608: loss 0.1491 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.1559 f1 14.65 precision 23.17 recall 12.18
Time: 36.81
Dev: loss 0.0745 f1 44.66 precision 58.51 recall 36.11
Save model weight!

Epoch 2|30:
Batch 1|1608: loss 0.0885 f1 0.00 precision 0.00 recall 0.00
Batch 101|1608: loss 0.0352 f1 40.00 precision 50.00 recall 33.33
Batch 201|1608: loss 0.0177 f1 66.67 precision 100.00 recall 50.00
Batch 301|1608: loss 0.0679 f1 40.00 precision 100.00 recall 25.00
Batch 401|1608: loss 0.0666 f1 50.00 precision 100.00 recall 33.33
Batch 501|1608: loss 0.0597 f1 40.00 precision 100.00 recall 25.00
Batch 601|1608: loss 0.0521 f1 0.00 precision 0.00 recall 0.00
Batch 701|1608: loss 0.1186 f1 40.00 precision 100.00 recall 25.00
Batch 801|1608: loss 0.0469 f1 40.00 precision 50.00 recall 33.33
Batch 901|1608: loss 0.0510 f1 50.00 precision 50.00 recall 50.00
Batch 1001|1608: loss 0.0577 f1 66.67 precision 66.67 recall 66.67
Batch 1101|1608: loss 0.1351 f1 50.00 precision 100.00 recall 33.33
Batch 1201|1608: loss 0.0200 f1 80.00 precision 100.00 recall 66.67
Batch 1301|1608: loss 0.0344 f1 0.00 precision 0.00 recall 0.00
Batch 1401|1608: loss 0.0124 f1 0.00 precision 0.00 recall 0.00
Batch 1501|1608: loss 0.0140 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.1563 f1 50.00 precision 50.00 recall 50.00
Batch 1608|1608: loss 0.0483 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0731 f1 37.76 precision 49.17 recall 34.49
Time: 37.26
Dev: loss 0.0602 f1 49.28 precision 58.13 recall 42.76
Save model weight!

Epoch 3|30:
Batch 1|1608: loss 0.0211 f1 80.00 precision 66.67 recall 100.00
Batch 101|1608: loss 0.0359 f1 33.33 precision 50.00 recall 25.00
Batch 201|1608: loss 0.0837 f1 54.55 precision 60.00 recall 50.00
Batch 301|1608: loss 0.0753 f1 60.00 precision 100.00 recall 42.86
Batch 401|1608: loss 0.0223 f1 66.67 precision 100.00 recall 50.00
Batch 501|1608: loss 0.0334 f1 0.00 precision 0.00 recall 0.00
Batch 601|1608: loss 0.0877 f1 0.00 precision 0.00 recall 0.00
Batch 701|1608: loss 0.0981 f1 40.00 precision 100.00 recall 25.00
Batch 801|1608: loss 0.0505 f1 66.67 precision 50.00 recall 100.00
Batch 901|1608: loss 0.1211 f1 0.00 precision 0.00 recall 0.00
Batch 1001|1608: loss 0.0803 f1 44.44 precision 66.67 recall 33.33
Batch 1101|1608: loss 0.1585 f1 28.57 precision 50.00 recall 20.00
Batch 1201|1608: loss 0.0230 f1 75.00 precision 100.00 recall 60.00
Batch 1301|1608: loss 0.0617 f1 57.14 precision 66.67 recall 50.00
Batch 1401|1608: loss 0.0259 f1 66.67 precision 50.00 recall 100.00
Batch 1501|1608: loss 0.0180 f1 66.67 precision 50.00 recall 100.00
Batch 1601|1608: loss 0.0060 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0026 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0609 f1 43.89 precision 53.09 recall 41.70
Time: 37.74
Dev: loss 0.0563 f1 50.98 precision 61.48 recall 43.54
Save model weight!

Epoch 4|30:
Batch 1|1608: loss 0.0794 f1 0.00 precision 0.00 recall 0.00
Batch 101|1608: loss 0.0312 f1 72.73 precision 100.00 recall 57.14
Batch 201|1608: loss 0.0883 f1 57.14 precision 66.67 recall 50.00
Batch 301|1608: loss 0.0391 f1 50.00 precision 100.00 recall 33.33
Batch 401|1608: loss 0.0875 f1 40.00 precision 33.33 recall 50.00
Batch 501|1608: loss 0.0523 f1 60.00 precision 75.00 recall 50.00
Batch 601|1608: loss 0.0764 f1 40.00 precision 50.00 recall 33.33
Batch 701|1608: loss 0.0711 f1 15.38 precision 14.29 recall 16.67
Batch 801|1608: loss 0.0351 f1 57.14 precision 66.67 recall 50.00
Batch 901|1608: loss 0.0673 f1 66.67 precision 75.00 recall 60.00
Batch 1001|1608: loss 0.0417 f1 40.00 precision 50.00 recall 33.33
Batch 1101|1608: loss 0.0100 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0497 f1 66.67 precision 50.00 recall 100.00
Batch 1301|1608: loss 0.0396 f1 28.57 precision 25.00 recall 33.33
Batch 1401|1608: loss 0.0329 f1 50.00 precision 100.00 recall 33.33
Batch 1501|1608: loss 0.0402 f1 66.67 precision 50.00 recall 100.00
Batch 1601|1608: loss 0.0221 f1 75.00 precision 75.00 recall 75.00
Batch 1608|1608: loss 0.0232 f1 66.67 precision 100.00 recall 50.00
Train: loss 0.0538 f1 48.65 precision 57.16 recall 47.00
Time: 37.54
Dev: loss 0.0540 f1 52.44 precision 59.02 recall 47.18
Save model weight!

Epoch 5|30:
Batch 1|1608: loss 0.1097 f1 44.44 precision 57.14 recall 36.36
Batch 101|1608: loss 0.0896 f1 36.36 precision 33.33 recall 40.00
Batch 201|1608: loss 0.0522 f1 50.00 precision 66.67 recall 40.00
Batch 301|1608: loss 0.0435 f1 85.71 precision 100.00 recall 75.00
Batch 401|1608: loss 0.0311 f1 50.00 precision 50.00 recall 50.00
Batch 501|1608: loss 0.0923 f1 66.67 precision 62.50 recall 71.43
Batch 601|1608: loss 0.0579 f1 22.22 precision 20.00 recall 25.00
Batch 701|1608: loss 0.0343 f1 80.00 precision 100.00 recall 66.67
Batch 801|1608: loss 0.0540 f1 66.67 precision 100.00 recall 50.00
Batch 901|1608: loss 0.0472 f1 50.00 precision 66.67 recall 40.00
Batch 1001|1608: loss 0.0588 f1 44.44 precision 50.00 recall 40.00
Batch 1101|1608: loss 0.0896 f1 0.00 precision 0.00 recall 0.00
Batch 1201|1608: loss 0.0345 f1 0.00 precision 0.00 recall 0.00
Batch 1301|1608: loss 0.0118 f1 0.00 precision 0.00 recall 0.00
Batch 1401|1608: loss 0.0700 f1 50.00 precision 50.00 recall 50.00
Batch 1501|1608: loss 0.0407 f1 50.00 precision 66.67 recall 40.00
Batch 1601|1608: loss 0.0357 f1 0.00 precision 0.00 recall 0.00
Batch 1608|1608: loss 0.0146 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0484 f1 49.78 precision 57.51 recall 48.24
Time: 37.63
Dev: loss 0.0548 f1 49.36 precision 64.25 recall 40.08

Epoch 6|30:
Batch 1|1608: loss 0.0072 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0212 f1 57.14 precision 50.00 recall 66.67
Batch 201|1608: loss 0.0135 f1 66.67 precision 100.00 recall 50.00
Batch 301|1608: loss 0.0419 f1 54.55 precision 60.00 recall 50.00
Batch 401|1608: loss 0.0270 f1 85.71 precision 100.00 recall 75.00
Batch 501|1608: loss 0.0336 f1 66.67 precision 50.00 recall 100.00
Batch 601|1608: loss 0.0140 f1 0.00 precision 0.00 recall 0.00
Batch 701|1608: loss 0.0300 f1 0.00 precision 0.00 recall 0.00
Batch 801|1608: loss 0.0214 f1 80.00 precision 100.00 recall 66.67
Batch 901|1608: loss 0.0155 f1 50.00 precision 50.00 recall 50.00
Batch 1001|1608: loss 0.0313 f1 66.67 precision 50.00 recall 100.00
Batch 1101|1608: loss 0.0601 f1 44.44 precision 40.00 recall 50.00
Batch 1201|1608: loss 0.1181 f1 66.67 precision 100.00 recall 50.00
Batch 1301|1608: loss 0.0723 f1 57.14 precision 66.67 recall 50.00
Batch 1401|1608: loss 0.0387 f1 0.00 precision 0.00 recall 0.00
Batch 1501|1608: loss 0.0438 f1 57.14 precision 66.67 recall 50.00
Batch 1601|1608: loss 0.0150 f1 85.71 precision 100.00 recall 75.00
Batch 1608|1608: loss 0.0919 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0429 f1 54.72 precision 61.78 recall 53.42
Time: 37.52
Dev: loss 0.0533 f1 54.56 precision 56.42 recall 52.82
Save model weight!

Epoch 7|30:
Batch 1|1608: loss 0.0295 f1 85.71 precision 100.00 recall 75.00
Batch 101|1608: loss 0.0294 f1 0.00 precision 0.00 recall 0.00
Batch 201|1608: loss 0.0177 f1 66.67 precision 50.00 recall 100.00
Batch 301|1608: loss 0.0145 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0965 f1 0.00 precision 0.00 recall 0.00
Batch 501|1608: loss 0.0417 f1 85.71 precision 100.00 recall 75.00
Batch 601|1608: loss 0.0079 f1 0.00 precision 0.00 recall 0.00
Batch 701|1608: loss 0.0030 f1 0.00 precision 0.00 recall 0.00
Batch 801|1608: loss 0.0098 f1 66.67 precision 100.00 recall 50.00
Batch 901|1608: loss 0.0198 f1 80.00 precision 66.67 recall 100.00
Batch 1001|1608: loss 0.0293 f1 57.14 precision 66.67 recall 50.00
Batch 1101|1608: loss 0.0539 f1 50.00 precision 33.33 recall 100.00
Batch 1201|1608: loss 0.0165 f1 0.00 precision 0.00 recall 0.00
Batch 1301|1608: loss 0.0265 f1 85.71 precision 75.00 recall 100.00
Batch 1401|1608: loss 0.0168 f1 88.89 precision 80.00 recall 100.00
Batch 1501|1608: loss 0.0330 f1 66.67 precision 100.00 recall 50.00
Batch 1601|1608: loss 0.0725 f1 40.00 precision 50.00 recall 33.33
Batch 1608|1608: loss 0.1394 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0377 f1 57.73 precision 64.00 recall 56.73
Time: 37.54
Dev: loss 0.0539 f1 54.17 precision 59.13 recall 49.97

Epoch 8|30:
Batch 1|1608: loss 0.0291 f1 0.00 precision 0.00 recall 0.00
Batch 101|1608: loss 0.0260 f1 0.00 precision 0.00 recall 0.00
Batch 201|1608: loss 0.0008 f1 0.00 precision 0.00 recall 0.00
Batch 301|1608: loss 0.0534 f1 66.67 precision 60.00 recall 75.00
Batch 401|1608: loss 0.0382 f1 60.00 precision 60.00 recall 60.00
Batch 501|1608: loss 0.0203 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0237 f1 83.33 precision 83.33 recall 83.33
Batch 701|1608: loss 0.0130 f1 80.00 precision 100.00 recall 66.67
Batch 801|1608: loss 0.0157 f1 90.91 precision 83.33 recall 100.00
Batch 901|1608: loss 0.0143 f1 66.67 precision 66.67 recall 66.67
Batch 1001|1608: loss 0.0074 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0241 f1 66.67 precision 66.67 recall 66.67
Batch 1201|1608: loss 0.0493 f1 44.44 precision 33.33 recall 66.67
Batch 1301|1608: loss 0.0592 f1 50.00 precision 66.67 recall 40.00
Batch 1401|1608: loss 0.0154 f1 75.00 precision 75.00 recall 75.00
Batch 1501|1608: loss 0.0126 f1 0.00 precision 0.00 recall 0.00
Batch 1601|1608: loss 0.0202 f1 66.67 precision 100.00 recall 50.00
Batch 1608|1608: loss 0.0076 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0329 f1 61.67 precision 67.43 recall 60.70
Time: 37.61
Dev: loss 0.0552 f1 53.45 precision 60.90 recall 47.62

Epoch 9|30:
Batch 1|1608: loss 0.0118 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0016 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0085 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0206 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0439 f1 0.00 precision 0.00 recall 0.00
Batch 501|1608: loss 0.0052 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0234 f1 75.00 precision 60.00 recall 100.00
Batch 701|1608: loss 0.0061 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0196 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0176 f1 80.00 precision 100.00 recall 66.67
Batch 1001|1608: loss 0.0046 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0215 f1 66.67 precision 60.00 recall 75.00
Batch 1201|1608: loss 0.0276 f1 80.00 precision 100.00 recall 66.67
Batch 1301|1608: loss 0.0211 f1 85.71 precision 75.00 recall 100.00
Batch 1401|1608: loss 0.0104 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0066 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0940 f1 66.67 precision 66.67 recall 66.67
Batch 1608|1608: loss 0.0154 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0282 f1 65.85 precision 70.63 recall 65.45
Time: 37.63
Dev: loss 0.0563 f1 55.82 precision 57.34 recall 54.39
Save model weight!

Epoch 10|30:
Batch 1|1608: loss 0.0049 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0384 f1 66.67 precision 60.00 recall 75.00
Batch 201|1608: loss 0.0191 f1 50.00 precision 33.33 recall 100.00
Batch 301|1608: loss 0.0262 f1 75.00 precision 100.00 recall 60.00
Batch 401|1608: loss 0.0524 f1 57.14 precision 66.67 recall 50.00
Batch 501|1608: loss 0.0693 f1 57.14 precision 66.67 recall 50.00
Batch 601|1608: loss 0.0180 f1 88.89 precision 100.00 recall 80.00
Batch 701|1608: loss 0.0270 f1 66.67 precision 75.00 recall 60.00
Batch 801|1608: loss 0.0292 f1 88.89 precision 80.00 recall 100.00
Batch 901|1608: loss 0.0090 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0355 f1 0.00 precision 0.00 recall 0.00
Batch 1101|1608: loss 0.0366 f1 0.00 precision 0.00 recall 0.00
Batch 1201|1608: loss 0.0132 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0063 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0179 f1 80.00 precision 80.00 recall 80.00
Batch 1501|1608: loss 0.0225 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0010 f1 0.00 precision 0.00 recall 0.00
Batch 1608|1608: loss 0.0121 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0243 f1 69.05 precision 73.23 recall 68.76
Time: 37.69
Dev: loss 0.0612 f1 52.46 precision 60.06 recall 46.56

Epoch 11|30:
Batch 1|1608: loss 0.0270 f1 40.00 precision 50.00 recall 33.33
Batch 101|1608: loss 0.0067 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0079 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0310 f1 57.14 precision 66.67 recall 50.00
Batch 401|1608: loss 0.0143 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0094 f1 66.67 precision 66.67 recall 66.67
Batch 601|1608: loss 0.0292 f1 66.67 precision 66.67 recall 66.67
Batch 701|1608: loss 0.0248 f1 85.71 precision 75.00 recall 100.00
Batch 801|1608: loss 0.0054 f1 0.00 precision 0.00 recall 0.00
Batch 901|1608: loss 0.0236 f1 66.67 precision 100.00 recall 50.00
Batch 1001|1608: loss 0.0410 f1 75.00 precision 100.00 recall 60.00
Batch 1101|1608: loss 0.0092 f1 85.71 precision 75.00 recall 100.00
Batch 1201|1608: loss 0.0122 f1 80.00 precision 100.00 recall 66.67
Batch 1301|1608: loss 0.0150 f1 57.14 precision 66.67 recall 50.00
Batch 1401|1608: loss 0.0064 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0250 f1 82.35 precision 77.78 recall 87.50
Batch 1601|1608: loss 0.0140 f1 33.33 precision 33.33 recall 33.33
Batch 1608|1608: loss 0.0110 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0204 f1 74.43 precision 77.63 recall 74.49
Time: 37.52
Dev: loss 0.0641 f1 51.47 precision 56.86 recall 47.01

Epoch 12|30:
Batch 1|1608: loss 0.0078 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0035 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0044 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0222 f1 85.71 precision 75.00 recall 100.00
Batch 401|1608: loss 0.0479 f1 57.14 precision 66.67 recall 50.00
Batch 501|1608: loss 0.0113 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0041 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0176 f1 85.71 precision 100.00 recall 75.00
Batch 801|1608: loss 0.0044 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0734 f1 28.57 precision 25.00 recall 33.33
Batch 1001|1608: loss 0.0170 f1 66.67 precision 100.00 recall 50.00
Batch 1101|1608: loss 0.0056 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0064 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0891 f1 76.92 precision 83.33 recall 71.43
Batch 1401|1608: loss 0.0082 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0413 f1 72.73 precision 66.67 recall 80.00
Batch 1601|1608: loss 0.0005 f1 0.00 precision 0.00 recall 0.00
Batch 1608|1608: loss 0.0127 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0183 f1 76.08 precision 78.56 recall 76.52
Time: 37.71
Dev: loss 0.0644 f1 54.42 precision 58.00 recall 51.26

Epoch 13|30:
Batch 1|1608: loss 0.0117 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0039 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0081 f1 75.00 precision 75.00 recall 75.00
Batch 301|1608: loss 0.0141 f1 80.00 precision 100.00 recall 66.67
Batch 401|1608: loss 0.0026 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0229 f1 71.43 precision 71.43 recall 71.43
Batch 601|1608: loss 0.0069 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0249 f1 80.00 precision 66.67 recall 100.00
Batch 801|1608: loss 0.0150 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0414 f1 60.00 precision 60.00 recall 60.00
Batch 1001|1608: loss 0.0070 f1 85.71 precision 75.00 recall 100.00
Batch 1101|1608: loss 0.0045 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0302 f1 80.00 precision 80.00 recall 80.00
Batch 1301|1608: loss 0.0015 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0130 f1 85.71 precision 75.00 recall 100.00
Batch 1501|1608: loss 0.0152 f1 75.00 precision 75.00 recall 75.00
Batch 1601|1608: loss 0.0271 f1 61.54 precision 57.14 recall 66.67
Batch 1608|1608: loss 0.0257 f1 80.00 precision 66.67 recall 100.00
Train: loss 0.0160 f1 78.32 precision 80.68 recall 78.52
Time: 37.69
Dev: loss 0.0691 f1 52.83 precision 54.45 recall 51.31

Epoch 14|30:
Batch 1|1608: loss 0.0030 f1 0.00 precision 0.00 recall 0.00
Batch 101|1608: loss 0.0096 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0102 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0035 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0070 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0037 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0043 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0092 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0017 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0021 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0373 f1 66.67 precision 100.00 recall 50.00
Batch 1101|1608: loss 0.0044 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0416 f1 80.00 precision 100.00 recall 66.67
Batch 1301|1608: loss 0.0124 f1 66.67 precision 66.67 recall 66.67
Batch 1401|1608: loss 0.0545 f1 88.89 precision 80.00 recall 100.00
Batch 1501|1608: loss 0.0027 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0247 f1 88.89 precision 80.00 recall 100.00
Batch 1608|1608: loss 0.0042 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0142 f1 81.61 precision 83.52 recall 81.91
Time: 37.62
Dev: loss 0.0715 f1 53.60 precision 59.05 recall 49.08

Epoch 15|30:
Batch 1|1608: loss 0.0041 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0055 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0008 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0168 f1 92.31 precision 85.71 recall 100.00
Batch 401|1608: loss 0.0147 f1 88.89 precision 80.00 recall 100.00
Batch 501|1608: loss 0.0003 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0019 f1 0.00 precision 0.00 recall 0.00
Batch 701|1608: loss 0.0039 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0087 f1 66.67 precision 50.00 recall 100.00
Batch 901|1608: loss 0.0022 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0171 f1 0.00 precision 0.00 recall 0.00
Batch 1101|1608: loss 0.0050 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0020 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0075 f1 85.71 precision 100.00 recall 75.00
Batch 1401|1608: loss 0.0018 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0019 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0062 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0357 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0132 f1 81.76 precision 83.56 recall 82.20
Time: 37.56
Dev: loss 0.0773 f1 50.99 precision 57.88 recall 45.56

Epoch 16|30:
Batch 1|1608: loss 0.0348 f1 80.00 precision 100.00 recall 66.67
Batch 101|1608: loss 0.0254 f1 80.00 precision 100.00 recall 66.67
Batch 201|1608: loss 0.0163 f1 85.71 precision 100.00 recall 75.00
Batch 301|1608: loss 0.0126 f1 50.00 precision 50.00 recall 50.00
Batch 401|1608: loss 0.0118 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0081 f1 85.71 precision 75.00 recall 100.00
Batch 601|1608: loss 0.0054 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0074 f1 66.67 precision 50.00 recall 100.00
Batch 801|1608: loss 0.0075 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0003 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0033 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0142 f1 72.73 precision 66.67 recall 80.00
Batch 1201|1608: loss 0.0119 f1 92.31 precision 100.00 recall 85.71
Batch 1301|1608: loss 0.0054 f1 80.00 precision 100.00 recall 66.67
Batch 1401|1608: loss 0.0047 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0044 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0104 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0002 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0120 f1 83.42 precision 84.85 recall 83.90
Time: 37.54
Dev: loss 0.0754 f1 53.50 precision 54.56 recall 52.49

Epoch 17|30:
Batch 1|1608: loss 0.0030 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0515 f1 57.14 precision 50.00 recall 66.67
Batch 201|1608: loss 0.0059 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0015 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0049 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0144 f1 75.00 precision 75.00 recall 75.00
Batch 601|1608: loss 0.0047 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0051 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0036 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0229 f1 85.71 precision 75.00 recall 100.00
Batch 1001|1608: loss 0.0031 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0037 f1 0.00 precision 0.00 recall 0.00
Batch 1201|1608: loss 0.0184 f1 80.00 precision 66.67 recall 100.00
Batch 1301|1608: loss 0.0206 f1 0.00 precision 0.00 recall 0.00
Batch 1401|1608: loss 0.0030 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0089 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0155 f1 80.00 precision 66.67 recall 100.00
Batch 1608|1608: loss 0.0008 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0112 f1 84.89 precision 86.06 recall 85.48
Time: 37.53
Dev: loss 0.0781 f1 52.23 precision 59.01 recall 46.84

Epoch 18|30:
Batch 1|1608: loss 0.0061 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0020 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0005 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0007 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0430 f1 50.00 precision 33.33 recall 100.00
Batch 501|1608: loss 0.0019 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0045 f1 80.00 precision 100.00 recall 66.67
Batch 701|1608: loss 0.0243 f1 80.00 precision 66.67 recall 100.00
Batch 801|1608: loss 0.0077 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0211 f1 80.00 precision 100.00 recall 66.67
Batch 1001|1608: loss 0.0074 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0010 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0224 f1 80.00 precision 80.00 recall 80.00
Batch 1301|1608: loss 0.0174 f1 94.12 precision 100.00 recall 88.89
Batch 1401|1608: loss 0.0133 f1 66.67 precision 66.67 recall 66.67
Batch 1501|1608: loss 0.0080 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0033 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0000 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0104 f1 85.77 precision 86.99 recall 86.24
Time: 37.52
Dev: loss 0.0799 f1 53.26 precision 56.17 recall 50.64

Epoch 19|30:
Batch 1|1608: loss 0.0035 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0022 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0092 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0063 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0133 f1 80.00 precision 100.00 recall 66.67
Batch 501|1608: loss 0.0020 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0079 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0057 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0104 f1 80.00 precision 66.67 recall 100.00
Batch 901|1608: loss 0.0192 f1 72.73 precision 66.67 recall 80.00
Batch 1001|1608: loss 0.0072 f1 75.00 precision 75.00 recall 75.00
Batch 1101|1608: loss 0.0022 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0039 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0016 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0197 f1 80.00 precision 66.67 recall 100.00
Batch 1501|1608: loss 0.0012 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0015 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0038 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0101 f1 86.09 precision 86.83 recall 86.90
Time: 37.58
Dev: loss 0.0797 f1 54.02 precision 56.42 recall 51.82

Epoch 20|30:
Batch 1|1608: loss 0.0085 f1 0.00 precision 0.00 recall 0.00
Batch 101|1608: loss 0.0072 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0004 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0056 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0247 f1 85.71 precision 75.00 recall 100.00
Batch 501|1608: loss 0.0066 f1 66.67 precision 50.00 recall 100.00
Batch 601|1608: loss 0.0056 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0007 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0028 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0047 f1 66.67 precision 100.00 recall 50.00
Batch 1001|1608: loss 0.0065 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0226 f1 40.00 precision 33.33 recall 50.00
Batch 1201|1608: loss 0.0035 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0423 f1 57.14 precision 50.00 recall 66.67
Batch 1401|1608: loss 0.0004 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0022 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0086 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0409 f1 75.00 precision 75.00 recall 75.00
Train: loss 0.0092 f1 86.12 precision 86.98 recall 86.81
Time: 37.58
Dev: loss 0.0827 f1 53.03 precision 59.73 recall 47.68

Epoch 21|30:
Batch 1|1608: loss 0.0004 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0003 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0011 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0006 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0127 f1 83.33 precision 83.33 recall 83.33
Batch 501|1608: loss 0.0028 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0065 f1 90.91 precision 83.33 recall 100.00
Batch 701|1608: loss 0.0022 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0001 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0042 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0007 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0026 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0027 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0016 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0024 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0330 f1 66.67 precision 50.00 recall 100.00
Batch 1601|1608: loss 0.0520 f1 50.00 precision 50.00 recall 50.00
Batch 1608|1608: loss 0.0052 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0091 f1 87.34 precision 88.34 recall 87.88
Time: 37.77
Dev: loss 0.0824 f1 53.79 precision 56.37 recall 51.43

Epoch 22|30:
Batch 1|1608: loss 0.0074 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0004 f1 0.00 precision 0.00 recall 0.00
Batch 201|1608: loss 0.0037 f1 80.00 precision 66.67 recall 100.00
Batch 301|1608: loss 0.0449 f1 66.67 precision 50.00 recall 100.00
Batch 401|1608: loss 0.0001 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0165 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0024 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0139 f1 80.00 precision 66.67 recall 100.00
Batch 801|1608: loss 0.0012 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0038 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0117 f1 80.00 precision 80.00 recall 80.00
Batch 1101|1608: loss 0.0160 f1 80.00 precision 66.67 recall 100.00
Batch 1201|1608: loss 0.0001 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0006 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0407 f1 66.67 precision 66.67 recall 66.67
Batch 1501|1608: loss 0.0029 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0599 f1 40.00 precision 50.00 recall 33.33
Batch 1608|1608: loss 0.0266 f1 88.89 precision 80.00 recall 100.00
Train: loss 0.0087 f1 86.33 precision 86.99 recall 87.11
Time: 37.60
Dev: loss 0.0824 f1 53.77 precision 50.22 recall 57.85

Epoch 23|30:
Batch 1|1608: loss 0.0087 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0012 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0002 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0015 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0008 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0062 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0012 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0052 f1 66.67 precision 66.67 recall 66.67
Batch 801|1608: loss 0.0049 f1 85.71 precision 100.00 recall 75.00
Batch 901|1608: loss 0.0037 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0022 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0116 f1 50.00 precision 50.00 recall 50.00
Batch 1201|1608: loss 0.0046 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0007 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0011 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0041 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0013 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0016 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0083 f1 87.98 precision 88.66 recall 88.81
Time: 37.61
Dev: loss 0.0903 f1 51.22 precision 61.18 recall 44.05

Epoch 24|30:
Batch 1|1608: loss 0.0009 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0010 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0028 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0017 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0011 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0119 f1 66.67 precision 100.00 recall 50.00
Batch 601|1608: loss 0.0055 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0089 f1 0.00 precision 0.00 recall 0.00
Batch 801|1608: loss 0.0009 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0252 f1 66.67 precision 100.00 recall 50.00
Batch 1001|1608: loss 0.0058 f1 0.00 precision 0.00 recall 0.00
Batch 1101|1608: loss 0.0015 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0214 f1 66.67 precision 50.00 recall 100.00
Batch 1301|1608: loss 0.0017 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0114 f1 88.89 precision 100.00 recall 80.00
Batch 1501|1608: loss 0.0011 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0008 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0023 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0080 f1 87.86 precision 88.53 recall 88.62
Time: 37.63
Dev: loss 0.0834 f1 54.16 precision 57.34 recall 51.31

Epoch 25|30:
Batch 1|1608: loss 0.0031 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0009 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0238 f1 80.00 precision 100.00 recall 66.67
Batch 301|1608: loss 0.0369 f1 66.67 precision 60.00 recall 75.00
Batch 401|1608: loss 0.0024 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0044 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0021 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0002 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0387 f1 66.67 precision 50.00 recall 100.00
Batch 901|1608: loss 0.0018 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0007 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0199 f1 0.00 precision 0.00 recall 0.00
Batch 1201|1608: loss 0.0149 f1 66.67 precision 100.00 recall 50.00
Batch 1301|1608: loss 0.0150 f1 60.00 precision 75.00 recall 50.00
Batch 1401|1608: loss 0.0027 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0416 f1 57.14 precision 66.67 recall 50.00
Batch 1601|1608: loss 0.0126 f1 80.00 precision 66.67 recall 100.00
Batch 1608|1608: loss 0.0031 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0076 f1 88.54 precision 89.23 recall 89.25
Time: 37.59
Dev: loss 0.0902 f1 52.84 precision 58.99 recall 47.85

Epoch 26|30:
Batch 1|1608: loss 0.0146 f1 94.12 precision 100.00 recall 88.89
Batch 101|1608: loss 0.0187 f1 76.92 precision 71.43 recall 83.33
Batch 201|1608: loss 0.0046 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0015 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0114 f1 66.67 precision 100.00 recall 50.00
Batch 501|1608: loss 0.0010 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0285 f1 40.00 precision 50.00 recall 33.33
Batch 701|1608: loss 0.0017 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0008 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0049 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0103 f1 85.71 precision 75.00 recall 100.00
Batch 1101|1608: loss 0.0027 f1 0.00 precision 0.00 recall 0.00
Batch 1201|1608: loss 0.0009 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0085 f1 90.91 precision 100.00 recall 83.33
Batch 1401|1608: loss 0.0034 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0508 f1 0.00 precision 0.00 recall 0.00
Batch 1601|1608: loss 0.0011 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0281 f1 75.00 precision 75.00 recall 75.00
Train: loss 0.0075 f1 88.41 precision 89.08 recall 89.00
Time: 37.61
Dev: loss 0.0894 f1 51.30 precision 59.47 recall 45.11

Epoch 27|30:
Batch 1|1608: loss 0.0036 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0003 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0016 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0003 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0054 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0142 f1 66.67 precision 100.00 recall 50.00
Batch 601|1608: loss 0.0002 f1 0.00 precision 0.00 recall 0.00
Batch 701|1608: loss 0.0163 f1 88.89 precision 80.00 recall 100.00
Batch 801|1608: loss 0.0017 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0116 f1 90.91 precision 83.33 recall 100.00
Batch 1001|1608: loss 0.0001 f1 0.00 precision 0.00 recall 0.00
Batch 1101|1608: loss 0.0314 f1 80.00 precision 66.67 recall 100.00
Batch 1201|1608: loss 0.0017 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0014 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0003 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0093 f1 75.00 precision 75.00 recall 75.00
Batch 1601|1608: loss 0.0152 f1 66.67 precision 50.00 recall 100.00
Batch 1608|1608: loss 0.0002 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0076 f1 87.57 precision 88.21 recall 88.32
Time: 37.56
Dev: loss 0.0844 f1 54.10 precision 56.40 recall 51.98

Epoch 28|30:
Batch 1|1608: loss 0.0006 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0001 f1 0.00 precision 0.00 recall 0.00
Batch 201|1608: loss 0.0029 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0004 f1 100.00 precision 100.00 recall 100.00
Batch 401|1608: loss 0.0002 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0030 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0044 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0007 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0005 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0215 f1 88.89 precision 80.00 recall 100.00
Batch 1001|1608: loss 0.0065 f1 80.00 precision 100.00 recall 66.67
Batch 1101|1608: loss 0.0213 f1 80.00 precision 75.00 recall 85.71
Batch 1201|1608: loss 0.0013 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0140 f1 85.71 precision 75.00 recall 100.00
Batch 1401|1608: loss 0.0094 f1 66.67 precision 100.00 recall 50.00
Batch 1501|1608: loss 0.0036 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0006 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0146 f1 66.67 precision 100.00 recall 50.00
Train: loss 0.0073 f1 88.36 precision 89.02 recall 89.09
Time: 37.60
Dev: loss 0.0854 f1 54.06 precision 55.36 recall 52.82

Epoch 29|30:
Batch 1|1608: loss 0.0014 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0027 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0043 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0120 f1 80.00 precision 80.00 recall 80.00
Batch 401|1608: loss 0.0005 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0028 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0005 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0007 f1 0.00 precision 0.00 recall 0.00
Batch 801|1608: loss 0.0007 f1 100.00 precision 100.00 recall 100.00
Batch 901|1608: loss 0.0097 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0010 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0001 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0065 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0340 f1 85.71 precision 75.00 recall 100.00
Batch 1401|1608: loss 0.0023 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0002 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0011 f1 100.00 precision 100.00 recall 100.00
Batch 1608|1608: loss 0.0019 f1 100.00 precision 100.00 recall 100.00
Train: loss 0.0070 f1 89.27 precision 89.83 recall 90.01
Time: 37.57
Dev: loss 0.0892 f1 53.09 precision 55.52 recall 50.87

Epoch 30|30:
Batch 1|1608: loss 0.0010 f1 100.00 precision 100.00 recall 100.00
Batch 101|1608: loss 0.0155 f1 100.00 precision 100.00 recall 100.00
Batch 201|1608: loss 0.0005 f1 100.00 precision 100.00 recall 100.00
Batch 301|1608: loss 0.0637 f1 57.14 precision 66.67 recall 50.00
Batch 401|1608: loss 0.0057 f1 100.00 precision 100.00 recall 100.00
Batch 501|1608: loss 0.0023 f1 100.00 precision 100.00 recall 100.00
Batch 601|1608: loss 0.0045 f1 100.00 precision 100.00 recall 100.00
Batch 701|1608: loss 0.0014 f1 100.00 precision 100.00 recall 100.00
Batch 801|1608: loss 0.0001 f1 0.00 precision 0.00 recall 0.00
Batch 901|1608: loss 0.0051 f1 100.00 precision 100.00 recall 100.00
Batch 1001|1608: loss 0.0001 f1 100.00 precision 100.00 recall 100.00
Batch 1101|1608: loss 0.0016 f1 100.00 precision 100.00 recall 100.00
Batch 1201|1608: loss 0.0002 f1 100.00 precision 100.00 recall 100.00
Batch 1301|1608: loss 0.0016 f1 100.00 precision 100.00 recall 100.00
Batch 1401|1608: loss 0.0015 f1 100.00 precision 100.00 recall 100.00
Batch 1501|1608: loss 0.0017 f1 100.00 precision 100.00 recall 100.00
Batch 1601|1608: loss 0.0230 f1 40.00 precision 100.00 recall 25.00
Batch 1608|1608: loss 0.0001 f1 0.00 precision 0.00 recall 0.00
Train: loss 0.0069 f1 88.73 precision 89.33 recall 89.35
Time: 37.77
Dev: loss 0.0858 f1 53.91 precision 56.18 recall 51.82

Restore best model !
Test: f1 56.68 precision 58.93 recall 54.60
Namespace(no_train=True, train_batch_size=8, eval_batch_size=8, lr=0.0001, num_epochs=30, log_step=100, no_cuda=False, word_embedding_dim=300, use_pretrained=True, use_postag=True, postag_embedding_dim=25, hidden_size=512, dropout_rate=0.5)
Namespace(no_train=True, train_batch_size=8, eval_batch_size=8, lr=0.0001, num_epochs=30, log_step=100, no_cuda=False, word_embedding_dim=300, use_pretrained=True, use_postag=True, postag_embedding_dim=25, hidden_size=512, dropout_rate=0.5)
Restore best model !
Test: f1 55.90 precision 60.39 recall 52.03
